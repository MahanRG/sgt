{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Authors: Chitta Ranjan <cran2367@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import warnings\n",
    "\n",
    "########\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "np.random.seed(7) # fix random seed for reproducibility\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "import sklearn.metrics\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sgt import Sgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgt = Sgt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence = np.array([\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"])\n",
    "alphabets = np.array([\"A\", \"B\", \"C\"])\n",
    "lengthsensitive = True\n",
    "kappa = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (array([2, 4, 6, 7, 9]),)),\n",
       " ('B', (array([0, 1, 8]),)),\n",
       " ('C', (array([3, 5]),))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgt.getpositions(sequence = sequence, alphabets = alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.369361</td>\n",
       "      <td>0.442463</td>\n",
       "      <td>0.537637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.414884</td>\n",
       "      <td>0.468038</td>\n",
       "      <td>0.162774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>0.454136</td>\n",
       "      <td>0.068693</td>\n",
       "      <td>0.214492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "A  0.369361  0.442463  0.537637\n",
       "B  0.414884  0.468038  0.162774\n",
       "C  0.454136  0.068693  0.214492"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgt.fit(sequence, alphabets, lengthsensitive, kappa, flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [[\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"], [\"C\", \"Z\", \"Z\", \"Z\", \"D\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90616284 1.31002279 2.6184865  0.         0.         0.86569371\n",
      "  1.23042262 0.52543984 0.         0.         1.37141609 0.28262508\n",
      "  1.35335283 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.09157819 0.92166965 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.92166965\n",
      "  1.45182361 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "s = sgt.fit_transform(corpus)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein Sequence Data Analysis\n",
    "\n",
    "The data used here is taken from www.uniprot.org. This is a public database for proteins. The data contains the protein sequences and their functions. In the following, we will demonstrate \n",
    "- clustering of the sequences.\n",
    "- classification of the sequences with the functions as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'E', 'I', 'E', 'K', 'T', 'N', 'R', 'M', 'N', 'A', 'L', 'F', 'E', 'F', 'Y', 'A', 'A', 'L', 'L', 'T', 'D', 'K', 'Q', 'M', 'N', 'Y', 'I', 'E', 'L', 'Y', 'Y', 'A', 'D', 'D', 'Y', 'S', 'L', 'A', 'E', 'I', 'A', 'E', 'E', 'F', 'G', 'V', 'S', 'R', 'Q', 'A', 'V', 'Y', 'D', 'N', 'I', 'K', 'R', 'T', 'E', 'K', 'I', 'L', 'E', 'D', 'Y', 'E', 'M', 'K', 'L', 'H', 'M', 'Y', 'S', 'D', 'Y', 'I', 'V', 'R', 'S', 'Q', 'I', 'F', 'D', 'Q', 'I', 'L', 'E', 'R', 'Y', 'P', 'K', 'D', 'D', 'F', 'L', 'Q', 'E', 'Q', 'I', 'E', 'I', 'L', 'T', 'S', 'I', 'D', 'N', 'R', 'E']\n"
     ]
    }
   ],
   "source": [
    "protein_data=pd.DataFrame.from_csv('../data/protein_classification.csv')\n",
    "X=protein_data['Sequence']\n",
    "def split(word): \n",
    "    return [char for char in word] \n",
    "\n",
    "sequences = [split(x) for x in X]\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgt = Sgt(kappa = 1, lengthsensitive = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = sgt.fit_transform(corpus=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2112, 400)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Clustering\n",
    "We perform PCA on the sequence embeddings and then do kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6019403543806409\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.120859</td>\n",
       "      <td>1.046758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.655127</td>\n",
       "      <td>-0.211338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034157</td>\n",
       "      <td>0.312279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.502963</td>\n",
       "      <td>0.015795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.128862</td>\n",
       "      <td>1.034211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2\n",
       "0 -1.120859  1.046758\n",
       "1 -0.655127 -0.211338\n",
       "2  0.034157  0.312279\n",
       "3  0.502963  0.015795\n",
       "4 -1.128862  1.034211"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(embedding)\n",
    "X=pca.transform(embedding)\n",
    "\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "df = pd.DataFrame(data=X, columns=['x1', 'x2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAEyCAYAAACVsznTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x158561dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, max_iter =300)\n",
    "kmeans.fit(df)\n",
    "\n",
    "labels = kmeans.predict(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "colmap = {1: 'r', 2: 'g', 3: 'b'}\n",
    "colors = list(map(lambda x: colmap[x+1], labels))\n",
    "plt.scatter(df['x1'], df['x2'], color=colors, alpha=0.5, edgecolor=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Sequence Classification\n",
    "We perform PCA on the sequence embeddings and then do kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = protein_data['Function [CC]']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a 10-fold cross-validation to measure the performance of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inferno/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, input_shape=(400,), kernel_initializer=\"uniform\")`\n",
      "/Users/inferno/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, kernel_initializer=\"uniform\")`\n",
      "/Users/inferno/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "kfold = 10\n",
    "X = pd.DataFrame(embedding)\n",
    "y = encoded_y\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "skf = KFold(n_splits = kfold, shuffle = True, random_state = random_state)\n",
    "k = 0\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train = X_train.as_matrix(columns = None)\n",
    "    X_test = X_test.as_matrix(columns = None)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape = (X_train.shape[1],), init = 'uniform')) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, init='uniform'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred = model.predict_proba(X_train).round().astype(int)\n",
    "\n",
    "    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k+=1\n",
    "    \n",
    "print ('Average f1 score', np.mean(test_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weblog Data Analysis\n",
    "This data sample is taken from https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset. \n",
    "This is a network intrusion data containing audit logs and any attack as a positive label. Since, network intrusion is a rare event, the data is unbalanced. Here we will,\n",
    "- build a sequence classification model to predict a network intrusion.\n",
    "\n",
    "Each sequence contains in the data is a series of activity, for example, {login, password}. The _alphabets_ in the input data sequences are already encoded into integers. The original sequences data file is also present in the `/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['seqlen', 'seq', 'class'], dtype='object')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darpa_data = pd.DataFrame.from_csv('../data/darpa_data.csv')\n",
    "darpa_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = darpa_data['seq']\n",
    "sequences = [x.split('~') for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = darpa_data['class']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequence embeddings\n",
    "In this data, the sequence embeddings should be length-sensitive. The lengths are important here because sequences with similar patterns but different lengths can have different labels. Consider a simple example of two sessions: `{login, pswd, login, pswd,...}` and `{login, pswd,...(repeated several times)..., login, pswd}`. While the first session can be a regular user mistyping the password once, the other session is possibly an attack to guess the password. Thus, the sequence lengths are as important as the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgt_darpa = Sgt(kappa = 5, lengthsensitive = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = sgt_darpa.fit_transform(corpus=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2391</th>\n",
       "      <th>2392</th>\n",
       "      <th>2393</th>\n",
       "      <th>2394</th>\n",
       "      <th>2395</th>\n",
       "      <th>2396</th>\n",
       "      <th>2397</th>\n",
       "      <th>2398</th>\n",
       "      <th>2399</th>\n",
       "      <th>2400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069114</td>\n",
       "      <td>0.594892</td>\n",
       "      <td>1.531443e-01</td>\n",
       "      <td>5.846226e-01</td>\n",
       "      <td>2.180165e-01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.804190e-09</td>\n",
       "      <td>7.041516e-10</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.004958e-12</td>\n",
       "      <td>1.316455e-04</td>\n",
       "      <td>1.046458e-07</td>\n",
       "      <td>5.863092e-16</td>\n",
       "      <td>7.568986e-23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.423141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.785666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.950089e-03</td>\n",
       "      <td>2.239981e-04</td>\n",
       "      <td>2.343180e-07</td>\n",
       "      <td>4.388479e-03</td>\n",
       "      <td>3.606850e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173488</td>\n",
       "      <td>4.260410e-03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.966394e-16</td>\n",
       "      <td>5.337432e-01</td>\n",
       "      <td>1.746567e-13</td>\n",
       "      <td>1.667217e-04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1             2             3             4             5     \\\n",
       "0  0.069114  0.594892  1.531443e-01  5.846226e-01  2.180165e-01  0.000000e+00   \n",
       "1  0.000000  0.000000  4.804190e-09  7.041516e-10  0.000000e+00  2.004958e-12   \n",
       "2  0.423141  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "3  0.785666  0.000000  0.000000e+00  0.000000e+00  1.950089e-03  2.239981e-04   \n",
       "4  0.000000  0.173488  4.260410e-03  0.000000e+00  4.966394e-16  5.337432e-01   \n",
       "\n",
       "           6             7             8             9     ...   2391  2392  \\\n",
       "0  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  ...    0.0   0.0   \n",
       "1  1.316455e-04  1.046458e-07  5.863092e-16  7.568986e-23  ...    0.0   0.0   \n",
       "2  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  ...    0.0   0.0   \n",
       "3  2.343180e-07  4.388479e-03  3.606850e-03  0.000000e+00  ...    0.0   0.0   \n",
       "4  1.746567e-13  1.667217e-04  0.000000e+00  0.000000e+00  ...    0.0   0.0   \n",
       "\n",
       "   2393  2394  2395  2396  2397  2398  2399  2400  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 2401 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding).to_csv(path_or_buf='tmp.csv', index=False)\n",
    "pd.DataFrame(embedding).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA on the embeddings\n",
    "The embeddings are sparse. We, therefore, apply PCA on the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9862350164327149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=35)\n",
    "pca.fit(embedding)\n",
    "X = pca.transform(embedding)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Multi-Layer Perceptron Classifier\n",
    "The PCA transforms of the embeddings are used directly as inputs to an MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_156 (Dense)            (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_156 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_87 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_157 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inferno/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_shape=(35,), kernel_initializer=\"uniform\")`\n",
      "/Users/inferno/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "73/73 [==============================] - 4s 55ms/step - loss: 0.1404 - acc: 0.6164\n",
      "Epoch 2/300\n",
      "73/73 [==============================] - 0s 221us/step - loss: 0.1387 - acc: 0.8904\n",
      "Epoch 3/300\n",
      "73/73 [==============================] - 0s 223us/step - loss: 0.1368 - acc: 0.9315\n",
      "Epoch 4/300\n",
      "73/73 [==============================] - 0s 224us/step - loss: 0.1351 - acc: 0.9178\n",
      "Epoch 5/300\n",
      "73/73 [==============================] - 0s 222us/step - loss: 0.1347 - acc: 0.8630\n",
      "Epoch 6/300\n",
      "73/73 [==============================] - 0s 211us/step - loss: 0.1317 - acc: 0.9178\n",
      "Epoch 7/300\n",
      "73/73 [==============================] - 0s 217us/step - loss: 0.1300 - acc: 0.9315\n",
      "Epoch 8/300\n",
      "73/73 [==============================] - 0s 217us/step - loss: 0.1265 - acc: 0.9315\n",
      "Epoch 9/300\n",
      "73/73 [==============================] - 0s 215us/step - loss: 0.1250 - acc: 0.9178\n",
      "Epoch 10/300\n",
      "73/73 [==============================] - 0s 221us/step - loss: 0.1216 - acc: 0.9178\n",
      "Epoch 11/300\n",
      "73/73 [==============================] - 0s 219us/step - loss: 0.1178 - acc: 0.9178\n",
      "Epoch 12/300\n",
      "73/73 [==============================] - 0s 210us/step - loss: 0.1135 - acc: 0.9178\n",
      "Epoch 13/300\n",
      "73/73 [==============================] - 0s 218us/step - loss: 0.1117 - acc: 0.9178\n",
      "Epoch 14/300\n",
      "73/73 [==============================] - 0s 222us/step - loss: 0.1045 - acc: 0.9178\n",
      "Epoch 15/300\n",
      "73/73 [==============================] - 0s 216us/step - loss: 0.1030 - acc: 0.9178\n",
      "Epoch 16/300\n",
      "73/73 [==============================] - 0s 208us/step - loss: 0.0976 - acc: 0.9178\n",
      "Epoch 17/300\n",
      "73/73 [==============================] - 0s 207us/step - loss: 0.0952 - acc: 0.9178\n",
      "Epoch 18/300\n",
      "73/73 [==============================] - 0s 207us/step - loss: 0.0881 - acc: 0.9178\n",
      "Epoch 19/300\n",
      "73/73 [==============================] - 0s 219us/step - loss: 0.0820 - acc: 0.9178\n",
      "Epoch 20/300\n",
      "73/73 [==============================] - 0s 214us/step - loss: 0.0817 - acc: 0.9178\n",
      "Epoch 21/300\n",
      "73/73 [==============================] - 0s 213us/step - loss: 0.0798 - acc: 0.9178\n",
      "Epoch 22/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0628 - acc: 0.933 - 0s 202us/step - loss: 0.0703 - acc: 0.9178\n",
      "Epoch 23/300\n",
      "73/73 [==============================] - 0s 212us/step - loss: 0.0644 - acc: 0.9178\n",
      "Epoch 24/300\n",
      "73/73 [==============================] - 0s 220us/step - loss: 0.0647 - acc: 0.9178\n",
      "Epoch 25/300\n",
      "73/73 [==============================] - 0s 221us/step - loss: 0.0590 - acc: 0.9178\n",
      "Epoch 26/300\n",
      "73/73 [==============================] - 0s 218us/step - loss: 0.0577 - acc: 0.9178\n",
      "Epoch 27/300\n",
      "73/73 [==============================] - 0s 217us/step - loss: 0.0556 - acc: 0.9178\n",
      "Epoch 28/300\n",
      "73/73 [==============================] - 0s 213us/step - loss: 0.0524 - acc: 0.9178\n",
      "Epoch 29/300\n",
      "73/73 [==============================] - 0s 259us/step - loss: 0.0526 - acc: 0.9178\n",
      "Epoch 30/300\n",
      "73/73 [==============================] - 0s 271us/step - loss: 0.0476 - acc: 0.9178\n",
      "Epoch 31/300\n",
      "73/73 [==============================] - 0s 265us/step - loss: 0.0434 - acc: 0.9178\n",
      "Epoch 32/300\n",
      "73/73 [==============================] - 0s 232us/step - loss: 0.0405 - acc: 0.9178\n",
      "Epoch 33/300\n",
      "73/73 [==============================] - 0s 252us/step - loss: 0.0416 - acc: 0.9178\n",
      "Epoch 34/300\n",
      "73/73 [==============================] - 0s 219us/step - loss: 0.0375 - acc: 0.9178\n",
      "Epoch 35/300\n",
      "73/73 [==============================] - 0s 272us/step - loss: 0.0402 - acc: 0.9178\n",
      "Epoch 36/300\n",
      "73/73 [==============================] - 0s 232us/step - loss: 0.0376 - acc: 0.9178\n",
      "Epoch 37/300\n",
      "73/73 [==============================] - 0s 221us/step - loss: 0.0323 - acc: 0.9178\n",
      "Epoch 38/300\n",
      "73/73 [==============================] - 0s 216us/step - loss: 0.0374 - acc: 0.9178\n",
      "Epoch 39/300\n",
      "73/73 [==============================] - 0s 216us/step - loss: 0.0341 - acc: 0.9178\n",
      "Epoch 40/300\n",
      "73/73 [==============================] - 0s 215us/step - loss: 0.0341 - acc: 0.9178\n",
      "Epoch 41/300\n",
      "73/73 [==============================] - 0s 217us/step - loss: 0.0339 - acc: 0.9178\n",
      "Epoch 42/300\n",
      "73/73 [==============================] - 0s 229us/step - loss: 0.0345 - acc: 0.9178\n",
      "Epoch 43/300\n",
      "73/73 [==============================] - 0s 227us/step - loss: 0.0298 - acc: 0.9178\n",
      "Epoch 44/300\n",
      "73/73 [==============================] - 0s 188us/step - loss: 0.0319 - acc: 0.9178\n",
      "Epoch 45/300\n",
      "73/73 [==============================] - 0s 188us/step - loss: 0.0261 - acc: 0.9178\n",
      "Epoch 46/300\n",
      "73/73 [==============================] - 0s 203us/step - loss: 0.0273 - acc: 0.9178\n",
      "Epoch 47/300\n",
      "73/73 [==============================] - 0s 195us/step - loss: 0.0308 - acc: 0.9178\n",
      "Epoch 48/300\n",
      "73/73 [==============================] - 0s 193us/step - loss: 0.0288 - acc: 0.9178\n",
      "Epoch 49/300\n",
      "73/73 [==============================] - 0s 172us/step - loss: 0.0297 - acc: 0.9178\n",
      "Epoch 50/300\n",
      "73/73 [==============================] - 0s 170us/step - loss: 0.0262 - acc: 0.9178\n",
      "Epoch 51/300\n",
      "73/73 [==============================] - 0s 158us/step - loss: 0.0263 - acc: 0.9178\n",
      "Epoch 52/300\n",
      "73/73 [==============================] - 0s 183us/step - loss: 0.0251 - acc: 0.9178\n",
      "Epoch 53/300\n",
      "73/73 [==============================] - 0s 215us/step - loss: 0.0268 - acc: 0.9178\n",
      "Epoch 54/300\n",
      "73/73 [==============================] - 0s 204us/step - loss: 0.0271 - acc: 0.9178\n",
      "Epoch 55/300\n",
      "73/73 [==============================] - 0s 207us/step - loss: 0.0292 - acc: 0.9178\n",
      "Epoch 56/300\n",
      "73/73 [==============================] - 0s 199us/step - loss: 0.0240 - acc: 0.9178\n",
      "Epoch 57/300\n",
      "73/73 [==============================] - 0s 186us/step - loss: 0.0270 - acc: 0.9178\n",
      "Epoch 58/300\n",
      "73/73 [==============================] - 0s 182us/step - loss: 0.0245 - acc: 0.9178\n",
      "Epoch 59/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0270 - acc: 0.9178\n",
      "Epoch 60/300\n",
      "73/73 [==============================] - 0s 174us/step - loss: 0.0272 - acc: 0.9178\n",
      "Epoch 61/300\n",
      "73/73 [==============================] - 0s 173us/step - loss: 0.0249 - acc: 0.9178\n",
      "Epoch 62/300\n",
      "73/73 [==============================] - 0s 190us/step - loss: 0.0243 - acc: 0.9178\n",
      "Epoch 63/300\n",
      "73/73 [==============================] - 0s 173us/step - loss: 0.0236 - acc: 0.9178\n",
      "Epoch 64/300\n",
      "73/73 [==============================] - 0s 168us/step - loss: 0.0258 - acc: 0.9178\n",
      "Epoch 65/300\n",
      "73/73 [==============================] - 0s 170us/step - loss: 0.0244 - acc: 0.9178\n",
      "Epoch 66/300\n",
      "73/73 [==============================] - 0s 207us/step - loss: 0.0242 - acc: 0.9178\n",
      "Epoch 67/300\n",
      "73/73 [==============================] - 0s 183us/step - loss: 0.0239 - acc: 0.9178\n",
      "Epoch 68/300\n",
      "73/73 [==============================] - 0s 174us/step - loss: 0.0246 - acc: 0.9178\n",
      "Epoch 69/300\n",
      "73/73 [==============================] - 0s 188us/step - loss: 0.0231 - acc: 0.9178\n",
      "Epoch 70/300\n",
      "73/73 [==============================] - 0s 184us/step - loss: 0.0240 - acc: 0.9178\n",
      "Epoch 71/300\n",
      "73/73 [==============================] - 0s 183us/step - loss: 0.0254 - acc: 0.9178\n",
      "Epoch 72/300\n",
      "73/73 [==============================] - 0s 196us/step - loss: 0.0243 - acc: 0.9178\n",
      "Epoch 73/300\n",
      "73/73 [==============================] - 0s 187us/step - loss: 0.0248 - acc: 0.9178\n",
      "Epoch 74/300\n",
      "73/73 [==============================] - 0s 198us/step - loss: 0.0212 - acc: 0.9178\n",
      "Epoch 75/300\n",
      "73/73 [==============================] - 0s 179us/step - loss: 0.0218 - acc: 0.9178\n",
      "Epoch 76/300\n",
      "73/73 [==============================] - 0s 184us/step - loss: 0.0247 - acc: 0.9178\n",
      "Epoch 77/300\n",
      "73/73 [==============================] - 0s 176us/step - loss: 0.0247 - acc: 0.9178\n",
      "Epoch 78/300\n",
      "73/73 [==============================] - 0s 172us/step - loss: 0.0235 - acc: 0.9178\n",
      "Epoch 79/300\n",
      "73/73 [==============================] - 0s 176us/step - loss: 0.0222 - acc: 0.9178\n",
      "Epoch 80/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0242 - acc: 0.9178\n",
      "Epoch 81/300\n",
      "73/73 [==============================] - 0s 189us/step - loss: 0.0229 - acc: 0.9178\n",
      "Epoch 82/300\n",
      "73/73 [==============================] - 0s 193us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 83/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0238 - acc: 0.9178\n",
      "Epoch 84/300\n",
      "73/73 [==============================] - 0s 167us/step - loss: 0.0212 - acc: 0.9178\n",
      "Epoch 85/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 165us/step - loss: 0.0237 - acc: 0.9178\n",
      "Epoch 86/300\n",
      "73/73 [==============================] - 0s 175us/step - loss: 0.0203 - acc: 0.9178\n",
      "Epoch 87/300\n",
      "73/73 [==============================] - 0s 169us/step - loss: 0.0223 - acc: 0.9178\n",
      "Epoch 88/300\n",
      "73/73 [==============================] - 0s 173us/step - loss: 0.0205 - acc: 0.9178\n",
      "Epoch 89/300\n",
      "73/73 [==============================] - 0s 170us/step - loss: 0.0215 - acc: 0.9178\n",
      "Epoch 90/300\n",
      "73/73 [==============================] - 0s 164us/step - loss: 0.0204 - acc: 0.9178\n",
      "Epoch 91/300\n",
      "73/73 [==============================] - 0s 174us/step - loss: 0.0219 - acc: 0.9178\n",
      "Epoch 92/300\n",
      "73/73 [==============================] - 0s 161us/step - loss: 0.0238 - acc: 0.9178\n",
      "Epoch 93/300\n",
      "73/73 [==============================] - 0s 163us/step - loss: 0.0242 - acc: 0.9178\n",
      "Epoch 94/300\n",
      "73/73 [==============================] - 0s 159us/step - loss: 0.0221 - acc: 0.9178\n",
      "Epoch 95/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0207 - acc: 0.9178\n",
      "Epoch 96/300\n",
      "73/73 [==============================] - 0s 226us/step - loss: 0.0252 - acc: 0.9178\n",
      "Epoch 97/300\n",
      "73/73 [==============================] - 0s 251us/step - loss: 0.0252 - acc: 0.9178\n",
      "Epoch 98/300\n",
      "73/73 [==============================] - 0s 257us/step - loss: 0.0227 - acc: 0.9178\n",
      "Epoch 99/300\n",
      "73/73 [==============================] - 0s 251us/step - loss: 0.0214 - acc: 0.9178\n",
      "Epoch 100/300\n",
      "73/73 [==============================] - 0s 235us/step - loss: 0.0235 - acc: 0.9178\n",
      "Epoch 101/300\n",
      "73/73 [==============================] - 0s 247us/step - loss: 0.0207 - acc: 0.9178\n",
      "Epoch 102/300\n",
      "73/73 [==============================] - 0s 266us/step - loss: 0.0201 - acc: 0.9178\n",
      "Epoch 103/300\n",
      "73/73 [==============================] - 0s 206us/step - loss: 0.0211 - acc: 0.9178\n",
      "Epoch 104/300\n",
      "73/73 [==============================] - 0s 262us/step - loss: 0.0193 - acc: 0.9178\n",
      "Epoch 105/300\n",
      "73/73 [==============================] - 0s 270us/step - loss: 0.0218 - acc: 0.9178\n",
      "Epoch 106/300\n",
      "73/73 [==============================] - 0s 212us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 107/300\n",
      "73/73 [==============================] - 0s 248us/step - loss: 0.0214 - acc: 0.9178\n",
      "Epoch 108/300\n",
      "73/73 [==============================] - 0s 228us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 109/300\n",
      "73/73 [==============================] - 0s 237us/step - loss: 0.0236 - acc: 0.9178\n",
      "Epoch 110/300\n",
      "73/73 [==============================] - 0s 236us/step - loss: 0.0226 - acc: 0.9178\n",
      "Epoch 111/300\n",
      "73/73 [==============================] - 0s 223us/step - loss: 0.0234 - acc: 0.9178\n",
      "Epoch 112/300\n",
      "73/73 [==============================] - 0s 245us/step - loss: 0.0226 - acc: 0.9178\n",
      "Epoch 113/300\n",
      "73/73 [==============================] - 0s 244us/step - loss: 0.0208 - acc: 0.9178\n",
      "Epoch 114/300\n",
      "73/73 [==============================] - 0s 246us/step - loss: 0.0218 - acc: 0.9178\n",
      "Epoch 115/300\n",
      "73/73 [==============================] - 0s 220us/step - loss: 0.0207 - acc: 0.9178\n",
      "Epoch 116/300\n",
      "73/73 [==============================] - 0s 224us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 117/300\n",
      "73/73 [==============================] - 0s 221us/step - loss: 0.0223 - acc: 0.9178\n",
      "Epoch 118/300\n",
      "73/73 [==============================] - 0s 212us/step - loss: 0.0203 - acc: 0.9178\n",
      "Epoch 119/300\n",
      "73/73 [==============================] - 0s 242us/step - loss: 0.0214 - acc: 0.9178\n",
      "Epoch 120/300\n",
      "73/73 [==============================] - 0s 200us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 121/300\n",
      "73/73 [==============================] - 0s 195us/step - loss: 0.0205 - acc: 0.9178\n",
      "Epoch 122/300\n",
      "73/73 [==============================] - 0s 194us/step - loss: 0.0198 - acc: 0.9178\n",
      "Epoch 123/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0215 - acc: 0.9178\n",
      "Epoch 124/300\n",
      "73/73 [==============================] - 0s 171us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 125/300\n",
      "73/73 [==============================] - 0s 200us/step - loss: 0.0198 - acc: 0.9178\n",
      "Epoch 126/300\n",
      "73/73 [==============================] - 0s 194us/step - loss: 0.0224 - acc: 0.9178\n",
      "Epoch 127/300\n",
      "73/73 [==============================] - 0s 201us/step - loss: 0.0212 - acc: 0.9178\n",
      "Epoch 128/300\n",
      "73/73 [==============================] - 0s 200us/step - loss: 0.0219 - acc: 0.9178\n",
      "Epoch 129/300\n",
      "73/73 [==============================] - 0s 187us/step - loss: 0.0218 - acc: 0.9178\n",
      "Epoch 130/300\n",
      "73/73 [==============================] - 0s 195us/step - loss: 0.0214 - acc: 0.9178\n",
      "Epoch 131/300\n",
      "73/73 [==============================] - 0s 179us/step - loss: 0.0204 - acc: 0.9178\n",
      "Epoch 132/300\n",
      "73/73 [==============================] - 0s 189us/step - loss: 0.0244 - acc: 0.9178\n",
      "Epoch 133/300\n",
      "73/73 [==============================] - 0s 183us/step - loss: 0.0204 - acc: 0.9178\n",
      "Epoch 134/300\n",
      "73/73 [==============================] - 0s 194us/step - loss: 0.0193 - acc: 0.9178\n",
      "Epoch 135/300\n",
      "73/73 [==============================] - 0s 236us/step - loss: 0.0194 - acc: 0.9178\n",
      "Epoch 136/300\n",
      "73/73 [==============================] - 0s 212us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 137/300\n",
      "73/73 [==============================] - 0s 205us/step - loss: 0.0221 - acc: 0.9178\n",
      "Epoch 138/300\n",
      "73/73 [==============================] - 0s 207us/step - loss: 0.0221 - acc: 0.9178\n",
      "Epoch 139/300\n",
      "73/73 [==============================] - 0s 215us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 140/300\n",
      "73/73 [==============================] - 0s 194us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 141/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0203 - acc: 0.9178\n",
      "Epoch 142/300\n",
      "73/73 [==============================] - 0s 164us/step - loss: 0.0226 - acc: 0.9178\n",
      "Epoch 143/300\n",
      "73/73 [==============================] - 0s 174us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 144/300\n",
      "73/73 [==============================] - 0s 174us/step - loss: 0.0204 - acc: 0.9178\n",
      "Epoch 145/300\n",
      "73/73 [==============================] - 0s 173us/step - loss: 0.0246 - acc: 0.9178\n",
      "Epoch 146/300\n",
      "73/73 [==============================] - 0s 216us/step - loss: 0.0243 - acc: 0.9178\n",
      "Epoch 147/300\n",
      "73/73 [==============================] - 0s 214us/step - loss: 0.0204 - acc: 0.9178\n",
      "Epoch 148/300\n",
      "73/73 [==============================] - 0s 223us/step - loss: 0.0231 - acc: 0.9178\n",
      "Epoch 149/300\n",
      "73/73 [==============================] - 0s 239us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 150/300\n",
      "73/73 [==============================] - 0s 234us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 151/300\n",
      "73/73 [==============================] - 0s 222us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 152/300\n",
      "73/73 [==============================] - 0s 219us/step - loss: 0.0202 - acc: 0.9178\n",
      "Epoch 153/300\n",
      "73/73 [==============================] - 0s 220us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 154/300\n",
      "73/73 [==============================] - 0s 230us/step - loss: 0.0198 - acc: 0.9178\n",
      "Epoch 155/300\n",
      "73/73 [==============================] - 0s 238us/step - loss: 0.0235 - acc: 0.9178\n",
      "Epoch 156/300\n",
      "73/73 [==============================] - 0s 208us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 157/300\n",
      "73/73 [==============================] - 0s 224us/step - loss: 0.0223 - acc: 0.9178\n",
      "Epoch 158/300\n",
      "73/73 [==============================] - 0s 218us/step - loss: 0.0255 - acc: 0.9178\n",
      "Epoch 159/300\n",
      "73/73 [==============================] - 0s 219us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 160/300\n",
      "73/73 [==============================] - 0s 258us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 161/300\n",
      "73/73 [==============================] - 0s 231us/step - loss: 0.0211 - acc: 0.9178\n",
      "Epoch 162/300\n",
      "73/73 [==============================] - 0s 226us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 163/300\n",
      "73/73 [==============================] - 0s 220us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 164/300\n",
      "73/73 [==============================] - 0s 214us/step - loss: 0.0238 - acc: 0.9178\n",
      "Epoch 165/300\n",
      "73/73 [==============================] - 0s 233us/step - loss: 0.0231 - acc: 0.9178\n",
      "Epoch 166/300\n",
      "73/73 [==============================] - 0s 219us/step - loss: 0.0215 - acc: 0.9178\n",
      "Epoch 167/300\n",
      "73/73 [==============================] - 0s 232us/step - loss: 0.0213 - acc: 0.9178\n",
      "Epoch 168/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 206us/step - loss: 0.0203 - acc: 0.9178\n",
      "Epoch 169/300\n",
      "73/73 [==============================] - 0s 173us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 170/300\n",
      "73/73 [==============================] - 0s 225us/step - loss: 0.0189 - acc: 0.9178\n",
      "Epoch 171/300\n",
      "73/73 [==============================] - 0s 222us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 172/300\n",
      "73/73 [==============================] - 0s 222us/step - loss: 0.0208 - acc: 0.9178\n",
      "Epoch 173/300\n",
      "73/73 [==============================] - 0s 234us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 174/300\n",
      "73/73 [==============================] - 0s 235us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 175/300\n",
      "73/73 [==============================] - 0s 226us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 176/300\n",
      "73/73 [==============================] - 0s 220us/step - loss: 0.0215 - acc: 0.9178\n",
      "Epoch 177/300\n",
      "73/73 [==============================] - 0s 223us/step - loss: 0.0228 - acc: 0.9178\n",
      "Epoch 178/300\n",
      "73/73 [==============================] - 0s 218us/step - loss: 0.0238 - acc: 0.9178\n",
      "Epoch 179/300\n",
      "73/73 [==============================] - 0s 233us/step - loss: 0.0226 - acc: 0.9178\n",
      "Epoch 180/300\n",
      "73/73 [==============================] - 0s 229us/step - loss: 0.0225 - acc: 0.9178\n",
      "Epoch 181/300\n",
      "73/73 [==============================] - 0s 220us/step - loss: 0.0218 - acc: 0.9178\n",
      "Epoch 182/300\n",
      "73/73 [==============================] - 0s 222us/step - loss: 0.0233 - acc: 0.9178\n",
      "Epoch 183/300\n",
      "73/73 [==============================] - 0s 202us/step - loss: 0.0207 - acc: 0.9178\n",
      "Epoch 184/300\n",
      "73/73 [==============================] - 0s 190us/step - loss: 0.0223 - acc: 0.9178\n",
      "Epoch 185/300\n",
      "73/73 [==============================] - 0s 183us/step - loss: 0.0213 - acc: 0.9178\n",
      "Epoch 186/300\n",
      "73/73 [==============================] - 0s 180us/step - loss: 0.0233 - acc: 0.9178\n",
      "Epoch 187/300\n",
      "73/73 [==============================] - 0s 182us/step - loss: 0.0202 - acc: 0.9178\n",
      "Epoch 188/300\n",
      "73/73 [==============================] - 0s 186us/step - loss: 0.0214 - acc: 0.9178\n",
      "Epoch 189/300\n",
      "73/73 [==============================] - 0s 164us/step - loss: 0.0217 - acc: 0.9178\n",
      "Epoch 190/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0219 - acc: 0.9178\n",
      "Epoch 191/300\n",
      "73/73 [==============================] - 0s 187us/step - loss: 0.0208 - acc: 0.9178\n",
      "Epoch 192/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0222 - acc: 0.9178\n",
      "Epoch 193/300\n",
      "73/73 [==============================] - 0s 182us/step - loss: 0.0201 - acc: 0.9178\n",
      "Epoch 194/300\n",
      "73/73 [==============================] - 0s 189us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 195/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0214 - acc: 0.9178\n",
      "Epoch 196/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0196 - acc: 0.9178\n",
      "Epoch 197/300\n",
      "73/73 [==============================] - 0s 172us/step - loss: 0.0217 - acc: 0.9178\n",
      "Epoch 198/300\n",
      "73/73 [==============================] - 0s 180us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 199/300\n",
      "73/73 [==============================] - 0s 180us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 200/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0282 - acc: 0.800 - 0s 182us/step - loss: 0.0208 - acc: 0.9178\n",
      "Epoch 201/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0212 - acc: 0.9178\n",
      "Epoch 202/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 203/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 204/300\n",
      "73/73 [==============================] - 0s 167us/step - loss: 0.0204 - acc: 0.9178\n",
      "Epoch 205/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0192 - acc: 0.9178\n",
      "Epoch 206/300\n",
      "73/73 [==============================] - 0s 175us/step - loss: 0.0211 - acc: 0.9178\n",
      "Epoch 207/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 208/300\n",
      "73/73 [==============================] - 0s 166us/step - loss: 0.0203 - acc: 0.9178\n",
      "Epoch 209/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0230 - acc: 0.9178\n",
      "Epoch 210/300\n",
      "73/73 [==============================] - 0s 171us/step - loss: 0.0199 - acc: 0.9178\n",
      "Epoch 211/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0109 - acc: 0.933 - 0s 166us/step - loss: 0.0195 - acc: 0.9178\n",
      "Epoch 212/300\n",
      "73/73 [==============================] - 0s 176us/step - loss: 0.0195 - acc: 0.9178\n",
      "Epoch 213/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0202 - acc: 0.9178\n",
      "Epoch 214/300\n",
      "73/73 [==============================] - 0s 170us/step - loss: 0.0194 - acc: 0.9178\n",
      "Epoch 215/300\n",
      "73/73 [==============================] - 0s 167us/step - loss: 0.0222 - acc: 0.9178\n",
      "Epoch 216/300\n",
      "73/73 [==============================] - 0s 170us/step - loss: 0.0218 - acc: 0.9178\n",
      "Epoch 217/300\n",
      "73/73 [==============================] - 0s 169us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 218/300\n",
      "73/73 [==============================] - 0s 208us/step - loss: 0.0221 - acc: 0.9178\n",
      "Epoch 219/300\n",
      "73/73 [==============================] - 0s 165us/step - loss: 0.0213 - acc: 0.9178\n",
      "Epoch 220/300\n",
      "73/73 [==============================] - 0s 172us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 221/300\n",
      "73/73 [==============================] - 0s 186us/step - loss: 0.0223 - acc: 0.9178\n",
      "Epoch 222/300\n",
      "73/73 [==============================] - 0s 192us/step - loss: 0.0187 - acc: 0.9178\n",
      "Epoch 223/300\n",
      "73/73 [==============================] - 0s 193us/step - loss: 0.0200 - acc: 0.9178\n",
      "Epoch 224/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 225/300\n",
      "73/73 [==============================] - 0s 184us/step - loss: 0.0201 - acc: 0.9178\n",
      "Epoch 226/300\n",
      "73/73 [==============================] - 0s 170us/step - loss: 0.0212 - acc: 0.9178\n",
      "Epoch 227/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 228/300\n",
      "73/73 [==============================] - 0s 177us/step - loss: 0.0203 - acc: 0.9178\n",
      "Epoch 229/300\n",
      "73/73 [==============================] - 0s 182us/step - loss: 0.0197 - acc: 0.9178\n",
      "Epoch 230/300\n",
      "73/73 [==============================] - 0s 169us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 231/300\n",
      "73/73 [==============================] - 0s 172us/step - loss: 0.0187 - acc: 0.9178\n",
      "Epoch 232/300\n",
      "73/73 [==============================] - 0s 175us/step - loss: 0.0210 - acc: 0.9178\n",
      "Epoch 233/300\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.0441 - acc: 0.866 - 0s 176us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 234/300\n",
      "73/73 [==============================] - 0s 174us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 235/300\n",
      "73/73 [==============================] - 0s 188us/step - loss: 0.0197 - acc: 0.9178\n",
      "Epoch 236/300\n",
      "73/73 [==============================] - 0s 182us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 237/300\n",
      "73/73 [==============================] - 0s 182us/step - loss: 0.0184 - acc: 0.9178\n",
      "Epoch 238/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0190 - acc: 0.9178\n",
      "Epoch 239/300\n",
      "73/73 [==============================] - 0s 177us/step - loss: 0.0219 - acc: 0.9178\n",
      "Epoch 240/300\n",
      "73/73 [==============================] - 0s 172us/step - loss: 0.0197 - acc: 0.9178\n",
      "Epoch 241/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 242/300\n",
      "73/73 [==============================] - 0s 170us/step - loss: 0.0241 - acc: 0.9178\n",
      "Epoch 243/300\n",
      "73/73 [==============================] - 0s 173us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 244/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0221 - acc: 0.9178\n",
      "Epoch 245/300\n",
      "73/73 [==============================] - 0s 178us/step - loss: 0.0200 - acc: 0.9178\n",
      "Epoch 246/300\n",
      "73/73 [==============================] - 0s 225us/step - loss: 0.0220 - acc: 0.9178\n",
      "Epoch 247/300\n",
      "73/73 [==============================] - 0s 198us/step - loss: 0.0222 - acc: 0.9178\n",
      "Epoch 248/300\n",
      "73/73 [==============================] - 0s 171us/step - loss: 0.0223 - acc: 0.9178\n",
      "Epoch 249/300\n",
      "73/73 [==============================] - 0s 167us/step - loss: 0.0215 - acc: 0.9178\n",
      "Epoch 250/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 190us/step - loss: 0.0212 - acc: 0.9178\n",
      "Epoch 251/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0215 - acc: 0.9178\n",
      "Epoch 252/300\n",
      "73/73 [==============================] - 0s 184us/step - loss: 0.0215 - acc: 0.9178\n",
      "Epoch 253/300\n",
      "73/73 [==============================] - 0s 176us/step - loss: 0.0200 - acc: 0.9178\n",
      "Epoch 254/300\n",
      "73/73 [==============================] - 0s 173us/step - loss: 0.0193 - acc: 0.9178\n",
      "Epoch 255/300\n",
      "73/73 [==============================] - 0s 166us/step - loss: 0.0184 - acc: 0.9178\n",
      "Epoch 256/300\n",
      "73/73 [==============================] - 0s 158us/step - loss: 0.0199 - acc: 0.9178\n",
      "Epoch 257/300\n",
      "73/73 [==============================] - 0s 177us/step - loss: 0.0229 - acc: 0.9178\n",
      "Epoch 258/300\n",
      "73/73 [==============================] - 0s 168us/step - loss: 0.0198 - acc: 0.9178\n",
      "Epoch 259/300\n",
      "73/73 [==============================] - 0s 173us/step - loss: 0.0212 - acc: 0.9178\n",
      "Epoch 260/300\n",
      "73/73 [==============================] - 0s 172us/step - loss: 0.0208 - acc: 0.9178\n",
      "Epoch 261/300\n",
      "73/73 [==============================] - 0s 161us/step - loss: 0.0227 - acc: 0.9178\n",
      "Epoch 262/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0208 - acc: 0.9178\n",
      "Epoch 263/300\n",
      "73/73 [==============================] - 0s 189us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 264/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0201 - acc: 0.9178\n",
      "Epoch 265/300\n",
      "73/73 [==============================] - 0s 189us/step - loss: 0.0184 - acc: 0.9178\n",
      "Epoch 266/300\n",
      "73/73 [==============================] - 0s 208us/step - loss: 0.0216 - acc: 0.9178\n",
      "Epoch 267/300\n",
      "73/73 [==============================] - 0s 187us/step - loss: 0.0194 - acc: 0.9178\n",
      "Epoch 268/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0197 - acc: 0.9178\n",
      "Epoch 269/300\n",
      "73/73 [==============================] - 0s 177us/step - loss: 0.0197 - acc: 0.9178\n",
      "Epoch 270/300\n",
      "73/73 [==============================] - 0s 161us/step - loss: 0.0240 - acc: 0.9178\n",
      "Epoch 271/300\n",
      "73/73 [==============================] - 0s 185us/step - loss: 0.0203 - acc: 0.9178\n",
      "Epoch 272/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0219 - acc: 0.9178\n",
      "Epoch 273/300\n",
      "73/73 [==============================] - 0s 190us/step - loss: 0.0195 - acc: 0.9178\n",
      "Epoch 274/300\n",
      "73/73 [==============================] - 0s 188us/step - loss: 0.0222 - acc: 0.9178\n",
      "Epoch 275/300\n",
      "73/73 [==============================] - 0s 175us/step - loss: 0.0211 - acc: 0.9178\n",
      "Epoch 276/300\n",
      "73/73 [==============================] - 0s 159us/step - loss: 0.0246 - acc: 0.9178\n",
      "Epoch 277/300\n",
      "73/73 [==============================] - 0s 171us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 278/300\n",
      "73/73 [==============================] - 0s 189us/step - loss: 0.0211 - acc: 0.9178\n",
      "Epoch 279/300\n",
      "73/73 [==============================] - 0s 174us/step - loss: 0.0221 - acc: 0.9178\n",
      "Epoch 280/300\n",
      "73/73 [==============================] - 0s 182us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 281/300\n",
      "73/73 [==============================] - 0s 181us/step - loss: 0.0217 - acc: 0.9178\n",
      "Epoch 282/300\n",
      "73/73 [==============================] - 0s 189us/step - loss: 0.0183 - acc: 0.9178\n",
      "Epoch 283/300\n",
      "73/73 [==============================] - 0s 198us/step - loss: 0.0196 - acc: 0.9178\n",
      "Epoch 284/300\n",
      "73/73 [==============================] - 0s 195us/step - loss: 0.0184 - acc: 0.9178\n",
      "Epoch 285/300\n",
      "73/73 [==============================] - 0s 188us/step - loss: 0.0205 - acc: 0.9178\n",
      "Epoch 286/300\n",
      "73/73 [==============================] - 0s 198us/step - loss: 0.0218 - acc: 0.9178\n",
      "Epoch 287/300\n",
      "73/73 [==============================] - 0s 198us/step - loss: 0.0218 - acc: 0.9178\n",
      "Epoch 288/300\n",
      "73/73 [==============================] - 0s 166us/step - loss: 0.0192 - acc: 0.9178\n",
      "Epoch 289/300\n",
      "73/73 [==============================] - 0s 197us/step - loss: 0.0212 - acc: 0.9178\n",
      "Epoch 290/300\n",
      "73/73 [==============================] - 0s 187us/step - loss: 0.0226 - acc: 0.9178\n",
      "Epoch 291/300\n",
      "73/73 [==============================] - 0s 186us/step - loss: 0.0190 - acc: 0.9178\n",
      "Epoch 292/300\n",
      "73/73 [==============================] - 0s 188us/step - loss: 0.0209 - acc: 0.9178\n",
      "Epoch 293/300\n",
      "73/73 [==============================] - 0s 187us/step - loss: 0.0206 - acc: 0.9178\n",
      "Epoch 294/300\n",
      "73/73 [==============================] - 0s 205us/step - loss: 0.0214 - acc: 0.9178\n",
      "Epoch 295/300\n",
      "73/73 [==============================] - 0s 182us/step - loss: 0.0192 - acc: 0.9178\n",
      "Epoch 296/300\n",
      "73/73 [==============================] - 0s 176us/step - loss: 0.0237 - acc: 0.9178\n",
      "Epoch 297/300\n",
      "73/73 [==============================] - 0s 166us/step - loss: 0.0217 - acc: 0.9178\n",
      "Epoch 298/300\n",
      "73/73 [==============================] - 0s 184us/step - loss: 0.0194 - acc: 0.9178\n",
      "Epoch 299/300\n",
      "73/73 [==============================] - 0s 171us/step - loss: 0.0207 - acc: 0.9178\n",
      "Epoch 300/300\n",
      "73/73 [==============================] - 0s 174us/step - loss: 0.0197 - acc: 0.9178\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_158 (Dense)            (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_158 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_88 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_159 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.1470 - acc: 0.3378\n",
      "Epoch 2/300\n",
      "74/74 [==============================] - 0s 211us/step - loss: 0.1453 - acc: 0.5405\n",
      "Epoch 3/300\n",
      "74/74 [==============================] - 0s 182us/step - loss: 0.1428 - acc: 0.6622\n",
      "Epoch 4/300\n",
      "74/74 [==============================] - 0s 178us/step - loss: 0.1426 - acc: 0.7432\n",
      "Epoch 5/300\n",
      "74/74 [==============================] - 0s 170us/step - loss: 0.1405 - acc: 0.8243\n",
      "Epoch 6/300\n",
      "74/74 [==============================] - 0s 177us/step - loss: 0.1383 - acc: 0.8243\n",
      "Epoch 7/300\n",
      "74/74 [==============================] - 0s 179us/step - loss: 0.1364 - acc: 0.8243\n",
      "Epoch 8/300\n",
      "74/74 [==============================] - 0s 169us/step - loss: 0.1337 - acc: 0.7973\n",
      "Epoch 9/300\n",
      "74/74 [==============================] - 0s 183us/step - loss: 0.1301 - acc: 0.8378\n",
      "Epoch 10/300\n",
      "74/74 [==============================] - 0s 179us/step - loss: 0.1282 - acc: 0.8784\n",
      "Epoch 11/300\n",
      "74/74 [==============================] - 0s 166us/step - loss: 0.1250 - acc: 0.8243\n",
      "Epoch 12/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.1215 - acc: 0.8378\n",
      "Epoch 13/300\n",
      "74/74 [==============================] - 0s 173us/step - loss: 0.1189 - acc: 0.8378\n",
      "Epoch 14/300\n",
      "74/74 [==============================] - 0s 171us/step - loss: 0.1122 - acc: 0.8514\n",
      "Epoch 15/300\n",
      "74/74 [==============================] - 0s 167us/step - loss: 0.1095 - acc: 0.8514\n",
      "Epoch 16/300\n",
      "74/74 [==============================] - 0s 171us/step - loss: 0.1067 - acc: 0.8514\n",
      "Epoch 17/300\n",
      "74/74 [==============================] - 0s 165us/step - loss: 0.1020 - acc: 0.8649\n",
      "Epoch 18/300\n",
      "74/74 [==============================] - 0s 180us/step - loss: 0.0977 - acc: 0.8514\n",
      "Epoch 19/300\n",
      "74/74 [==============================] - 0s 182us/step - loss: 0.0936 - acc: 0.8514\n",
      "Epoch 20/300\n",
      "74/74 [==============================] - 0s 172us/step - loss: 0.0898 - acc: 0.8649\n",
      "Epoch 21/300\n",
      "74/74 [==============================] - 0s 166us/step - loss: 0.0839 - acc: 0.8784\n",
      "Epoch 22/300\n",
      "74/74 [==============================] - 0s 177us/step - loss: 0.0815 - acc: 0.8649\n",
      "Epoch 23/300\n",
      "74/74 [==============================] - 0s 173us/step - loss: 0.0760 - acc: 0.8784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/300\n",
      "74/74 [==============================] - 0s 214us/step - loss: 0.0759 - acc: 0.8784\n",
      "Epoch 25/300\n",
      "74/74 [==============================] - 0s 211us/step - loss: 0.0703 - acc: 0.8784\n",
      "Epoch 26/300\n",
      "74/74 [==============================] - 0s 199us/step - loss: 0.0663 - acc: 0.8784\n",
      "Epoch 27/300\n",
      "74/74 [==============================] - 0s 192us/step - loss: 0.0659 - acc: 0.8784\n",
      "Epoch 28/300\n",
      "74/74 [==============================] - 0s 215us/step - loss: 0.0609 - acc: 0.8919\n",
      "Epoch 29/300\n",
      "74/74 [==============================] - 0s 230us/step - loss: 0.0592 - acc: 0.8784\n",
      "Epoch 30/300\n",
      "74/74 [==============================] - 0s 239us/step - loss: 0.0576 - acc: 0.8919\n",
      "Epoch 31/300\n",
      "74/74 [==============================] - 0s 225us/step - loss: 0.0521 - acc: 0.8919\n",
      "Epoch 32/300\n",
      "74/74 [==============================] - 0s 233us/step - loss: 0.0530 - acc: 0.8784\n",
      "Epoch 33/300\n",
      "74/74 [==============================] - 0s 231us/step - loss: 0.0462 - acc: 0.8784\n",
      "Epoch 34/300\n",
      "74/74 [==============================] - 0s 214us/step - loss: 0.0478 - acc: 0.8919\n",
      "Epoch 35/300\n",
      "74/74 [==============================] - 0s 216us/step - loss: 0.0472 - acc: 0.8919\n",
      "Epoch 36/300\n",
      "74/74 [==============================] - 0s 230us/step - loss: 0.0480 - acc: 0.8919\n",
      "Epoch 37/300\n",
      "74/74 [==============================] - 0s 227us/step - loss: 0.0438 - acc: 0.8919\n",
      "Epoch 38/300\n",
      "74/74 [==============================] - 0s 214us/step - loss: 0.0482 - acc: 0.8919\n",
      "Epoch 39/300\n",
      "74/74 [==============================] - 0s 226us/step - loss: 0.0426 - acc: 0.8919\n",
      "Epoch 40/300\n",
      "74/74 [==============================] - 0s 228us/step - loss: 0.0408 - acc: 0.8919\n",
      "Epoch 41/300\n",
      "74/74 [==============================] - 0s 228us/step - loss: 0.0427 - acc: 0.8784\n",
      "Epoch 42/300\n",
      "74/74 [==============================] - 0s 206us/step - loss: 0.0405 - acc: 0.8919\n",
      "Epoch 43/300\n",
      "74/74 [==============================] - 0s 204us/step - loss: 0.0423 - acc: 0.8919\n",
      "Epoch 44/300\n",
      "74/74 [==============================] - 0s 199us/step - loss: 0.0396 - acc: 0.8919\n",
      "Epoch 45/300\n",
      "74/74 [==============================] - 0s 191us/step - loss: 0.0391 - acc: 0.8919\n",
      "Epoch 46/300\n",
      "74/74 [==============================] - 0s 186us/step - loss: 0.0401 - acc: 0.8784\n",
      "Epoch 47/300\n",
      "74/74 [==============================] - 0s 202us/step - loss: 0.0379 - acc: 0.8919\n",
      "Epoch 48/300\n",
      "74/74 [==============================] - 0s 193us/step - loss: 0.0379 - acc: 0.8784\n",
      "Epoch 49/300\n",
      "74/74 [==============================] - 0s 198us/step - loss: 0.0354 - acc: 0.8919\n",
      "Epoch 50/300\n",
      "74/74 [==============================] - 0s 198us/step - loss: 0.0355 - acc: 0.8784\n",
      "Epoch 51/300\n",
      "74/74 [==============================] - 0s 217us/step - loss: 0.0344 - acc: 0.8919\n",
      "Epoch 52/300\n",
      "74/74 [==============================] - 0s 230us/step - loss: 0.0349 - acc: 0.8784\n",
      "Epoch 53/300\n",
      "74/74 [==============================] - 0s 235us/step - loss: 0.0359 - acc: 0.8919\n",
      "Epoch 54/300\n",
      "74/74 [==============================] - 0s 237us/step - loss: 0.0334 - acc: 0.8919\n",
      "Epoch 55/300\n",
      "74/74 [==============================] - 0s 226us/step - loss: 0.0335 - acc: 0.8919\n",
      "Epoch 56/300\n",
      "74/74 [==============================] - 0s 215us/step - loss: 0.0376 - acc: 0.8919\n",
      "Epoch 57/300\n",
      "74/74 [==============================] - 0s 234us/step - loss: 0.0360 - acc: 0.8919\n",
      "Epoch 58/300\n",
      "74/74 [==============================] - 0s 221us/step - loss: 0.0330 - acc: 0.8919\n",
      "Epoch 59/300\n",
      "74/74 [==============================] - 0s 214us/step - loss: 0.0331 - acc: 0.8919\n",
      "Epoch 60/300\n",
      "74/74 [==============================] - 0s 208us/step - loss: 0.0347 - acc: 0.8919\n",
      "Epoch 61/300\n",
      "74/74 [==============================] - 0s 207us/step - loss: 0.0313 - acc: 0.8919\n",
      "Epoch 62/300\n",
      "74/74 [==============================] - 0s 228us/step - loss: 0.0316 - acc: 0.8919\n",
      "Epoch 63/300\n",
      "74/74 [==============================] - 0s 230us/step - loss: 0.0337 - acc: 0.8919\n",
      "Epoch 64/300\n",
      "74/74 [==============================] - 0s 262us/step - loss: 0.0342 - acc: 0.8919\n",
      "Epoch 65/300\n",
      "74/74 [==============================] - 0s 246us/step - loss: 0.0335 - acc: 0.8919\n",
      "Epoch 66/300\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0441 - acc: 0.866 - 0s 247us/step - loss: 0.0337 - acc: 0.8784\n",
      "Epoch 67/300\n",
      "74/74 [==============================] - 0s 243us/step - loss: 0.0313 - acc: 0.8919\n",
      "Epoch 68/300\n",
      "74/74 [==============================] - 0s 233us/step - loss: 0.0342 - acc: 0.8919\n",
      "Epoch 69/300\n",
      "74/74 [==============================] - 0s 291us/step - loss: 0.0325 - acc: 0.8919\n",
      "Epoch 70/300\n",
      "74/74 [==============================] - 0s 287us/step - loss: 0.0330 - acc: 0.8919\n",
      "Epoch 71/300\n",
      "74/74 [==============================] - 0s 233us/step - loss: 0.0326 - acc: 0.8919\n",
      "Epoch 72/300\n",
      "74/74 [==============================] - 0s 226us/step - loss: 0.0324 - acc: 0.8919\n",
      "Epoch 73/300\n",
      "74/74 [==============================] - 0s 231us/step - loss: 0.0303 - acc: 0.8919\n",
      "Epoch 74/300\n",
      "74/74 [==============================] - 0s 248us/step - loss: 0.0295 - acc: 0.8919\n",
      "Epoch 75/300\n",
      "74/74 [==============================] - 0s 238us/step - loss: 0.0290 - acc: 0.8919\n",
      "Epoch 76/300\n",
      "74/74 [==============================] - 0s 248us/step - loss: 0.0321 - acc: 0.8919\n",
      "Epoch 77/300\n",
      "74/74 [==============================] - 0s 247us/step - loss: 0.0292 - acc: 0.8919\n",
      "Epoch 78/300\n",
      "74/74 [==============================] - 0s 240us/step - loss: 0.0342 - acc: 0.8919\n",
      "Epoch 79/300\n",
      "74/74 [==============================] - 0s 225us/step - loss: 0.0323 - acc: 0.8919\n",
      "Epoch 80/300\n",
      "74/74 [==============================] - 0s 236us/step - loss: 0.0315 - acc: 0.8919\n",
      "Epoch 81/300\n",
      "74/74 [==============================] - 0s 228us/step - loss: 0.0346 - acc: 0.8919\n",
      "Epoch 82/300\n",
      "74/74 [==============================] - 0s 232us/step - loss: 0.0311 - acc: 0.8919\n",
      "Epoch 83/300\n",
      "74/74 [==============================] - 0s 239us/step - loss: 0.0334 - acc: 0.8919\n",
      "Epoch 84/300\n",
      "74/74 [==============================] - 0s 233us/step - loss: 0.0309 - acc: 0.8919\n",
      "Epoch 85/300\n",
      "74/74 [==============================] - 0s 235us/step - loss: 0.0297 - acc: 0.8919\n",
      "Epoch 86/300\n",
      "74/74 [==============================] - 0s 216us/step - loss: 0.0311 - acc: 0.8919\n",
      "Epoch 87/300\n",
      "74/74 [==============================] - 0s 203us/step - loss: 0.0305 - acc: 0.8919\n",
      "Epoch 88/300\n",
      "74/74 [==============================] - 0s 186us/step - loss: 0.0284 - acc: 0.8919\n",
      "Epoch 89/300\n",
      "74/74 [==============================] - 0s 196us/step - loss: 0.0310 - acc: 0.8919\n",
      "Epoch 90/300\n",
      "74/74 [==============================] - 0s 179us/step - loss: 0.0308 - acc: 0.8919\n",
      "Epoch 91/300\n",
      "74/74 [==============================] - 0s 161us/step - loss: 0.0286 - acc: 0.8919\n",
      "Epoch 92/300\n",
      "74/74 [==============================] - 0s 164us/step - loss: 0.0285 - acc: 0.8919\n",
      "Epoch 93/300\n",
      "74/74 [==============================] - 0s 180us/step - loss: 0.0286 - acc: 0.8919\n",
      "Epoch 94/300\n",
      "74/74 [==============================] - 0s 181us/step - loss: 0.0361 - acc: 0.8919\n",
      "Epoch 95/300\n",
      "74/74 [==============================] - 0s 182us/step - loss: 0.0293 - acc: 0.8919\n",
      "Epoch 96/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0296 - acc: 0.8919\n",
      "Epoch 97/300\n",
      "74/74 [==============================] - 0s 194us/step - loss: 0.0316 - acc: 0.8919\n",
      "Epoch 98/300\n",
      "74/74 [==============================] - 0s 168us/step - loss: 0.0298 - acc: 0.8919\n",
      "Epoch 99/300\n",
      "74/74 [==============================] - 0s 168us/step - loss: 0.0292 - acc: 0.8919\n",
      "Epoch 100/300\n",
      "74/74 [==============================] - 0s 179us/step - loss: 0.0296 - acc: 0.8919\n",
      "Epoch 101/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0311 - acc: 0.8919\n",
      "Epoch 102/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0320 - acc: 0.8919\n",
      "Epoch 103/300\n",
      "74/74 [==============================] - 0s 191us/step - loss: 0.0295 - acc: 0.8919\n",
      "Epoch 104/300\n",
      "74/74 [==============================] - 0s 197us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 105/300\n",
      "74/74 [==============================] - 0s 218us/step - loss: 0.0314 - acc: 0.8919\n",
      "Epoch 106/300\n",
      "74/74 [==============================] - 0s 214us/step - loss: 0.0287 - acc: 0.8919\n",
      "Epoch 107/300\n",
      "74/74 [==============================] - 0s 224us/step - loss: 0.0282 - acc: 0.8919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/300\n",
      "74/74 [==============================] - 0s 214us/step - loss: 0.0285 - acc: 0.8919\n",
      "Epoch 109/300\n",
      "74/74 [==============================] - 0s 234us/step - loss: 0.0260 - acc: 0.8919\n",
      "Epoch 110/300\n",
      "74/74 [==============================] - 0s 248us/step - loss: 0.0323 - acc: 0.8919\n",
      "Epoch 111/300\n",
      "74/74 [==============================] - 0s 253us/step - loss: 0.0273 - acc: 0.8919\n",
      "Epoch 112/300\n",
      "74/74 [==============================] - 0s 198us/step - loss: 0.0339 - acc: 0.8919\n",
      "Epoch 113/300\n",
      "74/74 [==============================] - 0s 226us/step - loss: 0.0275 - acc: 0.8919\n",
      "Epoch 114/300\n",
      "74/74 [==============================] - 0s 196us/step - loss: 0.0265 - acc: 0.8919\n",
      "Epoch 115/300\n",
      "74/74 [==============================] - 0s 212us/step - loss: 0.0330 - acc: 0.8919\n",
      "Epoch 116/300\n",
      "74/74 [==============================] - 0s 221us/step - loss: 0.0320 - acc: 0.8919\n",
      "Epoch 117/300\n",
      "74/74 [==============================] - 0s 193us/step - loss: 0.0291 - acc: 0.8919\n",
      "Epoch 118/300\n",
      "74/74 [==============================] - 0s 196us/step - loss: 0.0277 - acc: 0.8919\n",
      "Epoch 119/300\n",
      "74/74 [==============================] - 0s 214us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 120/300\n",
      "74/74 [==============================] - 0s 195us/step - loss: 0.0296 - acc: 0.8919\n",
      "Epoch 121/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0317 - acc: 0.8919\n",
      "Epoch 122/300\n",
      "74/74 [==============================] - 0s 189us/step - loss: 0.0313 - acc: 0.8919\n",
      "Epoch 123/300\n",
      "74/74 [==============================] - 0s 164us/step - loss: 0.0323 - acc: 0.8919\n",
      "Epoch 124/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0290 - acc: 0.8919\n",
      "Epoch 125/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0306 - acc: 0.8919\n",
      "Epoch 126/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0312 - acc: 0.8919\n",
      "Epoch 127/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0284 - acc: 0.8919\n",
      "Epoch 128/300\n",
      "74/74 [==============================] - 0s 164us/step - loss: 0.0312 - acc: 0.8919\n",
      "Epoch 129/300\n",
      "74/74 [==============================] - 0s 164us/step - loss: 0.0329 - acc: 0.8919\n",
      "Epoch 130/300\n",
      "74/74 [==============================] - 0s 181us/step - loss: 0.0299 - acc: 0.8919\n",
      "Epoch 131/300\n",
      "74/74 [==============================] - 0s 164us/step - loss: 0.0278 - acc: 0.8919\n",
      "Epoch 132/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0296 - acc: 0.8919\n",
      "Epoch 133/300\n",
      "74/74 [==============================] - 0s 170us/step - loss: 0.0301 - acc: 0.8919\n",
      "Epoch 134/300\n",
      "74/74 [==============================] - 0s 168us/step - loss: 0.0278 - acc: 0.8919\n",
      "Epoch 135/300\n",
      "74/74 [==============================] - 0s 171us/step - loss: 0.0307 - acc: 0.8919\n",
      "Epoch 136/300\n",
      "74/74 [==============================] - 0s 181us/step - loss: 0.0302 - acc: 0.8919\n",
      "Epoch 137/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0291 - acc: 0.8919\n",
      "Epoch 138/300\n",
      "74/74 [==============================] - 0s 168us/step - loss: 0.0301 - acc: 0.8919\n",
      "Epoch 139/300\n",
      "74/74 [==============================] - 0s 160us/step - loss: 0.0272 - acc: 0.8919\n",
      "Epoch 140/300\n",
      "74/74 [==============================] - 0s 165us/step - loss: 0.0293 - acc: 0.8919\n",
      "Epoch 141/300\n",
      "74/74 [==============================] - 0s 175us/step - loss: 0.0291 - acc: 0.8919\n",
      "Epoch 142/300\n",
      "74/74 [==============================] - 0s 183us/step - loss: 0.0292 - acc: 0.8919\n",
      "Epoch 143/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0314 - acc: 0.8919\n",
      "Epoch 144/300\n",
      "74/74 [==============================] - 0s 178us/step - loss: 0.0286 - acc: 0.8919\n",
      "Epoch 145/300\n",
      "74/74 [==============================] - 0s 168us/step - loss: 0.0288 - acc: 0.8919\n",
      "Epoch 146/300\n",
      "74/74 [==============================] - 0s 163us/step - loss: 0.0283 - acc: 0.8919\n",
      "Epoch 147/300\n",
      "74/74 [==============================] - 0s 166us/step - loss: 0.0277 - acc: 0.8919\n",
      "Epoch 148/300\n",
      "74/74 [==============================] - 0s 172us/step - loss: 0.0312 - acc: 0.8919\n",
      "Epoch 149/300\n",
      "74/74 [==============================] - 0s 167us/step - loss: 0.0288 - acc: 0.8919\n",
      "Epoch 150/300\n",
      "74/74 [==============================] - 0s 163us/step - loss: 0.0290 - acc: 0.8919\n",
      "Epoch 151/300\n",
      "74/74 [==============================] - 0s 159us/step - loss: 0.0255 - acc: 0.8919\n",
      "Epoch 152/300\n",
      "74/74 [==============================] - 0s 171us/step - loss: 0.0278 - acc: 0.8919\n",
      "Epoch 153/300\n",
      "74/74 [==============================] - 0s 180us/step - loss: 0.0300 - acc: 0.8919\n",
      "Epoch 154/300\n",
      "74/74 [==============================] - 0s 161us/step - loss: 0.0290 - acc: 0.8919\n",
      "Epoch 155/300\n",
      "74/74 [==============================] - 0s 181us/step - loss: 0.0270 - acc: 0.8919\n",
      "Epoch 156/300\n",
      "74/74 [==============================] - 0s 158us/step - loss: 0.0302 - acc: 0.8919\n",
      "Epoch 157/300\n",
      "74/74 [==============================] - 0s 163us/step - loss: 0.0289 - acc: 0.8919\n",
      "Epoch 158/300\n",
      "74/74 [==============================] - 0s 183us/step - loss: 0.0304 - acc: 0.8919\n",
      "Epoch 159/300\n",
      "74/74 [==============================] - 0s 178us/step - loss: 0.0321 - acc: 0.8919\n",
      "Epoch 160/300\n",
      "74/74 [==============================] - 0s 180us/step - loss: 0.0301 - acc: 0.8919\n",
      "Epoch 161/300\n",
      "74/74 [==============================] - 0s 173us/step - loss: 0.0284 - acc: 0.8919\n",
      "Epoch 162/300\n",
      "74/74 [==============================] - 0s 181us/step - loss: 0.0268 - acc: 0.8919\n",
      "Epoch 163/300\n",
      "74/74 [==============================] - 0s 161us/step - loss: 0.0303 - acc: 0.8919\n",
      "Epoch 164/300\n",
      "74/74 [==============================] - 0s 189us/step - loss: 0.0284 - acc: 0.8919\n",
      "Epoch 165/300\n",
      "74/74 [==============================] - 0s 218us/step - loss: 0.0307 - acc: 0.8919\n",
      "Epoch 166/300\n",
      "74/74 [==============================] - 0s 215us/step - loss: 0.0314 - acc: 0.8919\n",
      "Epoch 167/300\n",
      "74/74 [==============================] - 0s 200us/step - loss: 0.0322 - acc: 0.8919\n",
      "Epoch 168/300\n",
      "74/74 [==============================] - 0s 197us/step - loss: 0.0310 - acc: 0.8919\n",
      "Epoch 169/300\n",
      "74/74 [==============================] - 0s 241us/step - loss: 0.0294 - acc: 0.8919\n",
      "Epoch 170/300\n",
      "74/74 [==============================] - 0s 244us/step - loss: 0.0289 - acc: 0.8919\n",
      "Epoch 171/300\n",
      "74/74 [==============================] - 0s 206us/step - loss: 0.0269 - acc: 0.8919\n",
      "Epoch 172/300\n",
      "74/74 [==============================] - 0s 217us/step - loss: 0.0255 - acc: 0.8919\n",
      "Epoch 173/300\n",
      "74/74 [==============================] - 0s 221us/step - loss: 0.0297 - acc: 0.8919\n",
      "Epoch 174/300\n",
      "74/74 [==============================] - 0s 199us/step - loss: 0.0316 - acc: 0.8919\n",
      "Epoch 175/300\n",
      "74/74 [==============================] - 0s 200us/step - loss: 0.0305 - acc: 0.8919\n",
      "Epoch 176/300\n",
      "74/74 [==============================] - 0s 207us/step - loss: 0.0269 - acc: 0.8919\n",
      "Epoch 177/300\n",
      "74/74 [==============================] - 0s 202us/step - loss: 0.0308 - acc: 0.8919\n",
      "Epoch 178/300\n",
      "74/74 [==============================] - 0s 222us/step - loss: 0.0294 - acc: 0.8919\n",
      "Epoch 179/300\n",
      "74/74 [==============================] - 0s 213us/step - loss: 0.0291 - acc: 0.8919\n",
      "Epoch 180/300\n",
      "74/74 [==============================] - 0s 213us/step - loss: 0.0305 - acc: 0.8919\n",
      "Epoch 181/300\n",
      "74/74 [==============================] - 0s 229us/step - loss: 0.0307 - acc: 0.8919\n",
      "Epoch 182/300\n",
      "74/74 [==============================] - 0s 202us/step - loss: 0.0285 - acc: 0.8919\n",
      "Epoch 183/300\n",
      "74/74 [==============================] - 0s 208us/step - loss: 0.0281 - acc: 0.8919\n",
      "Epoch 184/300\n",
      "74/74 [==============================] - 0s 206us/step - loss: 0.0279 - acc: 0.8919\n",
      "Epoch 185/300\n",
      "74/74 [==============================] - 0s 207us/step - loss: 0.0263 - acc: 0.8919\n",
      "Epoch 186/300\n",
      "74/74 [==============================] - 0s 203us/step - loss: 0.0302 - acc: 0.8919\n",
      "Epoch 187/300\n",
      "74/74 [==============================] - 0s 210us/step - loss: 0.0267 - acc: 0.8919\n",
      "Epoch 188/300\n",
      "74/74 [==============================] - 0s 219us/step - loss: 0.0314 - acc: 0.8919\n",
      "Epoch 189/300\n",
      "74/74 [==============================] - 0s 207us/step - loss: 0.0307 - acc: 0.8919\n",
      "Epoch 190/300\n",
      "74/74 [==============================] - 0s 200us/step - loss: 0.0281 - acc: 0.8919\n",
      "Epoch 191/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 200us/step - loss: 0.0291 - acc: 0.8919\n",
      "Epoch 192/300\n",
      "74/74 [==============================] - 0s 204us/step - loss: 0.0314 - acc: 0.8919\n",
      "Epoch 193/300\n",
      "74/74 [==============================] - 0s 210us/step - loss: 0.0307 - acc: 0.8919\n",
      "Epoch 194/300\n",
      "74/74 [==============================] - 0s 210us/step - loss: 0.0288 - acc: 0.8919\n",
      "Epoch 195/300\n",
      "74/74 [==============================] - 0s 233us/step - loss: 0.0286 - acc: 0.8919\n",
      "Epoch 196/300\n",
      "74/74 [==============================] - 0s 254us/step - loss: 0.0292 - acc: 0.8919\n",
      "Epoch 197/300\n",
      "74/74 [==============================] - 0s 265us/step - loss: 0.0299 - acc: 0.8919\n",
      "Epoch 198/300\n",
      "74/74 [==============================] - 0s 219us/step - loss: 0.0288 - acc: 0.8919\n",
      "Epoch 199/300\n",
      "74/74 [==============================] - 0s 232us/step - loss: 0.0281 - acc: 0.8919\n",
      "Epoch 200/300\n",
      "74/74 [==============================] - 0s 240us/step - loss: 0.0298 - acc: 0.8919\n",
      "Epoch 201/300\n",
      "74/74 [==============================] - 0s 231us/step - loss: 0.0329 - acc: 0.8919\n",
      "Epoch 202/300\n",
      "74/74 [==============================] - 0s 220us/step - loss: 0.0297 - acc: 0.8919\n",
      "Epoch 203/300\n",
      "74/74 [==============================] - 0s 215us/step - loss: 0.0279 - acc: 0.8919\n",
      "Epoch 204/300\n",
      "74/74 [==============================] - 0s 221us/step - loss: 0.0299 - acc: 0.8919\n",
      "Epoch 205/300\n",
      "74/74 [==============================] - 0s 228us/step - loss: 0.0327 - acc: 0.8919\n",
      "Epoch 206/300\n",
      "74/74 [==============================] - 0s 217us/step - loss: 0.0285 - acc: 0.8919\n",
      "Epoch 207/300\n",
      "74/74 [==============================] - 0s 189us/step - loss: 0.0312 - acc: 0.8919\n",
      "Epoch 208/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0303 - acc: 0.8919\n",
      "Epoch 209/300\n",
      "74/74 [==============================] - 0s 180us/step - loss: 0.0310 - acc: 0.8919\n",
      "Epoch 210/300\n",
      "74/74 [==============================] - 0s 178us/step - loss: 0.0289 - acc: 0.8919\n",
      "Epoch 211/300\n",
      "74/74 [==============================] - 0s 182us/step - loss: 0.0293 - acc: 0.8919\n",
      "Epoch 212/300\n",
      "74/74 [==============================] - 0s 179us/step - loss: 0.0312 - acc: 0.8919\n",
      "Epoch 213/300\n",
      "74/74 [==============================] - 0s 181us/step - loss: 0.0311 - acc: 0.8919\n",
      "Epoch 214/300\n",
      "74/74 [==============================] - 0s 158us/step - loss: 0.0299 - acc: 0.8919\n",
      "Epoch 215/300\n",
      "74/74 [==============================] - 0s 167us/step - loss: 0.0298 - acc: 0.8919\n",
      "Epoch 216/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 217/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0291 - acc: 0.8919\n",
      "Epoch 218/300\n",
      "74/74 [==============================] - 0s 171us/step - loss: 0.0300 - acc: 0.8919\n",
      "Epoch 219/300\n",
      "74/74 [==============================] - 0s 169us/step - loss: 0.0286 - acc: 0.8919\n",
      "Epoch 220/300\n",
      "74/74 [==============================] - 0s 165us/step - loss: 0.0279 - acc: 0.8919\n",
      "Epoch 221/300\n",
      "74/74 [==============================] - 0s 186us/step - loss: 0.0288 - acc: 0.8919\n",
      "Epoch 222/300\n",
      "74/74 [==============================] - 0s 182us/step - loss: 0.0269 - acc: 0.8919\n",
      "Epoch 223/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0296 - acc: 0.8919\n",
      "Epoch 224/300\n",
      "74/74 [==============================] - 0s 170us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 225/300\n",
      "74/74 [==============================] - 0s 166us/step - loss: 0.0286 - acc: 0.8919\n",
      "Epoch 226/300\n",
      "74/74 [==============================] - 0s 167us/step - loss: 0.0296 - acc: 0.8919\n",
      "Epoch 227/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0303 - acc: 0.8919\n",
      "Epoch 228/300\n",
      "74/74 [==============================] - 0s 172us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 229/300\n",
      "74/74 [==============================] - 0s 175us/step - loss: 0.0285 - acc: 0.8919\n",
      "Epoch 230/300\n",
      "74/74 [==============================] - 0s 175us/step - loss: 0.0307 - acc: 0.8919\n",
      "Epoch 231/300\n",
      "74/74 [==============================] - 0s 169us/step - loss: 0.0278 - acc: 0.8919\n",
      "Epoch 232/300\n",
      "74/74 [==============================] - 0s 162us/step - loss: 0.0269 - acc: 0.8919\n",
      "Epoch 233/300\n",
      "74/74 [==============================] - 0s 177us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 234/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 235/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0299 - acc: 0.8919\n",
      "Epoch 236/300\n",
      "74/74 [==============================] - 0s 175us/step - loss: 0.0274 - acc: 0.8919\n",
      "Epoch 237/300\n",
      "74/74 [==============================] - 0s 168us/step - loss: 0.0301 - acc: 0.8919\n",
      "Epoch 238/300\n",
      "74/74 [==============================] - 0s 162us/step - loss: 0.0271 - acc: 0.8919\n",
      "Epoch 239/300\n",
      "74/74 [==============================] - 0s 170us/step - loss: 0.0303 - acc: 0.8919\n",
      "Epoch 240/300\n",
      "74/74 [==============================] - 0s 177us/step - loss: 0.0292 - acc: 0.8919\n",
      "Epoch 241/300\n",
      "74/74 [==============================] - 0s 193us/step - loss: 0.0293 - acc: 0.8919\n",
      "Epoch 242/300\n",
      "74/74 [==============================] - 0s 164us/step - loss: 0.0285 - acc: 0.8919\n",
      "Epoch 243/300\n",
      "74/74 [==============================] - 0s 176us/step - loss: 0.0286 - acc: 0.8919\n",
      "Epoch 244/300\n",
      "74/74 [==============================] - 0s 166us/step - loss: 0.0305 - acc: 0.8919\n",
      "Epoch 245/300\n",
      "74/74 [==============================] - 0s 177us/step - loss: 0.0269 - acc: 0.8919\n",
      "Epoch 246/300\n",
      "74/74 [==============================] - 0s 179us/step - loss: 0.0266 - acc: 0.8919\n",
      "Epoch 247/300\n",
      "74/74 [==============================] - 0s 190us/step - loss: 0.0277 - acc: 0.8919\n",
      "Epoch 248/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0297 - acc: 0.8919\n",
      "Epoch 249/300\n",
      "74/74 [==============================] - 0s 173us/step - loss: 0.0277 - acc: 0.8919\n",
      "Epoch 250/300\n",
      "74/74 [==============================] - 0s 162us/step - loss: 0.0285 - acc: 0.8919\n",
      "Epoch 251/300\n",
      "74/74 [==============================] - 0s 190us/step - loss: 0.0279 - acc: 0.8919\n",
      "Epoch 252/300\n",
      "74/74 [==============================] - 0s 172us/step - loss: 0.0291 - acc: 0.8919\n",
      "Epoch 253/300\n",
      "74/74 [==============================] - 0s 196us/step - loss: 0.0303 - acc: 0.8919\n",
      "Epoch 254/300\n",
      "74/74 [==============================] - 0s 182us/step - loss: 0.0263 - acc: 0.8919\n",
      "Epoch 255/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0257 - acc: 0.8919\n",
      "Epoch 256/300\n",
      "74/74 [==============================] - 0s 170us/step - loss: 0.0294 - acc: 0.8919\n",
      "Epoch 257/300\n",
      "74/74 [==============================] - 0s 171us/step - loss: 0.0284 - acc: 0.8919\n",
      "Epoch 258/300\n",
      "74/74 [==============================] - 0s 173us/step - loss: 0.0298 - acc: 0.8919\n",
      "Epoch 259/300\n",
      "74/74 [==============================] - 0s 175us/step - loss: 0.0273 - acc: 0.8919\n",
      "Epoch 260/300\n",
      "74/74 [==============================] - 0s 175us/step - loss: 0.0283 - acc: 0.8919\n",
      "Epoch 261/300\n",
      "74/74 [==============================] - 0s 192us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 262/300\n",
      "74/74 [==============================] - 0s 188us/step - loss: 0.0302 - acc: 0.8919\n",
      "Epoch 263/300\n",
      "74/74 [==============================] - 0s 172us/step - loss: 0.0310 - acc: 0.8919\n",
      "Epoch 264/300\n",
      "74/74 [==============================] - 0s 181us/step - loss: 0.0312 - acc: 0.8919\n",
      "Epoch 265/300\n",
      "74/74 [==============================] - 0s 189us/step - loss: 0.0267 - acc: 0.8919\n",
      "Epoch 266/300\n",
      "74/74 [==============================] - 0s 185us/step - loss: 0.0292 - acc: 0.8919\n",
      "Epoch 267/300\n",
      "74/74 [==============================] - 0s 180us/step - loss: 0.0299 - acc: 0.8919\n",
      "Epoch 268/300\n",
      "74/74 [==============================] - 0s 373us/step - loss: 0.0267 - acc: 0.8919\n",
      "Epoch 269/300\n",
      "74/74 [==============================] - 0s 783us/step - loss: 0.0289 - acc: 0.8919\n",
      "Epoch 270/300\n",
      "74/74 [==============================] - 0s 192us/step - loss: 0.0323 - acc: 0.8919\n",
      "Epoch 271/300\n",
      "74/74 [==============================] - 0s 442us/step - loss: 0.0293 - acc: 0.8919\n",
      "Epoch 272/300\n",
      "74/74 [==============================] - 0s 451us/step - loss: 0.0289 - acc: 0.8919\n",
      "Epoch 273/300\n",
      "74/74 [==============================] - 0s 212us/step - loss: 0.0286 - acc: 0.8919\n",
      "Epoch 274/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 567us/step - loss: 0.0294 - acc: 0.8919\n",
      "Epoch 275/300\n",
      "74/74 [==============================] - 0s 218us/step - loss: 0.0298 - acc: 0.8919\n",
      "Epoch 276/300\n",
      "74/74 [==============================] - 0s 213us/step - loss: 0.0282 - acc: 0.8919\n",
      "Epoch 277/300\n",
      "74/74 [==============================] - 0s 207us/step - loss: 0.0268 - acc: 0.8919\n",
      "Epoch 278/300\n",
      "74/74 [==============================] - 0s 221us/step - loss: 0.0292 - acc: 0.8919\n",
      "Epoch 279/300\n",
      "74/74 [==============================] - 0s 202us/step - loss: 0.0281 - acc: 0.8919\n",
      "Epoch 280/300\n",
      "74/74 [==============================] - 0s 208us/step - loss: 0.0258 - acc: 0.8919\n",
      "Epoch 281/300\n",
      "74/74 [==============================] - 0s 231us/step - loss: 0.0304 - acc: 0.8919\n",
      "Epoch 282/300\n",
      "74/74 [==============================] - 0s 187us/step - loss: 0.0273 - acc: 0.8919\n",
      "Epoch 283/300\n",
      "74/74 [==============================] - 0s 177us/step - loss: 0.0315 - acc: 0.8919\n",
      "Epoch 284/300\n",
      "74/74 [==============================] - 0s 160us/step - loss: 0.0278 - acc: 0.8919\n",
      "Epoch 285/300\n",
      "74/74 [==============================] - 0s 175us/step - loss: 0.0296 - acc: 0.8919\n",
      "Epoch 286/300\n",
      "74/74 [==============================] - 0s 188us/step - loss: 0.0283 - acc: 0.8919\n",
      "Epoch 287/300\n",
      "74/74 [==============================] - 0s 201us/step - loss: 0.0303 - acc: 0.8919\n",
      "Epoch 288/300\n",
      "74/74 [==============================] - 0s 184us/step - loss: 0.0308 - acc: 0.8919\n",
      "Epoch 289/300\n",
      "74/74 [==============================] - 0s 174us/step - loss: 0.0270 - acc: 0.8919\n",
      "Epoch 290/300\n",
      "74/74 [==============================] - 0s 178us/step - loss: 0.0296 - acc: 0.8919\n",
      "Epoch 291/300\n",
      "74/74 [==============================] - 0s 182us/step - loss: 0.0291 - acc: 0.8919\n",
      "Epoch 292/300\n",
      "74/74 [==============================] - 0s 227us/step - loss: 0.0271 - acc: 0.8919\n",
      "Epoch 293/300\n",
      "74/74 [==============================] - 0s 228us/step - loss: 0.0274 - acc: 0.8919\n",
      "Epoch 294/300\n",
      "74/74 [==============================] - 0s 240us/step - loss: 0.0270 - acc: 0.8919\n",
      "Epoch 295/300\n",
      "74/74 [==============================] - 0s 227us/step - loss: 0.0306 - acc: 0.8919\n",
      "Epoch 296/300\n",
      "74/74 [==============================] - 0s 228us/step - loss: 0.0280 - acc: 0.8919\n",
      "Epoch 297/300\n",
      "74/74 [==============================] - 0s 217us/step - loss: 0.0285 - acc: 0.8919\n",
      "Epoch 298/300\n",
      "74/74 [==============================] - 0s 213us/step - loss: 0.0276 - acc: 0.8919\n",
      "Epoch 299/300\n",
      "74/74 [==============================] - 0s 229us/step - loss: 0.0270 - acc: 0.8919\n",
      "Epoch 300/300\n",
      "74/74 [==============================] - 0s 184us/step - loss: 0.0297 - acc: 0.8919\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_160 (Dense)            (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_160 (Activation)  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_89 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_161 (Activation)  (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "75/75 [==============================] - 4s 55ms/step - loss: 0.1471 - acc: 0.4933\n",
      "Epoch 2/300\n",
      "75/75 [==============================] - 0s 190us/step - loss: 0.1451 - acc: 0.7067\n",
      "Epoch 3/300\n",
      "75/75 [==============================] - 0s 172us/step - loss: 0.1442 - acc: 0.7600\n",
      "Epoch 4/300\n",
      "75/75 [==============================] - 0s 146us/step - loss: 0.1424 - acc: 0.7733\n",
      "Epoch 5/300\n",
      "75/75 [==============================] - 0s 156us/step - loss: 0.1418 - acc: 0.7600\n",
      "Epoch 6/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.1403 - acc: 0.8400\n",
      "Epoch 7/300\n",
      "75/75 [==============================] - 0s 161us/step - loss: 0.1389 - acc: 0.8133\n",
      "Epoch 8/300\n",
      "75/75 [==============================] - 0s 168us/step - loss: 0.1374 - acc: 0.8400\n",
      "Epoch 9/300\n",
      "75/75 [==============================] - 0s 158us/step - loss: 0.1348 - acc: 0.8267\n",
      "Epoch 10/300\n",
      "75/75 [==============================] - 0s 165us/step - loss: 0.1321 - acc: 0.8267\n",
      "Epoch 11/300\n",
      "75/75 [==============================] - 0s 163us/step - loss: 0.1321 - acc: 0.8400\n",
      "Epoch 12/300\n",
      "75/75 [==============================] - 0s 162us/step - loss: 0.1283 - acc: 0.8400\n",
      "Epoch 13/300\n",
      "75/75 [==============================] - 0s 157us/step - loss: 0.1270 - acc: 0.8267\n",
      "Epoch 14/300\n",
      "75/75 [==============================] - 0s 159us/step - loss: 0.1213 - acc: 0.8400\n",
      "Epoch 15/300\n",
      "75/75 [==============================] - 0s 159us/step - loss: 0.1191 - acc: 0.8267\n",
      "Epoch 16/300\n",
      "75/75 [==============================] - 0s 168us/step - loss: 0.1170 - acc: 0.8400\n",
      "Epoch 17/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.1138 - acc: 0.8400\n",
      "Epoch 18/300\n",
      "75/75 [==============================] - 0s 177us/step - loss: 0.1095 - acc: 0.8267\n",
      "Epoch 19/300\n",
      "75/75 [==============================] - 0s 159us/step - loss: 0.1071 - acc: 0.8267\n",
      "Epoch 20/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.1035 - acc: 0.8400\n",
      "Epoch 21/300\n",
      "75/75 [==============================] - 0s 170us/step - loss: 0.0974 - acc: 0.8400\n",
      "Epoch 22/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0919 - acc: 0.8400\n",
      "Epoch 23/300\n",
      "75/75 [==============================] - 0s 160us/step - loss: 0.0940 - acc: 0.8400\n",
      "Epoch 24/300\n",
      "75/75 [==============================] - 0s 161us/step - loss: 0.0884 - acc: 0.8400\n",
      "Epoch 25/300\n",
      "75/75 [==============================] - 0s 155us/step - loss: 0.0840 - acc: 0.8400\n",
      "Epoch 26/300\n",
      "75/75 [==============================] - 0s 189us/step - loss: 0.0817 - acc: 0.8400\n",
      "Epoch 27/300\n",
      "75/75 [==============================] - 0s 182us/step - loss: 0.0778 - acc: 0.8400\n",
      "Epoch 28/300\n",
      "75/75 [==============================] - 0s 181us/step - loss: 0.0733 - acc: 0.8400\n",
      "Epoch 29/300\n",
      "75/75 [==============================] - 0s 163us/step - loss: 0.0714 - acc: 0.8400\n",
      "Epoch 30/300\n",
      "75/75 [==============================] - 0s 168us/step - loss: 0.0709 - acc: 0.8400\n",
      "Epoch 31/300\n",
      "75/75 [==============================] - 0s 190us/step - loss: 0.0693 - acc: 0.8400\n",
      "Epoch 32/300\n",
      "75/75 [==============================] - 0s 174us/step - loss: 0.0674 - acc: 0.8400\n",
      "Epoch 33/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0650 - acc: 0.8400\n",
      "Epoch 34/300\n",
      "75/75 [==============================] - 0s 183us/step - loss: 0.0622 - acc: 0.8400\n",
      "Epoch 35/300\n",
      "75/75 [==============================] - 0s 166us/step - loss: 0.0589 - acc: 0.8400\n",
      "Epoch 36/300\n",
      "75/75 [==============================] - 0s 182us/step - loss: 0.0567 - acc: 0.8400\n",
      "Epoch 37/300\n",
      "75/75 [==============================] - 0s 181us/step - loss: 0.0556 - acc: 0.8533\n",
      "Epoch 38/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0582 - acc: 0.8400\n",
      "Epoch 39/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0526 - acc: 0.8400\n",
      "Epoch 40/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0530 - acc: 0.8533\n",
      "Epoch 41/300\n",
      "75/75 [==============================] - 0s 451us/step - loss: 0.0519 - acc: 0.8400\n",
      "Epoch 42/300\n",
      "75/75 [==============================] - 0s 446us/step - loss: 0.0520 - acc: 0.8533\n",
      "Epoch 43/300\n",
      "75/75 [==============================] - 0s 310us/step - loss: 0.0485 - acc: 0.8533\n",
      "Epoch 44/300\n",
      "75/75 [==============================] - 0s 187us/step - loss: 0.0517 - acc: 0.8400\n",
      "Epoch 45/300\n",
      "75/75 [==============================] - 0s 191us/step - loss: 0.0509 - acc: 0.8533\n",
      "Epoch 46/300\n",
      "75/75 [==============================] - 0s 182us/step - loss: 0.0470 - acc: 0.8400\n",
      "Epoch 47/300\n",
      "75/75 [==============================] - 0s 172us/step - loss: 0.0502 - acc: 0.8400\n",
      "Epoch 48/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 499us/step - loss: 0.0511 - acc: 0.8400\n",
      "Epoch 49/300\n",
      "75/75 [==============================] - 0s 379us/step - loss: 0.0490 - acc: 0.8533\n",
      "Epoch 50/300\n",
      "75/75 [==============================] - 0s 194us/step - loss: 0.0468 - acc: 0.8400\n",
      "Epoch 51/300\n",
      "75/75 [==============================] - 0s 212us/step - loss: 0.0447 - acc: 0.8533\n",
      "Epoch 52/300\n",
      "75/75 [==============================] - 0s 575us/step - loss: 0.0468 - acc: 0.8533\n",
      "Epoch 53/300\n",
      "75/75 [==============================] - 0s 208us/step - loss: 0.0442 - acc: 0.8533\n",
      "Epoch 54/300\n",
      "75/75 [==============================] - 0s 270us/step - loss: 0.0446 - acc: 0.8400\n",
      "Epoch 55/300\n",
      "75/75 [==============================] - 0s 214us/step - loss: 0.0449 - acc: 0.8400\n",
      "Epoch 56/300\n",
      "75/75 [==============================] - 0s 200us/step - loss: 0.0453 - acc: 0.8400\n",
      "Epoch 57/300\n",
      "75/75 [==============================] - 0s 207us/step - loss: 0.0451 - acc: 0.8400\n",
      "Epoch 58/300\n",
      "75/75 [==============================] - 0s 190us/step - loss: 0.0436 - acc: 0.8533\n",
      "Epoch 59/300\n",
      "75/75 [==============================] - 0s 196us/step - loss: 0.0421 - acc: 0.8533\n",
      "Epoch 60/300\n",
      "75/75 [==============================] - 0s 199us/step - loss: 0.0401 - acc: 0.8533\n",
      "Epoch 61/300\n",
      "75/75 [==============================] - 0s 187us/step - loss: 0.0429 - acc: 0.8533\n",
      "Epoch 62/300\n",
      "75/75 [==============================] - 0s 184us/step - loss: 0.0443 - acc: 0.8533\n",
      "Epoch 63/300\n",
      "75/75 [==============================] - 0s 193us/step - loss: 0.0424 - acc: 0.8400\n",
      "Epoch 64/300\n",
      "75/75 [==============================] - 0s 186us/step - loss: 0.0436 - acc: 0.8533\n",
      "Epoch 65/300\n",
      "75/75 [==============================] - 0s 211us/step - loss: 0.0464 - acc: 0.8533\n",
      "Epoch 66/300\n",
      "75/75 [==============================] - 0s 202us/step - loss: 0.0424 - acc: 0.8400\n",
      "Epoch 67/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0409 - acc: 0.8400\n",
      "Epoch 68/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0399 - acc: 0.8533\n",
      "Epoch 69/300\n",
      "75/75 [==============================] - 0s 204us/step - loss: 0.0424 - acc: 0.8533\n",
      "Epoch 70/300\n",
      "75/75 [==============================] - 0s 194us/step - loss: 0.0403 - acc: 0.8533\n",
      "Epoch 71/300\n",
      "75/75 [==============================] - 0s 187us/step - loss: 0.0395 - acc: 0.8533\n",
      "Epoch 72/300\n",
      "75/75 [==============================] - 0s 189us/step - loss: 0.0431 - acc: 0.8533\n",
      "Epoch 73/300\n",
      "75/75 [==============================] - 0s 200us/step - loss: 0.0403 - acc: 0.8533\n",
      "Epoch 74/300\n",
      "75/75 [==============================] - 0s 202us/step - loss: 0.0426 - acc: 0.8533\n",
      "Epoch 75/300\n",
      "75/75 [==============================] - 0s 196us/step - loss: 0.0412 - acc: 0.8400\n",
      "Epoch 76/300\n",
      "75/75 [==============================] - 0s 190us/step - loss: 0.0381 - acc: 0.8533\n",
      "Epoch 77/300\n",
      "75/75 [==============================] - 0s 202us/step - loss: 0.0406 - acc: 0.8533\n",
      "Epoch 78/300\n",
      "75/75 [==============================] - 0s 193us/step - loss: 0.0406 - acc: 0.8533\n",
      "Epoch 79/300\n",
      "75/75 [==============================] - 0s 193us/step - loss: 0.0406 - acc: 0.8533\n",
      "Epoch 80/300\n",
      "75/75 [==============================] - 0s 195us/step - loss: 0.0404 - acc: 0.8533\n",
      "Epoch 81/300\n",
      "75/75 [==============================] - 0s 202us/step - loss: 0.0404 - acc: 0.8533\n",
      "Epoch 82/300\n",
      "75/75 [==============================] - 0s 205us/step - loss: 0.0394 - acc: 0.8533\n",
      "Epoch 83/300\n",
      "75/75 [==============================] - 0s 202us/step - loss: 0.0383 - acc: 0.8533\n",
      "Epoch 84/300\n",
      "75/75 [==============================] - 0s 201us/step - loss: 0.0392 - acc: 0.8533\n",
      "Epoch 85/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0407 - acc: 0.8533\n",
      "Epoch 86/300\n",
      "75/75 [==============================] - 0s 175us/step - loss: 0.0358 - acc: 0.8533\n",
      "Epoch 87/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.0396 - acc: 0.8533\n",
      "Epoch 88/300\n",
      "75/75 [==============================] - 0s 206us/step - loss: 0.0418 - acc: 0.8533\n",
      "Epoch 89/300\n",
      "75/75 [==============================] - 0s 197us/step - loss: 0.0388 - acc: 0.8533\n",
      "Epoch 90/300\n",
      "75/75 [==============================] - 0s 205us/step - loss: 0.0394 - acc: 0.8533\n",
      "Epoch 91/300\n",
      "75/75 [==============================] - 0s 203us/step - loss: 0.0386 - acc: 0.8533\n",
      "Epoch 92/300\n",
      "75/75 [==============================] - 0s 192us/step - loss: 0.0407 - acc: 0.8533\n",
      "Epoch 93/300\n",
      "75/75 [==============================] - 0s 213us/step - loss: 0.0411 - acc: 0.8533\n",
      "Epoch 94/300\n",
      "75/75 [==============================] - 0s 201us/step - loss: 0.0380 - acc: 0.8533\n",
      "Epoch 95/300\n",
      "75/75 [==============================] - 0s 212us/step - loss: 0.0365 - acc: 0.8533\n",
      "Epoch 96/300\n",
      "75/75 [==============================] - 0s 203us/step - loss: 0.0380 - acc: 0.8533\n",
      "Epoch 97/300\n",
      "75/75 [==============================] - 0s 193us/step - loss: 0.0411 - acc: 0.8533\n",
      "Epoch 98/300\n",
      "75/75 [==============================] - 0s 207us/step - loss: 0.0401 - acc: 0.8533\n",
      "Epoch 99/300\n",
      "75/75 [==============================] - 0s 218us/step - loss: 0.0350 - acc: 0.8533\n",
      "Epoch 100/300\n",
      "75/75 [==============================] - 0s 205us/step - loss: 0.0380 - acc: 0.8533\n",
      "Epoch 101/300\n",
      "75/75 [==============================] - 0s 189us/step - loss: 0.0412 - acc: 0.8533\n",
      "Epoch 102/300\n",
      "75/75 [==============================] - 0s 189us/step - loss: 0.0382 - acc: 0.8533\n",
      "Epoch 103/300\n",
      "75/75 [==============================] - 0s 188us/step - loss: 0.0412 - acc: 0.8533\n",
      "Epoch 104/300\n",
      "75/75 [==============================] - 0s 184us/step - loss: 0.0388 - acc: 0.8533\n",
      "Epoch 105/300\n",
      "75/75 [==============================] - 0s 196us/step - loss: 0.0349 - acc: 0.8533\n",
      "Epoch 106/300\n",
      "75/75 [==============================] - 0s 201us/step - loss: 0.0392 - acc: 0.8533\n",
      "Epoch 107/300\n",
      "75/75 [==============================] - 0s 340us/step - loss: 0.0392 - acc: 0.8533\n",
      "Epoch 108/300\n",
      "75/75 [==============================] - 0s 498us/step - loss: 0.0398 - acc: 0.8533\n",
      "Epoch 109/300\n",
      "75/75 [==============================] - 0s 338us/step - loss: 0.0399 - acc: 0.8533\n",
      "Epoch 110/300\n",
      "75/75 [==============================] - 0s 197us/step - loss: 0.0390 - acc: 0.8533\n",
      "Epoch 111/300\n",
      "75/75 [==============================] - 0s 584us/step - loss: 0.0372 - acc: 0.8533\n",
      "Epoch 112/300\n",
      "75/75 [==============================] - 0s 351us/step - loss: 0.0398 - acc: 0.8533\n",
      "Epoch 113/300\n",
      "75/75 [==============================] - 0s 275us/step - loss: 0.0379 - acc: 0.8533\n",
      "Epoch 114/300\n",
      "75/75 [==============================] - 0s 491us/step - loss: 0.0403 - acc: 0.8533\n",
      "Epoch 115/300\n",
      "75/75 [==============================] - 0s 188us/step - loss: 0.0400 - acc: 0.8533\n",
      "Epoch 116/300\n",
      "75/75 [==============================] - 0s 199us/step - loss: 0.0363 - acc: 0.8533\n",
      "Epoch 117/300\n",
      "75/75 [==============================] - 0s 194us/step - loss: 0.0387 - acc: 0.8533\n",
      "Epoch 118/300\n",
      "75/75 [==============================] - 0s 191us/step - loss: 0.0388 - acc: 0.8533\n",
      "Epoch 119/300\n",
      "75/75 [==============================] - 0s 195us/step - loss: 0.0374 - acc: 0.8533\n",
      "Epoch 120/300\n",
      "75/75 [==============================] - 0s 182us/step - loss: 0.0380 - acc: 0.8533\n",
      "Epoch 121/300\n",
      "75/75 [==============================] - 0s 174us/step - loss: 0.0377 - acc: 0.8533\n",
      "Epoch 122/300\n",
      "75/75 [==============================] - 0s 168us/step - loss: 0.0381 - acc: 0.8533\n",
      "Epoch 123/300\n",
      "75/75 [==============================] - 0s 161us/step - loss: 0.0378 - acc: 0.8533\n",
      "Epoch 124/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.0389 - acc: 0.8533\n",
      "Epoch 125/300\n",
      "75/75 [==============================] - 0s 172us/step - loss: 0.0387 - acc: 0.8533\n",
      "Epoch 126/300\n",
      "75/75 [==============================] - 0s 160us/step - loss: 0.0386 - acc: 0.8533\n",
      "Epoch 127/300\n",
      "75/75 [==============================] - 0s 155us/step - loss: 0.0395 - acc: 0.8533\n",
      "Epoch 128/300\n",
      "75/75 [==============================] - 0s 183us/step - loss: 0.0391 - acc: 0.8533\n",
      "Epoch 129/300\n",
      "75/75 [==============================] - 0s 162us/step - loss: 0.0364 - acc: 0.8533\n",
      "Epoch 130/300\n",
      "75/75 [==============================] - 0s 154us/step - loss: 0.0377 - acc: 0.8533\n",
      "Epoch 131/300\n",
      "75/75 [==============================] - 0s 172us/step - loss: 0.0386 - acc: 0.8533\n",
      "Epoch 132/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 168us/step - loss: 0.0393 - acc: 0.8533\n",
      "Epoch 133/300\n",
      "75/75 [==============================] - 0s 166us/step - loss: 0.0370 - acc: 0.8533\n",
      "Epoch 134/300\n",
      "75/75 [==============================] - 0s 161us/step - loss: 0.0371 - acc: 0.8533\n",
      "Epoch 135/300\n",
      "75/75 [==============================] - 0s 154us/step - loss: 0.0378 - acc: 0.8533\n",
      "Epoch 136/300\n",
      "75/75 [==============================] - 0s 181us/step - loss: 0.0388 - acc: 0.8533\n",
      "Epoch 137/300\n",
      "75/75 [==============================] - 0s 169us/step - loss: 0.0410 - acc: 0.8533\n",
      "Epoch 138/300\n",
      "75/75 [==============================] - 0s 155us/step - loss: 0.0368 - acc: 0.8533\n",
      "Epoch 139/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.0394 - acc: 0.8533\n",
      "Epoch 140/300\n",
      "75/75 [==============================] - 0s 155us/step - loss: 0.0358 - acc: 0.8533\n",
      "Epoch 141/300\n",
      "75/75 [==============================] - 0s 162us/step - loss: 0.0408 - acc: 0.8533\n",
      "Epoch 142/300\n",
      "75/75 [==============================] - 0s 169us/step - loss: 0.0395 - acc: 0.8533\n",
      "Epoch 143/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.0381 - acc: 0.8533\n",
      "Epoch 144/300\n",
      "75/75 [==============================] - 0s 183us/step - loss: 0.0386 - acc: 0.8533\n",
      "Epoch 145/300\n",
      "75/75 [==============================] - 0s 174us/step - loss: 0.0383 - acc: 0.8533\n",
      "Epoch 146/300\n",
      "75/75 [==============================] - 0s 174us/step - loss: 0.0366 - acc: 0.8533\n",
      "Epoch 147/300\n",
      "75/75 [==============================] - 0s 169us/step - loss: 0.0394 - acc: 0.8533\n",
      "Epoch 148/300\n",
      "75/75 [==============================] - 0s 169us/step - loss: 0.0365 - acc: 0.8533\n",
      "Epoch 149/300\n",
      "75/75 [==============================] - 0s 160us/step - loss: 0.0409 - acc: 0.8533\n",
      "Epoch 150/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 151/300\n",
      "75/75 [==============================] - 0s 155us/step - loss: 0.0369 - acc: 0.8533\n",
      "Epoch 152/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0368 - acc: 0.8533\n",
      "Epoch 153/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0369 - acc: 0.8533\n",
      "Epoch 154/300\n",
      "75/75 [==============================] - 0s 190us/step - loss: 0.0357 - acc: 0.8533\n",
      "Epoch 155/300\n",
      "75/75 [==============================] - 0s 181us/step - loss: 0.0387 - acc: 0.8533\n",
      "Epoch 156/300\n",
      "75/75 [==============================] - 0s 186us/step - loss: 0.0387 - acc: 0.8533\n",
      "Epoch 157/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0363 - acc: 0.8533\n",
      "Epoch 158/300\n",
      "75/75 [==============================] - 0s 180us/step - loss: 0.0393 - acc: 0.8533\n",
      "Epoch 159/300\n",
      "75/75 [==============================] - 0s 160us/step - loss: 0.0380 - acc: 0.8533\n",
      "Epoch 160/300\n",
      "75/75 [==============================] - 0s 170us/step - loss: 0.0378 - acc: 0.8533\n",
      "Epoch 161/300\n",
      "75/75 [==============================] - 0s 180us/step - loss: 0.0377 - acc: 0.8533\n",
      "Epoch 162/300\n",
      "75/75 [==============================] - 0s 172us/step - loss: 0.0371 - acc: 0.8533\n",
      "Epoch 163/300\n",
      "75/75 [==============================] - 0s 194us/step - loss: 0.0390 - acc: 0.8533\n",
      "Epoch 164/300\n",
      "75/75 [==============================] - 0s 175us/step - loss: 0.0379 - acc: 0.8533\n",
      "Epoch 165/300\n",
      "75/75 [==============================] - 0s 175us/step - loss: 0.0402 - acc: 0.8533\n",
      "Epoch 166/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0404 - acc: 0.8533\n",
      "Epoch 167/300\n",
      "75/75 [==============================] - 0s 166us/step - loss: 0.0404 - acc: 0.8533\n",
      "Epoch 168/300\n",
      "75/75 [==============================] - 0s 169us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 169/300\n",
      "75/75 [==============================] - 0s 180us/step - loss: 0.0387 - acc: 0.8533\n",
      "Epoch 170/300\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0140 - acc: 0.933 - 0s 181us/step - loss: 0.0358 - acc: 0.8533\n",
      "Epoch 171/300\n",
      "75/75 [==============================] - 0s 170us/step - loss: 0.0401 - acc: 0.8533\n",
      "Epoch 172/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0388 - acc: 0.8533\n",
      "Epoch 173/300\n",
      "75/75 [==============================] - 0s 182us/step - loss: 0.0390 - acc: 0.8533\n",
      "Epoch 174/300\n",
      "75/75 [==============================] - 0s 191us/step - loss: 0.0382 - acc: 0.8533\n",
      "Epoch 175/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0378 - acc: 0.8533\n",
      "Epoch 176/300\n",
      "75/75 [==============================] - 0s 158us/step - loss: 0.0376 - acc: 0.8533\n",
      "Epoch 177/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0379 - acc: 0.8533\n",
      "Epoch 178/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0389 - acc: 0.8533\n",
      "Epoch 179/300\n",
      "75/75 [==============================] - 0s 165us/step - loss: 0.0393 - acc: 0.8533\n",
      "Epoch 180/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0367 - acc: 0.8533\n",
      "Epoch 181/300\n",
      "75/75 [==============================] - 0s 175us/step - loss: 0.0353 - acc: 0.8533\n",
      "Epoch 182/300\n",
      "75/75 [==============================] - 0s 161us/step - loss: 0.0377 - acc: 0.8533\n",
      "Epoch 183/300\n",
      "75/75 [==============================] - 0s 187us/step - loss: 0.0398 - acc: 0.8533\n",
      "Epoch 184/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 185/300\n",
      "75/75 [==============================] - 0s 176us/step - loss: 0.0391 - acc: 0.8533\n",
      "Epoch 186/300\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0175 - acc: 0.866 - 0s 176us/step - loss: 0.0398 - acc: 0.8533\n",
      "Epoch 187/300\n",
      "75/75 [==============================] - 0s 182us/step - loss: 0.0385 - acc: 0.8533\n",
      "Epoch 188/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 189/300\n",
      "75/75 [==============================] - 0s 161us/step - loss: 0.0405 - acc: 0.8533\n",
      "Epoch 190/300\n",
      "75/75 [==============================] - 0s 189us/step - loss: 0.0375 - acc: 0.8533\n",
      "Epoch 191/300\n",
      "75/75 [==============================] - 0s 175us/step - loss: 0.0368 - acc: 0.8533\n",
      "Epoch 192/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0364 - acc: 0.8533\n",
      "Epoch 193/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0359 - acc: 0.8533\n",
      "Epoch 194/300\n",
      "75/75 [==============================] - 0s 174us/step - loss: 0.0394 - acc: 0.8533\n",
      "Epoch 195/300\n",
      "75/75 [==============================] - 0s 183us/step - loss: 0.0424 - acc: 0.8533\n",
      "Epoch 196/300\n",
      "75/75 [==============================] - 0s 193us/step - loss: 0.0375 - acc: 0.8533\n",
      "Epoch 197/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0366 - acc: 0.8533\n",
      "Epoch 198/300\n",
      "75/75 [==============================] - 0s 165us/step - loss: 0.0393 - acc: 0.8533\n",
      "Epoch 199/300\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.0387 - acc: 0.733 - 0s 179us/step - loss: 0.0380 - acc: 0.8533\n",
      "Epoch 200/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0391 - acc: 0.8533\n",
      "Epoch 201/300\n",
      "75/75 [==============================] - 0s 201us/step - loss: 0.0367 - acc: 0.8533\n",
      "Epoch 202/300\n",
      "75/75 [==============================] - 0s 200us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 203/300\n",
      "75/75 [==============================] - 0s 188us/step - loss: 0.0379 - acc: 0.8533\n",
      "Epoch 204/300\n",
      "75/75 [==============================] - 0s 181us/step - loss: 0.0383 - acc: 0.8533\n",
      "Epoch 205/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0364 - acc: 0.8533\n",
      "Epoch 206/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.0376 - acc: 0.8533\n",
      "Epoch 207/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0391 - acc: 0.8533\n",
      "Epoch 208/300\n",
      "75/75 [==============================] - 0s 168us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 209/300\n",
      "75/75 [==============================] - 0s 179us/step - loss: 0.0354 - acc: 0.8533\n",
      "Epoch 210/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0363 - acc: 0.8533\n",
      "Epoch 211/300\n",
      "75/75 [==============================] - 0s 174us/step - loss: 0.0396 - acc: 0.8533\n",
      "Epoch 212/300\n",
      "75/75 [==============================] - 0s 172us/step - loss: 0.0391 - acc: 0.8533\n",
      "Epoch 213/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 214/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 176us/step - loss: 0.0390 - acc: 0.8533\n",
      "Epoch 215/300\n",
      "75/75 [==============================] - 0s 182us/step - loss: 0.0389 - acc: 0.8533\n",
      "Epoch 216/300\n",
      "75/75 [==============================] - 0s 179us/step - loss: 0.0389 - acc: 0.8533\n",
      "Epoch 217/300\n",
      "75/75 [==============================] - 0s 187us/step - loss: 0.0373 - acc: 0.8533\n",
      "Epoch 218/300\n",
      "75/75 [==============================] - 0s 170us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 219/300\n",
      "75/75 [==============================] - 0s 164us/step - loss: 0.0368 - acc: 0.8533\n",
      "Epoch 220/300\n",
      "75/75 [==============================] - 0s 156us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 221/300\n",
      "75/75 [==============================] - 0s 183us/step - loss: 0.0355 - acc: 0.8533\n",
      "Epoch 222/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0378 - acc: 0.8533\n",
      "Epoch 223/300\n",
      "75/75 [==============================] - 0s 169us/step - loss: 0.0368 - acc: 0.8533\n",
      "Epoch 224/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0371 - acc: 0.8533\n",
      "Epoch 225/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0375 - acc: 0.8533\n",
      "Epoch 226/300\n",
      "75/75 [==============================] - 0s 160us/step - loss: 0.0347 - acc: 0.8533\n",
      "Epoch 227/300\n",
      "75/75 [==============================] - 0s 180us/step - loss: 0.0364 - acc: 0.8533\n",
      "Epoch 228/300\n",
      "75/75 [==============================] - 0s 176us/step - loss: 0.0366 - acc: 0.8533\n",
      "Epoch 229/300\n",
      "75/75 [==============================] - 0s 179us/step - loss: 0.0382 - acc: 0.8533\n",
      "Epoch 230/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0379 - acc: 0.8533\n",
      "Epoch 231/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0394 - acc: 0.8533\n",
      "Epoch 232/300\n",
      "75/75 [==============================] - 0s 158us/step - loss: 0.0383 - acc: 0.8533\n",
      "Epoch 233/300\n",
      "75/75 [==============================] - 0s 166us/step - loss: 0.0385 - acc: 0.8533\n",
      "Epoch 234/300\n",
      "75/75 [==============================] - 0s 175us/step - loss: 0.0370 - acc: 0.8533\n",
      "Epoch 235/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0359 - acc: 0.8533\n",
      "Epoch 236/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0368 - acc: 0.8533\n",
      "Epoch 237/300\n",
      "75/75 [==============================] - 0s 168us/step - loss: 0.0344 - acc: 0.8533\n",
      "Epoch 238/300\n",
      "75/75 [==============================] - 0s 158us/step - loss: 0.0391 - acc: 0.8533\n",
      "Epoch 239/300\n",
      "75/75 [==============================] - 0s 180us/step - loss: 0.0380 - acc: 0.8533\n",
      "Epoch 240/300\n",
      "75/75 [==============================] - 0s 172us/step - loss: 0.0377 - acc: 0.8533\n",
      "Epoch 241/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0380 - acc: 0.8533\n",
      "Epoch 242/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0351 - acc: 0.8533\n",
      "Epoch 243/300\n",
      "75/75 [==============================] - 0s 171us/step - loss: 0.0368 - acc: 0.8533\n",
      "Epoch 244/300\n",
      "75/75 [==============================] - 0s 154us/step - loss: 0.0369 - acc: 0.8533\n",
      "Epoch 245/300\n",
      "75/75 [==============================] - 0s 185us/step - loss: 0.0358 - acc: 0.8533\n",
      "Epoch 246/300\n",
      "75/75 [==============================] - 0s 168us/step - loss: 0.0389 - acc: 0.8533\n",
      "Epoch 247/300\n",
      "75/75 [==============================] - 0s 169us/step - loss: 0.0376 - acc: 0.8533\n",
      "Epoch 248/300\n",
      "75/75 [==============================] - 0s 169us/step - loss: 0.0360 - acc: 0.8533\n",
      "Epoch 249/300\n",
      "75/75 [==============================] - 0s 176us/step - loss: 0.0412 - acc: 0.8533\n",
      "Epoch 250/300\n",
      "75/75 [==============================] - 0s 155us/step - loss: 0.0363 - acc: 0.8533\n",
      "Epoch 251/300\n",
      "75/75 [==============================] - 0s 177us/step - loss: 0.0384 - acc: 0.8533\n",
      "Epoch 252/300\n",
      "75/75 [==============================] - 0s 174us/step - loss: 0.0364 - acc: 0.8533\n",
      "Epoch 253/300\n",
      "75/75 [==============================] - 0s 170us/step - loss: 0.0385 - acc: 0.8533\n",
      "Epoch 254/300\n",
      "75/75 [==============================] - 0s 177us/step - loss: 0.0359 - acc: 0.8533\n",
      "Epoch 255/300\n",
      "75/75 [==============================] - 0s 161us/step - loss: 0.0383 - acc: 0.8533\n",
      "Epoch 256/300\n",
      "75/75 [==============================] - 0s 159us/step - loss: 0.0379 - acc: 0.8533\n",
      "Epoch 257/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0388 - acc: 0.8533\n",
      "Epoch 258/300\n",
      "75/75 [==============================] - 0s 163us/step - loss: 0.0358 - acc: 0.8533\n",
      "Epoch 259/300\n",
      "75/75 [==============================] - 0s 180us/step - loss: 0.0359 - acc: 0.8533\n",
      "Epoch 260/300\n",
      "75/75 [==============================] - 0s 177us/step - loss: 0.0367 - acc: 0.8533\n",
      "Epoch 261/300\n",
      "75/75 [==============================] - 0s 165us/step - loss: 0.0364 - acc: 0.8533\n",
      "Epoch 262/300\n",
      "75/75 [==============================] - 0s 160us/step - loss: 0.0384 - acc: 0.8533\n",
      "Epoch 263/300\n",
      "75/75 [==============================] - 0s 170us/step - loss: 0.0390 - acc: 0.8533\n",
      "Epoch 264/300\n",
      "75/75 [==============================] - 0s 158us/step - loss: 0.0376 - acc: 0.8533\n",
      "Epoch 265/300\n",
      "75/75 [==============================] - 0s 166us/step - loss: 0.0379 - acc: 0.8533\n",
      "Epoch 266/300\n",
      "75/75 [==============================] - 0s 162us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 267/300\n",
      "75/75 [==============================] - 0s 161us/step - loss: 0.0357 - acc: 0.8533\n",
      "Epoch 268/300\n",
      "75/75 [==============================] - 0s 172us/step - loss: 0.0391 - acc: 0.8533\n",
      "Epoch 269/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0361 - acc: 0.8533\n",
      "Epoch 270/300\n",
      "75/75 [==============================] - 0s 159us/step - loss: 0.0365 - acc: 0.8533\n",
      "Epoch 271/300\n",
      "75/75 [==============================] - 0s 162us/step - loss: 0.0362 - acc: 0.8533\n",
      "Epoch 272/300\n",
      "75/75 [==============================] - 0s 154us/step - loss: 0.0365 - acc: 0.8533\n",
      "Epoch 273/300\n",
      "75/75 [==============================] - 0s 166us/step - loss: 0.0359 - acc: 0.8533\n",
      "Epoch 274/300\n",
      "75/75 [==============================] - 0s 174us/step - loss: 0.0389 - acc: 0.8533\n",
      "Epoch 275/300\n",
      "75/75 [==============================] - 0s 178us/step - loss: 0.0387 - acc: 0.8533\n",
      "Epoch 276/300\n",
      "75/75 [==============================] - 0s 179us/step - loss: 0.0349 - acc: 0.8533\n",
      "Epoch 277/300\n",
      "75/75 [==============================] - 0s 175us/step - loss: 0.0377 - acc: 0.8533\n",
      "Epoch 278/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0368 - acc: 0.8533\n",
      "Epoch 279/300\n",
      "75/75 [==============================] - 0s 176us/step - loss: 0.0376 - acc: 0.8533\n",
      "Epoch 280/300\n",
      "75/75 [==============================] - 0s 190us/step - loss: 0.0388 - acc: 0.8533\n",
      "Epoch 281/300\n",
      "75/75 [==============================] - 0s 205us/step - loss: 0.0372 - acc: 0.8533\n",
      "Epoch 282/300\n",
      "75/75 [==============================] - 0s 179us/step - loss: 0.0356 - acc: 0.8533\n",
      "Epoch 283/300\n",
      "75/75 [==============================] - 0s 188us/step - loss: 0.0374 - acc: 0.8533\n",
      "Epoch 284/300\n",
      "75/75 [==============================] - 0s 189us/step - loss: 0.0357 - acc: 0.8533\n",
      "Epoch 285/300\n",
      "75/75 [==============================] - 0s 186us/step - loss: 0.0377 - acc: 0.8533\n",
      "Epoch 286/300\n",
      "75/75 [==============================] - 0s 185us/step - loss: 0.0370 - acc: 0.8533\n",
      "Epoch 287/300\n",
      "75/75 [==============================] - 0s 179us/step - loss: 0.0355 - acc: 0.8533\n",
      "Epoch 288/300\n",
      "75/75 [==============================] - 0s 193us/step - loss: 0.0367 - acc: 0.8533\n",
      "Epoch 289/300\n",
      "75/75 [==============================] - 0s 205us/step - loss: 0.0356 - acc: 0.8533\n",
      "Epoch 290/300\n",
      "75/75 [==============================] - 0s 217us/step - loss: 0.0351 - acc: 0.8533\n",
      "Epoch 291/300\n",
      "75/75 [==============================] - 0s 197us/step - loss: 0.0362 - acc: 0.8533\n",
      "Epoch 292/300\n",
      "75/75 [==============================] - 0s 160us/step - loss: 0.0385 - acc: 0.8533\n",
      "Epoch 293/300\n",
      "75/75 [==============================] - 0s 163us/step - loss: 0.0389 - acc: 0.8533\n",
      "Epoch 294/300\n",
      "75/75 [==============================] - 0s 176us/step - loss: 0.0401 - acc: 0.8533\n",
      "Epoch 295/300\n",
      "75/75 [==============================] - 0s 166us/step - loss: 0.0350 - acc: 0.8533\n",
      "Epoch 296/300\n",
      "75/75 [==============================] - 0s 167us/step - loss: 0.0370 - acc: 0.8533\n",
      "Epoch 297/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 168us/step - loss: 0.0383 - acc: 0.8533\n",
      "Epoch 298/300\n",
      "75/75 [==============================] - 0s 175us/step - loss: 0.0396 - acc: 0.8533\n",
      "Epoch 299/300\n",
      "75/75 [==============================] - 0s 173us/step - loss: 0.0356 - acc: 0.8533\n",
      "Epoch 300/300\n",
      "75/75 [==============================] - 0s 205us/step - loss: 0.0360 - acc: 0.8533\n"
     ]
    }
   ],
   "source": [
    "kfold = 3\n",
    "random_state = 11\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "time_k = np.zeros(kfold)\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "epochs = 300\n",
    "batch_size = 15\n",
    "\n",
    "# class_weight = {0 : 1., 1: 1.,}  # The weights can be changed and made inversely proportional to the class size to improve the accuracy.\n",
    "class_weight = {0 : 0.12, 1: 0.88,}\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],), init='uniform')) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weight)\n",
    "    end_time = time.time()\n",
    "    time_k[k] = end_time-start_time\n",
    "\n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred = model.predict_proba(X_train).round().astype(int)\n",
    "    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test accuracy 0.5236467236467236\n",
      "Average Run time 9.076935768127441\n"
     ]
    }
   ],
   "source": [
    "print ('Average f1 score', np.mean(test_F1))\n",
    "print ('Average Run time', np.mean(time_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an LSTM Classifier on the sequences for comparison\n",
    "We built an LSTM Classifier on the sequences to compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = darpa_data['seq']\n",
    "encoded_X = np.ndarray(shape=(len(X),), dtype=list)\n",
    "for i in range(0,len(X)):\n",
    "    encoded_X[i]=X.iloc[i].split(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = np.max(darpa_data['seqlen'])\n",
    "encoded_X = sequence.pad_sequences(encoded_X, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inferno/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "73/73 [==============================] - 13s 178ms/step - loss: 0.6912 - acc: 0.7260\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 9s 123ms/step - loss: 0.6837 - acc: 0.8904\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 9s 122ms/step - loss: 0.6720 - acc: 0.8904\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 0.6497 - acc: 0.8904\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 0.5931 - acc: 0.8904\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 0.5135 - acc: 0.8904\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 0.4409 - acc: 0.8904\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 9s 125ms/step - loss: 0.4078 - acc: 0.8904\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 9s 129ms/step - loss: 0.3817 - acc: 0.8904\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 0.3647 - acc: 0.8904\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 0.3543 - acc: 0.8904\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.3470 - acc: 0.8904\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 8s 112ms/step - loss: 0.3471 - acc: 0.8904\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 0.3470 - acc: 0.8904\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 9s 119ms/step - loss: 0.3462 - acc: 0.8904\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.3464 - acc: 0.8904\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 8s 115ms/step - loss: 0.3463 - acc: 0.8904\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 10s 131ms/step - loss: 0.3464 - acc: 0.8904\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 0.3464 - acc: 0.8904\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 0.3460 - acc: 0.8904\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.3459 - acc: 0.8904\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.3458 - acc: 0.8904\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 0.3472 - acc: 0.8904\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 8s 113ms/step - loss: 0.3459 - acc: 0.8904\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 0.3464 - acc: 0.8904\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3459 - acc: 0.8904\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 0.3458 - acc: 0.8904\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 9s 126ms/step - loss: 0.3457 - acc: 0.8904\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 9s 118ms/step - loss: 0.3458 - acc: 0.8904\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 8s 116ms/step - loss: 0.3461 - acc: 0.8904\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3458 - acc: 0.8904\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 8s 111ms/step - loss: 0.3465 - acc: 0.8904\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.3469 - acc: 0.8904\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 0.3458 - acc: 0.8904\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3457 - acc: 0.8904\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 8s 104ms/step - loss: 0.3458 - acc: 0.8904\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 0.3460 - acc: 0.8904\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 0.3459 - acc: 0.8904\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 0.3459 - acc: 0.8904\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3460 - acc: 0.8904\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 8s 110ms/step - loss: 0.3469 - acc: 0.8904\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3460 - acc: 0.8904\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3464 - acc: 0.8904\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.3458 - acc: 0.8904\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 8s 107ms/step - loss: 0.3457 - acc: 0.8904\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 8s 106ms/step - loss: 0.3459 - acc: 0.8904\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 0.3470 - acc: 0.8904\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 9s 121ms/step - loss: 0.3464 - acc: 0.8904\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 9s 117ms/step - loss: 0.3459 - acc: 0.8904\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 8s 114ms/step - loss: 0.3460 - acc: 0.8904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/inferno/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "74/74 [==============================] - 12s 168ms/step - loss: 0.6900 - acc: 0.8784\n",
      "Epoch 2/50\n",
      "74/74 [==============================] - 8s 104ms/step - loss: 0.6839 - acc: 0.8784\n",
      "Epoch 3/50\n",
      "74/74 [==============================] - 8s 105ms/step - loss: 0.6744 - acc: 0.8784\n",
      "Epoch 4/50\n",
      "74/74 [==============================] - 8s 107ms/step - loss: 0.6610 - acc: 0.8784\n",
      "Epoch 5/50\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.6307 - acc: 0.8784\n",
      "Epoch 6/50\n",
      "74/74 [==============================] - 9s 121ms/step - loss: 0.5592 - acc: 0.8784\n",
      "Epoch 7/50\n",
      "74/74 [==============================] - 9s 118ms/step - loss: 0.4755 - acc: 0.8784\n",
      "Epoch 8/50\n",
      "74/74 [==============================] - 8s 107ms/step - loss: 0.4349 - acc: 0.8784\n",
      "Epoch 9/50\n",
      "74/74 [==============================] - 9s 115ms/step - loss: 0.4040 - acc: 0.8784\n",
      "Epoch 10/50\n",
      "74/74 [==============================] - 8s 107ms/step - loss: 0.3861 - acc: 0.8784\n",
      "Epoch 11/50\n",
      "74/74 [==============================] - 8s 107ms/step - loss: 0.3775 - acc: 0.8784\n",
      "Epoch 12/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3726 - acc: 0.8784\n",
      "Epoch 13/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3705 - acc: 0.8784\n",
      "Epoch 14/50\n",
      "74/74 [==============================] - 10s 130ms/step - loss: 0.3727 - acc: 0.8784\n",
      "Epoch 15/50\n",
      "74/74 [==============================] - 10s 132ms/step - loss: 0.3708 - acc: 0.8784\n",
      "Epoch 16/50\n",
      "74/74 [==============================] - 9s 121ms/step - loss: 0.3708 - acc: 0.8784\n",
      "Epoch 17/50\n",
      "74/74 [==============================] - 9s 119ms/step - loss: 0.3713 - acc: 0.8784\n",
      "Epoch 18/50\n",
      "74/74 [==============================] - 8s 112ms/step - loss: 0.3714 - acc: 0.8784\n",
      "Epoch 19/50\n",
      "74/74 [==============================] - 8s 111ms/step - loss: 0.3725 - acc: 0.8784\n",
      "Epoch 20/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3708 - acc: 0.8784\n",
      "Epoch 21/50\n",
      "74/74 [==============================] - 8s 108ms/step - loss: 0.3702 - acc: 0.8784\n",
      "Epoch 22/50\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.3703 - acc: 0.8784\n",
      "Epoch 23/50\n",
      "74/74 [==============================] - 9s 126ms/step - loss: 0.3705 - acc: 0.8784\n",
      "Epoch 24/50\n",
      "74/74 [==============================] - 9s 119ms/step - loss: 0.3703 - acc: 0.8784\n",
      "Epoch 25/50\n",
      "74/74 [==============================] - 8s 108ms/step - loss: 0.3709 - acc: 0.8784\n",
      "Epoch 26/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3706 - acc: 0.8784\n",
      "Epoch 27/50\n",
      "74/74 [==============================] - 8s 107ms/step - loss: 0.3708 - acc: 0.8784\n",
      "Epoch 28/50\n",
      "74/74 [==============================] - 8s 108ms/step - loss: 0.3703 - acc: 0.8784\n",
      "Epoch 29/50\n",
      "74/74 [==============================] - 8s 111ms/step - loss: 0.3703 - acc: 0.8784\n",
      "Epoch 30/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3705 - acc: 0.8784\n",
      "Epoch 31/50\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.3704 - acc: 0.8784\n",
      "Epoch 32/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3707 - acc: 0.8784\n",
      "Epoch 33/50\n",
      "74/74 [==============================] - 9s 123ms/step - loss: 0.3702 - acc: 0.8784\n",
      "Epoch 34/50\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.3704 - acc: 0.8784\n",
      "Epoch 35/50\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.3702 - acc: 0.8784\n",
      "Epoch 36/50\n",
      "74/74 [==============================] - 8s 115ms/step - loss: 0.3703 - acc: 0.8784\n",
      "Epoch 37/50\n",
      "74/74 [==============================] - 8s 113ms/step - loss: 0.3712 - acc: 0.8784\n",
      "Epoch 38/50\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.3701 - acc: 0.8784\n",
      "Epoch 39/50\n",
      "74/74 [==============================] - 8s 111ms/step - loss: 0.3703 - acc: 0.8784\n",
      "Epoch 40/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3704 - acc: 0.8784\n",
      "Epoch 41/50\n",
      "74/74 [==============================] - 8s 108ms/step - loss: 0.3705 - acc: 0.8784\n",
      "Epoch 42/50\n",
      "74/74 [==============================] - 9s 123ms/step - loss: 0.3703 - acc: 0.8784\n",
      "Epoch 43/50\n",
      "74/74 [==============================] - 9s 125ms/step - loss: 0.3704 - acc: 0.8784\n",
      "Epoch 44/50\n",
      "74/74 [==============================] - 8s 112ms/step - loss: 0.3705 - acc: 0.8784\n",
      "Epoch 45/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3704 - acc: 0.8784\n",
      "Epoch 46/50\n",
      "74/74 [==============================] - 8s 113ms/step - loss: 0.3704 - acc: 0.8784\n",
      "Epoch 47/50\n",
      "74/74 [==============================] - 8s 109ms/step - loss: 0.3706 - acc: 0.8784\n",
      "Epoch 48/50\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.3709 - acc: 0.8784\n",
      "Epoch 49/50\n",
      "74/74 [==============================] - 8s 111ms/step - loss: 0.3702 - acc: 0.8784\n",
      "Epoch 50/50\n",
      "74/74 [==============================] - 9s 115ms/step - loss: 0.3711 - acc: 0.8784\n",
      "Epoch 1/50\n",
      "75/75 [==============================] - 14s 186ms/step - loss: 0.6915 - acc: 0.6800\n",
      "Epoch 2/50\n",
      "75/75 [==============================] - 9s 117ms/step - loss: 0.6852 - acc: 0.8800\n",
      "Epoch 3/50\n",
      "75/75 [==============================] - 9s 116ms/step - loss: 0.6777 - acc: 0.8800\n",
      "Epoch 4/50\n",
      "75/75 [==============================] - 8s 110ms/step - loss: 0.6676 - acc: 0.8800\n",
      "Epoch 5/50\n",
      "75/75 [==============================] - 8s 111ms/step - loss: 0.6504 - acc: 0.8800\n",
      "Epoch 6/50\n",
      "75/75 [==============================] - 8s 109ms/step - loss: 0.6143 - acc: 0.8800\n",
      "Epoch 7/50\n",
      "75/75 [==============================] - 8s 109ms/step - loss: 0.5376 - acc: 0.8800\n",
      "Epoch 8/50\n",
      "75/75 [==============================] - 8s 108ms/step - loss: 0.4488 - acc: 0.8800\n",
      "Epoch 9/50\n",
      "75/75 [==============================] - 8s 111ms/step - loss: 0.4069 - acc: 0.8800\n",
      "Epoch 10/50\n",
      "75/75 [==============================] - 10s 131ms/step - loss: 0.3788 - acc: 0.8800\n",
      "Epoch 11/50\n",
      "75/75 [==============================] - 9s 115ms/step - loss: 0.3702 - acc: 0.8800\n",
      "Epoch 12/50\n",
      "75/75 [==============================] - 8s 113ms/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 13/50\n",
      "75/75 [==============================] - 8s 111ms/step - loss: 0.3686 - acc: 0.8800\n",
      "Epoch 14/50\n",
      "75/75 [==============================] - 8s 113ms/step - loss: 0.3678 - acc: 0.8800\n",
      "Epoch 15/50\n",
      "75/75 [==============================] - 8s 108ms/step - loss: 0.3678 - acc: 0.8800\n",
      "Epoch 16/50\n",
      "75/75 [==============================] - 8s 109ms/step - loss: 0.3689 - acc: 0.8800\n",
      "Epoch 17/50\n",
      "75/75 [==============================] - 8s 109ms/step - loss: 0.3677 - acc: 0.8800\n",
      "Epoch 18/50\n",
      "75/75 [==============================] - 8s 110ms/step - loss: 0.3673 - acc: 0.8800\n",
      "Epoch 19/50\n",
      "75/75 [==============================] - 9s 122ms/step - loss: 0.3674 - acc: 0.8800\n",
      "Epoch 20/50\n",
      "75/75 [==============================] - 9s 121ms/step - loss: 0.3672 - acc: 0.8800\n",
      "Epoch 21/50\n",
      "75/75 [==============================] - 9s 115ms/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 22/50\n",
      "75/75 [==============================] - 9s 121ms/step - loss: 0.3672 - acc: 0.8800\n",
      "Epoch 23/50\n",
      "75/75 [==============================] - 8s 111ms/step - loss: 0.3670 - acc: 0.8800\n",
      "Epoch 24/50\n",
      "75/75 [==============================] - 9s 121ms/step - loss: 0.3672 - acc: 0.8800\n",
      "Epoch 25/50\n",
      "75/75 [==============================] - 8s 112ms/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 26/50\n",
      "75/75 [==============================] - 8s 110ms/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 27/50\n",
      "75/75 [==============================] - 8s 106ms/step - loss: 0.3673 - acc: 0.8800\n",
      "Epoch 28/50\n",
      "75/75 [==============================] - 8s 112ms/step - loss: 0.3674 - acc: 0.8800\n",
      "Epoch 29/50\n",
      "75/75 [==============================] - 8s 107ms/step - loss: 0.3674 - acc: 0.8800\n",
      "Epoch 30/50\n",
      "75/75 [==============================] - 8s 106ms/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 31/50\n",
      "75/75 [==============================] - 8s 106ms/step - loss: 0.3683 - acc: 0.8800\n",
      "Epoch 32/50\n",
      "75/75 [==============================] - 8s 107ms/step - loss: 0.3680 - acc: 0.8800\n",
      "Epoch 33/50\n",
      "75/75 [==============================] - 9s 116ms/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 34/50\n",
      "75/75 [==============================] - 9s 125ms/step - loss: 0.3685 - acc: 0.8800\n",
      "Epoch 35/50\n",
      "75/75 [==============================] - 9s 116ms/step - loss: 0.3677 - acc: 0.8800\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 8s 113ms/step - loss: 0.3674 - acc: 0.8800\n",
      "Epoch 37/50\n",
      "75/75 [==============================] - 9s 124ms/step - loss: 0.3669 - acc: 0.8800\n",
      "Epoch 38/50\n",
      "75/75 [==============================] - 9s 118ms/step - loss: 0.3669 - acc: 0.8800\n",
      "Epoch 39/50\n",
      "75/75 [==============================] - 9s 114ms/step - loss: 0.3668 - acc: 0.8800\n",
      "Epoch 40/50\n",
      "75/75 [==============================] - 8s 108ms/step - loss: 0.3672 - acc: 0.8800\n",
      "Epoch 41/50\n",
      "75/75 [==============================] - 8s 112ms/step - loss: 0.3666 - acc: 0.8800\n",
      "Epoch 42/50\n",
      "75/75 [==============================] - 10s 132ms/step - loss: 0.3672 - acc: 0.8800\n",
      "Epoch 43/50\n",
      "75/75 [==============================] - 9s 117ms/step - loss: 0.3660 - acc: 0.8800\n",
      "Epoch 44/50\n",
      "75/75 [==============================] - 9s 121ms/step - loss: 0.3670 - acc: 0.8800\n",
      "Epoch 45/50\n",
      "75/75 [==============================] - 9s 120ms/step - loss: 0.3671 - acc: 0.8800\n",
      "Epoch 46/50\n",
      "75/75 [==============================] - 9s 119ms/step - loss: 0.3672 - acc: 0.8800\n",
      "Epoch 47/50\n",
      "75/75 [==============================] - 9s 116ms/step - loss: 0.3672 - acc: 0.8800\n",
      "Epoch 48/50\n",
      "75/75 [==============================] - 8s 106ms/step - loss: 0.3681 - acc: 0.8800\n",
      "Epoch 49/50\n",
      "75/75 [==============================] - 8s 107ms/step - loss: 0.3679 - acc: 0.8800\n",
      "Epoch 50/50\n",
      "75/75 [==============================] - 8s 111ms/step - loss: 0.3674 - acc: 0.8800\n"
     ]
    }
   ],
   "source": [
    "kfold = 3\n",
    "random_state = 11\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "time_k = np.zeros(kfold)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 15\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "\n",
    "for train_index, test_index in skf.split(encoded_X, y):\n",
    "    X_train, X_test = encoded_X[train_index], encoded_X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    embedding_vecor_length = 32\n",
    "    top_words=50\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, init='uniform'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    end_time=time.time()\n",
    "    time_k[k]=end_time-start_time\n",
    "\n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred=model.predict_proba(X_train).round().astype(int)\n",
    "    test_F1[k]=sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test accuracy 0.0\n",
      "Average Run time 425.68603706359863\n"
     ]
    }
   ],
   "source": [
    "print ('Average f1 score', np.mean(test_F1))\n",
    "print ('Average Run time', np.mean(time_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the LSTM classifier gives an F1 score of 0. This may be improved by changing the model. However, we find that the SGT embedding could work with a small and unbalanced data without the need of a complicated classifier model.\n",
    "\n",
    "LSTM models typically require more data for training and also has significantly more computation time. The LSTM model above took 425.6 secs while the MLP model took just 9.1 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
