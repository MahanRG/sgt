{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Authors: Chitta Ranjan <cran2367@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sgt definition.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Sequence Graph Transform (SGT) is a sequence embedding function. SGT extracts the short- and long-term sequence features and embeds them in a finite-dimensional feature space. With SGT you can tune the amount of short- to long-term patterns extracted in the embeddings without any  increase in the computation.\"\n",
    "\n",
    "```class Sgt():\n",
    "    '''\n",
    "    Compute embedding of a single or a collection of discrete item \n",
    "    sequences. A discrete item sequence is a sequence made from a set\n",
    "    discrete elements, also known as alphabet set. For example,\n",
    "    suppose the alphabet set is the set of roman letters, \n",
    "    {A, B, ..., Z}. This set is made of discrete elements. Examples of\n",
    "    sequences from such a set are AABADDSA, UADSFJPFFFOIHOUGD, etc.\n",
    "    Such sequence datasets are commonly found in online industry,\n",
    "    for example, item purchase history, where the alphabet set is\n",
    "    the set of all product items. Sequence datasets are abundant in\n",
    "    bioinformatics as protein sequences.\n",
    "    Using the embeddings created here, classification and clustering\n",
    "    models can be built for sequence datasets.\n",
    "    Read more in https://arxiv.org/pdf/1608.03533.pdf\n",
    "    '''\n",
    "```\n",
    "    Parameters\n",
    "    ----------\n",
    "    Input:\n",
    "\n",
    "    alphabets       Optional, except if mode is Spark. \n",
    "                    The set of alphabets that make up all \n",
    "                    the sequences in the dataset. If not passed, the\n",
    "                    alphabet set is automatically computed as the \n",
    "                    unique set of elements that make all the sequences.\n",
    "                    A list or 1d-array of the set of elements that make up the      \n",
    "                    sequences. For example, np.array([\"A\", \"B\", \"C\"].\n",
    "                    If mode is 'spark', the alphabets are necessary.\n",
    "\n",
    "    kappa           Tuning parameter, kappa > 0, to change the extraction of \n",
    "                    long-term dependency. Higher the value the lesser\n",
    "                    the long-term dependency captured in the embedding.\n",
    "                    Typical values for kappa are 1, 5, 10.\n",
    "\n",
    "    lengthsensitive Default false. This is set to true if the embedding of\n",
    "                    should have the information of the length of the sequence.\n",
    "                    If set to false then the embedding of two sequences with\n",
    "                    similar pattern but different lengths will be the same.\n",
    "                    lengthsensitive = false is similar to length-normalization.\n",
    "                    \n",
    "    flatten         Default True. If True the SGT embedding is flattened and returned as\n",
    "                    a vector. Otherwise, it is returned as a matrix with the row and col\n",
    "                    names same as the alphabets. The matrix form is used for            \n",
    "                    interpretation purposes. Especially, to understand how the alphabets\n",
    "                    are \"related\". Otherwise, for applying machine learning or deep\n",
    "                    learning algorithms, the embedding vectors are required.\n",
    "    \n",
    "    mode            Choices in {'default', 'multiprocessing', 'spark'}.\n",
    "    \n",
    "    processors      Used if mode is 'multiprocessing'. By default, the \n",
    "                    number of processors used in multiprocessing is\n",
    "                    number of available - 1.\n",
    "    \n",
    "    lazy            Used if mode is 'spark'. Default is False. If False,\n",
    "                    the SGT embeddings are computed for each sequence\n",
    "                    in the inputted RDD and returned as a list of \n",
    "                    embedding vectors. Otherwise, the RDD map is returned.\n",
    "    '''\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    def fit(sequence)\n",
    "    \n",
    "    Extract Sequence Graph Transform features using Algorithm-2 in https://arxiv.org/abs/1608.03533.\n",
    "    Input:\n",
    "    sequence        An array of discrete elements. For example,\n",
    "                    np.array([\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"].\n",
    "                    \n",
    "    Output: \n",
    "    sgt embedding   sgt matrix or vector (depending on Flatten==False or True) of the sequence\n",
    "    \n",
    "    \n",
    "    --\n",
    "    def fit_transform(corpus)\n",
    "    \n",
    "    Extract SGT embeddings for all sequences in a corpus. It finds\n",
    "    the alphabets encompassing all the sequences in the corpus, if not inputted. \n",
    "    However, if the mode is 'spark', then the alphabets list has to be\n",
    "    explicitly given in Sgt object declaration.\n",
    "    \n",
    "    Input:\n",
    "    corpus          A list of sequences. Each sequence is a list of alphabets.\n",
    "    \n",
    "    Output:\n",
    "    sgt embedding of all sequences in the corpus.\n",
    "    \n",
    "    \n",
    "    --\n",
    "    def transform(corpus)\n",
    "    \n",
    "    Find SGT embeddings of a new data sample belonging to the same population\n",
    "    of the corpus that was fitted initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import warnings\n",
    "\n",
    "########\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import sklearn.metrics\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(7) # fix random seed for reproducibility\n",
    "\n",
    "from sgt import Sgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation Test Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.906163</td>\n",
       "      <td>1.310023</td>\n",
       "      <td>2.618487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.865694</td>\n",
       "      <td>1.230423</td>\n",
       "      <td>0.525440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>1.371416</td>\n",
       "      <td>0.282625</td>\n",
       "      <td>1.353353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "A  0.906163  1.310023  2.618487\n",
       "B  0.865694  1.230423  0.525440\n",
       "C  1.371416  0.282625  1.353353"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning a sgt embedding as a matrix with \n",
    "# rows and columns as the sequence alphabets. \n",
    "# This embedding shows the relationship between \n",
    "# the alphabets. The higher the value the \n",
    "# stronger the relationship.\n",
    "\n",
    "sgt = Sgt(flatten=False)\n",
    "sequence = np.array([\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"])\n",
    "sgt.fit(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90616284 1.31002279 2.6184865  0.         0.         0.86569371\n",
      "  1.23042262 0.52543984 0.         0.         1.37141609 0.28262508\n",
      "  1.35335283 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.09157819 0.92166965 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.92166965\n",
      "  1.45182361]]\n"
     ]
    }
   ],
   "source": [
    "# Learning the sgt embeddings as vector for\n",
    "# all sequences in a corpus.\n",
    "\n",
    "sgt = Sgt(kappa=1, lengthsensitive=False)\n",
    "corpus = [[\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"], [\"C\", \"Z\", \"Z\", \"Z\", \"D\"]]\n",
    "\n",
    "s = sgt.fit_transform(corpus)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23305129 0.2791752  0.33922608 0.         0.         0.26177435\n",
      "  0.29531212 0.10270374 0.         0.         0.28654051 0.04334255\n",
      "  0.13533528 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01831564 0.29571168 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.29571168\n",
      "  0.3394528 ]]\n"
     ]
    }
   ],
   "source": [
    "# Change the parameters from default to\n",
    "# a tuned value.\n",
    "\n",
    "sgt = Sgt(kappa=5, lengthsensitive=True)\n",
    "corpus = [[\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"], [\"C\", \"Z\", \"Z\", \"Z\", \"D\"]]\n",
    "\n",
    "s = sgt.fit_transform(corpus)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.90616284 1.31002279 2.6184865  0.         0.         0.86569371\n",
      "  1.23042262 0.52543984 0.         0.         1.37141609 0.28262508\n",
      "  1.35335283 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.09157819 0.92166965 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.92166965\n",
      "  1.45182361]]\n"
     ]
    }
   ],
   "source": [
    "# Change the mode for faster computation.\n",
    "# Mode: 'multiprocessing'\n",
    "# Uses the multiple processors (CPUs) avalaible.\n",
    "\n",
    "corpus = [[\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"], [\"C\", \"Z\", \"Z\", \"Z\", \"D\"]]\n",
    "\n",
    "sgt = Sgt(mode='multiprocessing')\n",
    "s = sgt.fit_transform(corpus)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the mode for faster computation.\n",
    "# Mode: 'spark'\n",
    "# Uses spark RDD.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"app\")\n",
    "\n",
    "corpus = [[\"B\",\"B\",\"A\",\"C\",\"A\",\"C\",\"A\",\"A\",\"B\",\"A\"], [\"C\", \"Z\", \"Z\", \"Z\", \"D\"]]\n",
    "\n",
    "rdd = sc.parallelize(corpus)\n",
    "\n",
    "sgt_sc = sgt.Sgt(kappa = 1, \n",
    "                 lengthsensitive = False, \n",
    "                 mode=\"spark\", \n",
    "                 alphabets=[\"A\", \"B\", \"C\", \"D\", \"Z\"],\n",
    "                 lazy=False)\n",
    "\n",
    "s = sgt_sc.fit_transform(corpus=rdd)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real data examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protein Sequence Data Analysis\n",
    "\n",
    "The data used here is taken from www.uniprot.org. This is a public database for proteins. The data contains the protein sequences and their functions. In the following, we will demonstrate \n",
    "- clustering of the sequences.\n",
    "- classification of the sequences with the functions as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M', 'E', 'I', 'E', 'K', 'T', 'N', 'R', 'M', 'N', 'A', 'L', 'F', 'E', 'F', 'Y', 'A', 'A', 'L', 'L', 'T', 'D', 'K', 'Q', 'M', 'N', 'Y', 'I', 'E', 'L', 'Y', 'Y', 'A', 'D', 'D', 'Y', 'S', 'L', 'A', 'E', 'I', 'A', 'E', 'E', 'F', 'G', 'V', 'S', 'R', 'Q', 'A', 'V', 'Y', 'D', 'N', 'I', 'K', 'R', 'T', 'E', 'K', 'I', 'L', 'E', 'D', 'Y', 'E', 'M', 'K', 'L', 'H', 'M', 'Y', 'S', 'D', 'Y', 'I', 'V', 'R', 'S', 'Q', 'I', 'F', 'D', 'Q', 'I', 'L', 'E', 'R', 'Y', 'P', 'K', 'D', 'D', 'F', 'L', 'Q', 'E', 'Q', 'I', 'E', 'I', 'L', 'T', 'S', 'I', 'D', 'N', 'R', 'E']\n"
     ]
    }
   ],
   "source": [
    "protein_data=pd.read_csv('../data/protein_classification.csv')\n",
    "X=protein_data['Sequence']\n",
    "def split(word): \n",
    "    return [char for char in word] \n",
    "\n",
    "sequences = [split(x) for x in X]\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgt = Sgt(kappa=1, lengthsensitive=False, mode='multiprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 79.5 ms, sys: 46 ms, total: 125 ms\n",
      "Wall time: 6.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedding = sgt.fit_transform(corpus=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2112, 400)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence Clustering\n",
    "We perform PCA on the sequence embeddings and then do kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6432744907364913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.384913</td>\n",
       "      <td>-0.269873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022764</td>\n",
       "      <td>0.135995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.177792</td>\n",
       "      <td>-0.172454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.168074</td>\n",
       "      <td>-0.147334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.383616</td>\n",
       "      <td>-0.271163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2\n",
       "0  0.384913 -0.269873\n",
       "1  0.022764  0.135995\n",
       "2  0.177792 -0.172454\n",
       "3  0.168074 -0.147334\n",
       "4  0.383616 -0.271163"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(embedding)\n",
    "X=pca.transform(embedding)\n",
    "\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "df = pd.DataFrame(data=X, columns=['x1', 'x2'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13bd97438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, max_iter =300)\n",
    "kmeans.fit(df)\n",
    "\n",
    "labels = kmeans.predict(df)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "colmap = {1: 'r', 2: 'g', 3: 'b'}\n",
    "colors = list(map(lambda x: colmap[x+1], labels))\n",
    "plt.scatter(df['x1'], df['x2'], color=colors, alpha=0.5, edgecolor=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Sequence Classification\n",
    "We perform PCA on the sequence embeddings and then do kmeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = protein_data['Function [CC]']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a 10-fold cross-validation to measure the performance of the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 1.0\n"
     ]
    }
   ],
   "source": [
    "kfold = 10\n",
    "X = pd.DataFrame(embedding)\n",
    "y = encoded_y\n",
    "\n",
    "random_state = 1\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "skf = KFold(n_splits = kfold, shuffle = True, random_state = random_state)\n",
    "k = 0\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape = (X_train.shape[1],))) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "    \n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred = model.predict_proba(X_train).round().astype(int)\n",
    "\n",
    "    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k+=1\n",
    "    \n",
    "print ('Average f1 score', np.mean(test_F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weblog Data Analysis\n",
    "This data sample is taken from https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset. \n",
    "This is a network intrusion data containing audit logs and any attack as a positive label. Since, network intrusion is a rare event, the data is unbalanced. Here we will,\n",
    "- build a sequence classification model to predict a network intrusion.\n",
    "\n",
    "Each sequence contains in the data is a series of activity, for example, {login, password}. The _alphabets_ in the input data sequences are already encoded into integers. The original sequences data file is also present in the `/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['timeduration', 'seqlen', 'seq', 'class'], dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "darpa_data = pd.read_csv('../data/darpa_data.csv')\n",
    "darpa_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = darpa_data['seq']\n",
    "sequences = [x.split('~') for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = darpa_data['class']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y = encoder.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating sequence embeddings\n",
    "In this data, the sequence embeddings should be length-sensitive. The lengths are important here because sequences with similar patterns but different lengths can have different labels. Consider a simple example of two sessions: `{login, pswd, login, pswd,...}` and `{login, pswd,...(repeated several times)..., login, pswd}`. While the first session can be a regular user mistyping the password once, the other session is possibly an attack to guess the password. Thus, the sequence lengths are as important as the patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgt_darpa = Sgt(kappa=5, lengthsensitive=True, mode='multiprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = sgt_darpa.fit_transform(corpus=sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2391</th>\n",
       "      <th>2392</th>\n",
       "      <th>2393</th>\n",
       "      <th>2394</th>\n",
       "      <th>2395</th>\n",
       "      <th>2396</th>\n",
       "      <th>2397</th>\n",
       "      <th>2398</th>\n",
       "      <th>2399</th>\n",
       "      <th>2400</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.069114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.804190e-09</td>\n",
       "      <td>7.041516e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.004958e-12</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>1.046458e-07</td>\n",
       "      <td>5.863092e-16</td>\n",
       "      <td>7.568986e-23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540296</td>\n",
       "      <td>5.739230e-32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.785666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.950089e-03</td>\n",
       "      <td>2.239981e-04</td>\n",
       "      <td>2.343180e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.528133</td>\n",
       "      <td>1.576703e-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.516644e-29</td>\n",
       "      <td>1.484843e-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1             2             3     4             5         6     \\\n",
       "0  0.069114   0.0  0.000000e+00  0.000000e+00   0.0  0.000000e+00  0.000000   \n",
       "1  0.000000   0.0  4.804190e-09  7.041516e-10   0.0  2.004958e-12  0.000132   \n",
       "2  0.000000   0.0  0.000000e+00  0.000000e+00   0.0  0.000000e+00  0.000000   \n",
       "3  0.785666   0.0  0.000000e+00  0.000000e+00   0.0  0.000000e+00  0.000000   \n",
       "4  0.000000   0.0  0.000000e+00  0.000000e+00   0.0  0.000000e+00  0.000000   \n",
       "\n",
       "           7             8             9     ...  2391  2392  2393  2394  \\\n",
       "0  0.000000e+00  0.000000e+00  0.000000e+00  ...   0.0   0.0   0.0   0.0   \n",
       "1  1.046458e-07  5.863092e-16  7.568986e-23  ...   0.0   0.0   0.0   0.0   \n",
       "2  0.000000e+00  0.000000e+00  0.000000e+00  ...   0.0   0.0   0.0   0.0   \n",
       "3  1.950089e-03  2.239981e-04  2.343180e-07  ...   0.0   0.0   0.0   0.0   \n",
       "4  0.000000e+00  0.000000e+00  0.000000e+00  ...   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   2395      2396          2397  2398          2399          2400  \n",
       "0   0.0  0.000000  0.000000e+00   0.0  0.000000e+00  0.000000e+00  \n",
       "1   0.0  0.540296  5.739230e-32   0.0  0.000000e+00  0.000000e+00  \n",
       "2   0.0  0.000000  0.000000e+00   0.0  0.000000e+00  0.000000e+00  \n",
       "3   0.0  0.528133  1.576703e-09   0.0  2.516644e-29  1.484843e-57  \n",
       "4   0.0  0.000000  0.000000e+00   0.0  0.000000e+00  0.000000e+00  \n",
       "\n",
       "[5 rows x 2401 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(embedding).to_csv(path_or_buf='tmp.csv', index=False)\n",
    "pd.DataFrame(embedding).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA on the embeddings\n",
    "The embeddings are sparse. We, therefore, apply PCA on the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9887812978739061\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=35)\n",
    "pca.fit(embedding)\n",
    "X = pca.transform(embedding)\n",
    "print(np.sum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Multi-Layer Perceptron Classifier\n",
    "The PCA transforms of the embeddings are used directly as inputs to an MLP classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_30 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_30 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_31 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 74 samples\n",
      "Epoch 1/300\n",
      "74/74 [==============================] - 0s 6ms/sample - loss: 0.1404 - accuracy: 0.6216\n",
      "Epoch 2/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.1386 - accuracy: 0.6486\n",
      "Epoch 3/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.1404 - accuracy: 0.7568\n",
      "Epoch 4/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.1309 - accuracy: 0.7297\n",
      "Epoch 5/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.1274 - accuracy: 0.7162\n",
      "Epoch 6/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.1142 - accuracy: 0.7568\n",
      "Epoch 7/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.1041 - accuracy: 0.8784\n",
      "Epoch 8/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.1027 - accuracy: 0.8243\n",
      "Epoch 9/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0991 - accuracy: 0.8378\n",
      "Epoch 10/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0862 - accuracy: 0.8649\n",
      "Epoch 11/300\n",
      "74/74 [==============================] - 0s 107us/sample - loss: 0.0930 - accuracy: 0.8649\n",
      "Epoch 12/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0898 - accuracy: 0.8649\n",
      "Epoch 13/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0827 - accuracy: 0.8784\n",
      "Epoch 14/300\n",
      "74/74 [==============================] - 0s 154us/sample - loss: 0.0790 - accuracy: 0.8784\n",
      "Epoch 15/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0769 - accuracy: 0.8649\n",
      "Epoch 16/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0801 - accuracy: 0.8514\n",
      "Epoch 17/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0740 - accuracy: 0.8784\n",
      "Epoch 18/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0723 - accuracy: 0.8649\n",
      "Epoch 19/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0679 - accuracy: 0.8649\n",
      "Epoch 20/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0704 - accuracy: 0.8919\n",
      "Epoch 21/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0621 - accuracy: 0.8649\n",
      "Epoch 22/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0627 - accuracy: 0.8919\n",
      "Epoch 23/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0552 - accuracy: 0.8784\n",
      "Epoch 24/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0599 - accuracy: 0.8784\n",
      "Epoch 25/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0596 - accuracy: 0.8514\n",
      "Epoch 26/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0579 - accuracy: 0.8784\n",
      "Epoch 27/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0513 - accuracy: 0.8784\n",
      "Epoch 28/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0533 - accuracy: 0.8784\n",
      "Epoch 29/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0559 - accuracy: 0.8784\n",
      "Epoch 30/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0537 - accuracy: 0.8649\n",
      "Epoch 31/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0472 - accuracy: 0.8649\n",
      "Epoch 32/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0494 - accuracy: 0.8514\n",
      "Epoch 33/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0511 - accuracy: 0.8649\n",
      "Epoch 34/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0473 - accuracy: 0.8649\n",
      "Epoch 35/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0507 - accuracy: 0.8649\n",
      "Epoch 36/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0468 - accuracy: 0.8649\n",
      "Epoch 37/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0459 - accuracy: 0.8649\n",
      "Epoch 38/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0428 - accuracy: 0.8649\n",
      "Epoch 39/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0439 - accuracy: 0.8649\n",
      "Epoch 40/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0388 - accuracy: 0.8649\n",
      "Epoch 41/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0406 - accuracy: 0.8649\n",
      "Epoch 42/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0450 - accuracy: 0.8919\n",
      "Epoch 43/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0403 - accuracy: 0.8784\n",
      "Epoch 44/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0463 - accuracy: 0.8649\n",
      "Epoch 45/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0443 - accuracy: 0.8784\n",
      "Epoch 46/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0437 - accuracy: 0.8514\n",
      "Epoch 47/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0379 - accuracy: 0.8919\n",
      "Epoch 48/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0388 - accuracy: 0.8784\n",
      "Epoch 49/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0403 - accuracy: 0.8784\n",
      "Epoch 50/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0344 - accuracy: 0.8919\n",
      "Epoch 51/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0378 - accuracy: 0.8649\n",
      "Epoch 52/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0403 - accuracy: 0.8784\n",
      "Epoch 53/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0372 - accuracy: 0.9054\n",
      "Epoch 54/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0397 - accuracy: 0.8649\n",
      "Epoch 55/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0408 - accuracy: 0.8784\n",
      "Epoch 56/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0422 - accuracy: 0.8649\n",
      "Epoch 57/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0372 - accuracy: 0.8649\n",
      "Epoch 58/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0380 - accuracy: 0.8649\n",
      "Epoch 59/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0413 - accuracy: 0.8649\n",
      "Epoch 60/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0327 - accuracy: 0.8649\n",
      "Epoch 61/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0358 - accuracy: 0.8649\n",
      "Epoch 62/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 63/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0393 - accuracy: 0.8649\n",
      "Epoch 64/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0387 - accuracy: 0.8784\n",
      "Epoch 65/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0366 - accuracy: 0.8649\n",
      "Epoch 66/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0328 - accuracy: 0.8784\n",
      "Epoch 67/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0390 - accuracy: 0.8649\n",
      "Epoch 68/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0324 - accuracy: 0.8919\n",
      "Epoch 69/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 70/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0328 - accuracy: 0.8784\n",
      "Epoch 71/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 72/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0383 - accuracy: 0.8514\n",
      "Epoch 73/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0366 - accuracy: 0.8649\n",
      "Epoch 74/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0359 - accuracy: 0.8919\n",
      "Epoch 75/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0395 - accuracy: 0.8514\n",
      "Epoch 76/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0363 - accuracy: 0.8649\n",
      "Epoch 77/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0346 - accuracy: 0.8784\n",
      "Epoch 78/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0370 - accuracy: 0.8649\n",
      "Epoch 79/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0319 - accuracy: 0.8919\n",
      "Epoch 80/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 81/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0365 - accuracy: 0.8649\n",
      "Epoch 82/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0359 - accuracy: 0.8514\n",
      "Epoch 83/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0319 - accuracy: 0.8784\n",
      "Epoch 84/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0361 - accuracy: 0.8649\n",
      "Epoch 85/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0294 - accuracy: 0.8784\n",
      "Epoch 86/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0360 - accuracy: 0.8784\n",
      "Epoch 87/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0325 - accuracy: 0.8784\n",
      "Epoch 88/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0303 - accuracy: 0.8919\n",
      "Epoch 89/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 90/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0347 - accuracy: 0.8784\n",
      "Epoch 91/300\n",
      "74/74 [==============================] - 0s 139us/sample - loss: 0.0379 - accuracy: 0.8649\n",
      "Epoch 92/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0382 - accuracy: 0.8514\n",
      "Epoch 93/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0349 - accuracy: 0.8919\n",
      "Epoch 94/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0274 - accuracy: 0.8919\n",
      "Epoch 95/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0368 - accuracy: 0.8514\n",
      "Epoch 96/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0281 - accuracy: 0.8649\n",
      "Epoch 97/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0291 - accuracy: 0.9054\n",
      "Epoch 98/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0299 - accuracy: 0.9054\n",
      "Epoch 99/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0287 - accuracy: 0.8649\n",
      "Epoch 100/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0353 - accuracy: 0.8649\n",
      "Epoch 101/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0316 - accuracy: 0.8919\n",
      "Epoch 102/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0299 - accuracy: 0.8649\n",
      "Epoch 103/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0353 - accuracy: 0.8784\n",
      "Epoch 104/300\n",
      "74/74 [==============================] - 0s 105us/sample - loss: 0.0347 - accuracy: 0.8514\n",
      "Epoch 105/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0294 - accuracy: 0.8784\n",
      "Epoch 106/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0344 - accuracy: 0.8784\n",
      "Epoch 107/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0323 - accuracy: 0.8919\n",
      "Epoch 108/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0297 - accuracy: 0.9189\n",
      "Epoch 109/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0333 - accuracy: 0.8649\n",
      "Epoch 110/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0300 - accuracy: 0.8649\n",
      "Epoch 111/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0369 - accuracy: 0.8514\n",
      "Epoch 112/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0323 - accuracy: 0.8919\n",
      "Epoch 113/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0361 - accuracy: 0.8919\n",
      "Epoch 114/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0336 - accuracy: 0.8649\n",
      "Epoch 115/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0291 - accuracy: 0.8649\n",
      "Epoch 116/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 117/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0288 - accuracy: 0.8649\n",
      "Epoch 118/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0329 - accuracy: 0.8919\n",
      "Epoch 119/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0393 - accuracy: 0.8784\n",
      "Epoch 120/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0234 - accuracy: 0.8919\n",
      "Epoch 121/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0381 - accuracy: 0.8784\n",
      "Epoch 122/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0319 - accuracy: 0.8784\n",
      "Epoch 123/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0286 - accuracy: 0.8919\n",
      "Epoch 124/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0335 - accuracy: 0.8784\n",
      "Epoch 125/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0324 - accuracy: 0.9054\n",
      "Epoch 126/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0268 - accuracy: 0.8784\n",
      "Epoch 127/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 128/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0326 - accuracy: 0.9054\n",
      "Epoch 129/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0305 - accuracy: 0.8784\n",
      "Epoch 130/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0306 - accuracy: 0.8784\n",
      "Epoch 131/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 132/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0312 - accuracy: 0.8784\n",
      "Epoch 133/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0330 - accuracy: 0.8919\n",
      "Epoch 134/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 135/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0330 - accuracy: 0.8649\n",
      "Epoch 136/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 137/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0363 - accuracy: 0.8514\n",
      "Epoch 138/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0363 - accuracy: 0.8649\n",
      "Epoch 139/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 140/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0341 - accuracy: 0.8649\n",
      "Epoch 141/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0298 - accuracy: 0.8919\n",
      "Epoch 142/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0370 - accuracy: 0.8514\n",
      "Epoch 143/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 144/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0293 - accuracy: 0.8649\n",
      "Epoch 145/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0380 - accuracy: 0.8514\n",
      "Epoch 146/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0315 - accuracy: 0.8784\n",
      "Epoch 147/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 148/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 149/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0286 - accuracy: 0.8649\n",
      "Epoch 150/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0278 - accuracy: 0.8784\n",
      "Epoch 151/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0297 - accuracy: 0.8784\n",
      "Epoch 152/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0302 - accuracy: 0.9189\n",
      "Epoch 153/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 154/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0299 - accuracy: 0.8784\n",
      "Epoch 155/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 156/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 157/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 158/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0308 - accuracy: 0.8784\n",
      "Epoch 159/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0295 - accuracy: 0.8649\n",
      "Epoch 160/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0323 - accuracy: 0.8514\n",
      "Epoch 161/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0314 - accuracy: 0.8919\n",
      "Epoch 162/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 163/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0304 - accuracy: 0.9189\n",
      "Epoch 164/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0275 - accuracy: 0.8919\n",
      "Epoch 165/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0327 - accuracy: 0.8784\n",
      "Epoch 166/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 167/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 168/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0341 - accuracy: 0.8649\n",
      "Epoch 169/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 170/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0270 - accuracy: 0.8649\n",
      "Epoch 171/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0300 - accuracy: 0.8649\n",
      "Epoch 172/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0298 - accuracy: 0.9054\n",
      "Epoch 173/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0270 - accuracy: 0.8919\n",
      "Epoch 174/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0293 - accuracy: 0.8649\n",
      "Epoch 175/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0337 - accuracy: 0.8649\n",
      "Epoch 176/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0313 - accuracy: 0.8784\n",
      "Epoch 177/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0327 - accuracy: 0.8784\n",
      "Epoch 178/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0380 - accuracy: 0.8649\n",
      "Epoch 179/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0295 - accuracy: 0.8649\n",
      "Epoch 180/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0337 - accuracy: 0.8514\n",
      "Epoch 181/300\n",
      "74/74 [==============================] - 0s 137us/sample - loss: 0.0344 - accuracy: 0.8649\n",
      "Epoch 182/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0355 - accuracy: 0.8514\n",
      "Epoch 183/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0330 - accuracy: 0.8784\n",
      "Epoch 184/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0295 - accuracy: 0.8784\n",
      "Epoch 185/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0368 - accuracy: 0.8514\n",
      "Epoch 186/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0339 - accuracy: 0.8649\n",
      "Epoch 187/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0283 - accuracy: 0.8649\n",
      "Epoch 188/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0309 - accuracy: 0.8649\n",
      "Epoch 189/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0315 - accuracy: 0.8919\n",
      "Epoch 190/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0285 - accuracy: 0.8649\n",
      "Epoch 191/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0339 - accuracy: 0.8649\n",
      "Epoch 192/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0285 - accuracy: 0.8784\n",
      "Epoch 193/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 194/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0355 - accuracy: 0.8784\n",
      "Epoch 195/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0392 - accuracy: 0.8514\n",
      "Epoch 196/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0282 - accuracy: 0.8784\n",
      "Epoch 197/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0302 - accuracy: 0.8649\n",
      "Epoch 198/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 199/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0274 - accuracy: 0.8784\n",
      "Epoch 200/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0289 - accuracy: 0.8784\n",
      "Epoch 201/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0375 - accuracy: 0.8514\n",
      "Epoch 202/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0337 - accuracy: 0.8649\n",
      "Epoch 203/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 204/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0303 - accuracy: 0.8649\n",
      "Epoch 205/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0335 - accuracy: 0.8784\n",
      "Epoch 206/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0304 - accuracy: 0.8649\n",
      "Epoch 207/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0339 - accuracy: 0.8649\n",
      "Epoch 208/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0261 - accuracy: 0.8784\n",
      "Epoch 209/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0304 - accuracy: 0.8649\n",
      "Epoch 210/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0303 - accuracy: 0.8649\n",
      "Epoch 211/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0318 - accuracy: 0.8784\n",
      "Epoch 212/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0358 - accuracy: 0.8919\n",
      "Epoch 213/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0272 - accuracy: 0.8784\n",
      "Epoch 214/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0293 - accuracy: 0.8649\n",
      "Epoch 215/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0347 - accuracy: 0.8649\n",
      "Epoch 216/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0302 - accuracy: 0.8649\n",
      "Epoch 217/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0331 - accuracy: 0.8784\n",
      "Epoch 218/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0283 - accuracy: 0.8784\n",
      "Epoch 219/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 220/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0291 - accuracy: 0.8919\n",
      "Epoch 221/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0323 - accuracy: 0.8784\n",
      "Epoch 222/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0341 - accuracy: 0.8784\n",
      "Epoch 223/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0310 - accuracy: 0.8919\n",
      "Epoch 224/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0337 - accuracy: 0.8784\n",
      "Epoch 225/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0359 - accuracy: 0.8649\n",
      "Epoch 226/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0355 - accuracy: 0.8649\n",
      "Epoch 227/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 228/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0353 - accuracy: 0.8649\n",
      "Epoch 229/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0323 - accuracy: 0.8784\n",
      "Epoch 230/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 231/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0350 - accuracy: 0.8649\n",
      "Epoch 232/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0279 - accuracy: 0.8919\n",
      "Epoch 233/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 234/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 235/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0327 - accuracy: 0.8649\n",
      "Epoch 236/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 237/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0292 - accuracy: 0.8919\n",
      "Epoch 238/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0320 - accuracy: 0.8919\n",
      "Epoch 239/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0312 - accuracy: 0.8649\n",
      "Epoch 240/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 241/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0296 - accuracy: 0.8649\n",
      "Epoch 242/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 243/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0258 - accuracy: 0.8784\n",
      "Epoch 244/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0316 - accuracy: 0.8919\n",
      "Epoch 245/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0321 - accuracy: 0.8784\n",
      "Epoch 246/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 247/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 248/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0275 - accuracy: 0.8784\n",
      "Epoch 249/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 250/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 251/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0311 - accuracy: 0.8919\n",
      "Epoch 252/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 253/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 254/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0359 - accuracy: 0.8514\n",
      "Epoch 255/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0326 - accuracy: 0.8784\n",
      "Epoch 256/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 257/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0343 - accuracy: 0.8784\n",
      "Epoch 258/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0309 - accuracy: 0.8649\n",
      "Epoch 259/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0301 - accuracy: 0.8649\n",
      "Epoch 260/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0315 - accuracy: 0.8649\n",
      "Epoch 261/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0342 - accuracy: 0.8649\n",
      "Epoch 262/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0300 - accuracy: 0.8649\n",
      "Epoch 263/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0358 - accuracy: 0.8649\n",
      "Epoch 264/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0295 - accuracy: 0.8649\n",
      "Epoch 265/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0356 - accuracy: 0.8649\n",
      "Epoch 266/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 267/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0298 - accuracy: 0.8784\n",
      "Epoch 268/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0278 - accuracy: 0.8649\n",
      "Epoch 269/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0302 - accuracy: 0.8649\n",
      "Epoch 270/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0305 - accuracy: 0.8649\n",
      "Epoch 271/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 272/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0322 - accuracy: 0.8784\n",
      "Epoch 273/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0302 - accuracy: 0.8649\n",
      "Epoch 274/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0309 - accuracy: 0.8649\n",
      "Epoch 275/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0296 - accuracy: 0.8649\n",
      "Epoch 276/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0285 - accuracy: 0.8649\n",
      "Epoch 277/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 278/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0349 - accuracy: 0.8514\n",
      "Epoch 279/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0347 - accuracy: 0.8649\n",
      "Epoch 280/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0320 - accuracy: 0.8649\n",
      "Epoch 281/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0350 - accuracy: 0.8784\n",
      "Epoch 282/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0320 - accuracy: 0.8649\n",
      "Epoch 283/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 284/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0283 - accuracy: 0.8649\n",
      "Epoch 285/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 286/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 287/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0306 - accuracy: 0.8784\n",
      "Epoch 288/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 289/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0347 - accuracy: 0.8514\n",
      "Epoch 290/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0362 - accuracy: 0.8514\n",
      "Epoch 291/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0330 - accuracy: 0.8649\n",
      "Epoch 292/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 293/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0326 - accuracy: 0.8649\n",
      "Epoch 294/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0346 - accuracy: 0.8649\n",
      "Epoch 295/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0335 - accuracy: 0.8649\n",
      "Epoch 296/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0304 - accuracy: 0.8649\n",
      "Epoch 297/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0303 - accuracy: 0.8784\n",
      "Epoch 298/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 299/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 300/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0297 - accuracy: 0.8649\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_32 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_33 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 74 samples\n",
      "Epoch 1/300\n",
      "74/74 [==============================] - 0s 6ms/sample - loss: 0.1394 - accuracy: 0.6757\n",
      "Epoch 2/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.1322 - accuracy: 0.7568\n",
      "Epoch 3/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.1254 - accuracy: 0.7973\n",
      "Epoch 4/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.1130 - accuracy: 0.7973\n",
      "Epoch 5/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.1276 - accuracy: 0.7568\n",
      "Epoch 6/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.1141 - accuracy: 0.9054\n",
      "Epoch 7/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.1047 - accuracy: 0.8514\n",
      "Epoch 8/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.1044 - accuracy: 0.8784\n",
      "Epoch 9/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.1066 - accuracy: 0.8919\n",
      "Epoch 10/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0914 - accuracy: 0.8919\n",
      "Epoch 11/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0893 - accuracy: 0.9054\n",
      "Epoch 12/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0854 - accuracy: 0.9054\n",
      "Epoch 13/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0835 - accuracy: 0.8919\n",
      "Epoch 14/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0761 - accuracy: 0.9054\n",
      "Epoch 15/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0776 - accuracy: 0.9189\n",
      "Epoch 16/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0744 - accuracy: 0.9189\n",
      "Epoch 17/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0717 - accuracy: 0.9189\n",
      "Epoch 18/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0722 - accuracy: 0.9054\n",
      "Epoch 19/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0662 - accuracy: 0.8919\n",
      "Epoch 20/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0679 - accuracy: 0.9189\n",
      "Epoch 21/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0633 - accuracy: 0.9189\n",
      "Epoch 22/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0597 - accuracy: 0.9189\n",
      "Epoch 23/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0615 - accuracy: 0.8919\n",
      "Epoch 24/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0586 - accuracy: 0.9189\n",
      "Epoch 25/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0625 - accuracy: 0.9054\n",
      "Epoch 26/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0528 - accuracy: 0.9189\n",
      "Epoch 27/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0592 - accuracy: 0.9054\n",
      "Epoch 28/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0547 - accuracy: 0.9189\n",
      "Epoch 29/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0532 - accuracy: 0.9054\n",
      "Epoch 30/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0529 - accuracy: 0.9189\n",
      "Epoch 31/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0491 - accuracy: 0.9189\n",
      "Epoch 32/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0557 - accuracy: 0.9189\n",
      "Epoch 33/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0480 - accuracy: 0.9189\n",
      "Epoch 34/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0480 - accuracy: 0.9189\n",
      "Epoch 35/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0449 - accuracy: 0.9189\n",
      "Epoch 36/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0452 - accuracy: 0.9189\n",
      "Epoch 37/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0472 - accuracy: 0.9189\n",
      "Epoch 38/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0494 - accuracy: 0.9189\n",
      "Epoch 39/300\n",
      "74/74 [==============================] - 0s 171us/sample - loss: 0.0431 - accuracy: 0.9189\n",
      "Epoch 40/300\n",
      "74/74 [==============================] - 0s 157us/sample - loss: 0.0429 - accuracy: 0.9189\n",
      "Epoch 41/300\n",
      "74/74 [==============================] - 0s 151us/sample - loss: 0.0438 - accuracy: 0.9189\n",
      "Epoch 42/300\n",
      "74/74 [==============================] - 0s 177us/sample - loss: 0.0412 - accuracy: 0.9189\n",
      "Epoch 43/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0427 - accuracy: 0.9189\n",
      "Epoch 44/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0416 - accuracy: 0.9054\n",
      "Epoch 45/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0382 - accuracy: 0.9189\n",
      "Epoch 46/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0392 - accuracy: 0.9189\n",
      "Epoch 47/300\n",
      "74/74 [==============================] - 0s 106us/sample - loss: 0.0406 - accuracy: 0.9189\n",
      "Epoch 48/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0368 - accuracy: 0.9189\n",
      "Epoch 49/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0412 - accuracy: 0.9189\n",
      "Epoch 50/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0352 - accuracy: 0.9189\n",
      "Epoch 51/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0420 - accuracy: 0.9189\n",
      "Epoch 52/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0377 - accuracy: 0.9189\n",
      "Epoch 53/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0392 - accuracy: 0.9189\n",
      "Epoch 54/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0380 - accuracy: 0.9189\n",
      "Epoch 55/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0352 - accuracy: 0.9189\n",
      "Epoch 56/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0348 - accuracy: 0.9189\n",
      "Epoch 57/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0365 - accuracy: 0.9189\n",
      "Epoch 58/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0319 - accuracy: 0.9189\n",
      "Epoch 59/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0350 - accuracy: 0.9054\n",
      "Epoch 60/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0373 - accuracy: 0.9189\n",
      "Epoch 61/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0382 - accuracy: 0.9189\n",
      "Epoch 62/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0346 - accuracy: 0.9189\n",
      "Epoch 63/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0312 - accuracy: 0.9189\n",
      "Epoch 64/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0349 - accuracy: 0.9189\n",
      "Epoch 65/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0345 - accuracy: 0.9189\n",
      "Epoch 66/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0312 - accuracy: 0.9189\n",
      "Epoch 67/300\n",
      "74/74 [==============================] - 0s 107us/sample - loss: 0.0322 - accuracy: 0.9189\n",
      "Epoch 68/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0307 - accuracy: 0.9189\n",
      "Epoch 69/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0308 - accuracy: 0.9189\n",
      "Epoch 70/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0368 - accuracy: 0.9189\n",
      "Epoch 71/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0312 - accuracy: 0.9189\n",
      "Epoch 72/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0337 - accuracy: 0.9189\n",
      "Epoch 73/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0319 - accuracy: 0.9189\n",
      "Epoch 74/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0344 - accuracy: 0.9189\n",
      "Epoch 75/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0338 - accuracy: 0.9189\n",
      "Epoch 76/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0303 - accuracy: 0.9189\n",
      "Epoch 77/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0303 - accuracy: 0.9189\n",
      "Epoch 78/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0250 - accuracy: 0.9189\n",
      "Epoch 79/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0301 - accuracy: 0.9189\n",
      "Epoch 80/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0333 - accuracy: 0.9189\n",
      "Epoch 81/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0294 - accuracy: 0.9189\n",
      "Epoch 82/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0271 - accuracy: 0.9189\n",
      "Epoch 83/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0327 - accuracy: 0.9189\n",
      "Epoch 84/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0297 - accuracy: 0.9189\n",
      "Epoch 85/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0309 - accuracy: 0.9189\n",
      "Epoch 86/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0331 - accuracy: 0.9189\n",
      "Epoch 87/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0327 - accuracy: 0.9189\n",
      "Epoch 88/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0323 - accuracy: 0.9189\n",
      "Epoch 89/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0275 - accuracy: 0.9189\n",
      "Epoch 90/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0304 - accuracy: 0.9189\n",
      "Epoch 91/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 92/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0331 - accuracy: 0.9189\n",
      "Epoch 93/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0298 - accuracy: 0.9189\n",
      "Epoch 94/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0311 - accuracy: 0.9189\n",
      "Epoch 95/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0310 - accuracy: 0.9189\n",
      "Epoch 96/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0309 - accuracy: 0.9189\n",
      "Epoch 97/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0292 - accuracy: 0.9189\n",
      "Epoch 98/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0311 - accuracy: 0.9189\n",
      "Epoch 99/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0261 - accuracy: 0.9189\n",
      "Epoch 100/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0305 - accuracy: 0.9189\n",
      "Epoch 101/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0282 - accuracy: 0.9189\n",
      "Epoch 102/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0355 - accuracy: 0.9189\n",
      "Epoch 103/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 104/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0286 - accuracy: 0.9189\n",
      "Epoch 105/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0310 - accuracy: 0.9189\n",
      "Epoch 106/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0269 - accuracy: 0.9189\n",
      "Epoch 107/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0344 - accuracy: 0.9189\n",
      "Epoch 108/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0323 - accuracy: 0.9189\n",
      "Epoch 109/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0309 - accuracy: 0.9189\n",
      "Epoch 110/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 111/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0305 - accuracy: 0.9189\n",
      "Epoch 112/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0274 - accuracy: 0.9189\n",
      "Epoch 113/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 114/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 115/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0274 - accuracy: 0.9189\n",
      "Epoch 116/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0311 - accuracy: 0.9189\n",
      "Epoch 117/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0318 - accuracy: 0.9189\n",
      "Epoch 118/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 119/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0305 - accuracy: 0.9189\n",
      "Epoch 120/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 121/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0269 - accuracy: 0.9189\n",
      "Epoch 122/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0323 - accuracy: 0.9189\n",
      "Epoch 123/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0291 - accuracy: 0.9189\n",
      "Epoch 124/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0292 - accuracy: 0.9189\n",
      "Epoch 125/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 126/300\n",
      "74/74 [==============================] - 0s 103us/sample - loss: 0.0257 - accuracy: 0.9189\n",
      "Epoch 127/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0321 - accuracy: 0.9189\n",
      "Epoch 128/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0274 - accuracy: 0.9189\n",
      "Epoch 129/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0284 - accuracy: 0.9189\n",
      "Epoch 130/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0285 - accuracy: 0.9189\n",
      "Epoch 131/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0272 - accuracy: 0.9189\n",
      "Epoch 132/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0295 - accuracy: 0.9189\n",
      "Epoch 133/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0273 - accuracy: 0.9189\n",
      "Epoch 134/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0293 - accuracy: 0.9189\n",
      "Epoch 135/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0295 - accuracy: 0.9189\n",
      "Epoch 136/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0271 - accuracy: 0.9189\n",
      "Epoch 137/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0292 - accuracy: 0.9189\n",
      "Epoch 138/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0285 - accuracy: 0.9189\n",
      "Epoch 139/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0299 - accuracy: 0.9189\n",
      "Epoch 140/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0308 - accuracy: 0.9189\n",
      "Epoch 141/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0256 - accuracy: 0.9189\n",
      "Epoch 142/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 143/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0256 - accuracy: 0.9189\n",
      "Epoch 144/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0294 - accuracy: 0.9189\n",
      "Epoch 145/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0275 - accuracy: 0.9189\n",
      "Epoch 146/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0321 - accuracy: 0.9189\n",
      "Epoch 147/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 148/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0271 - accuracy: 0.9189\n",
      "Epoch 149/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0286 - accuracy: 0.9189\n",
      "Epoch 150/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0337 - accuracy: 0.9189\n",
      "Epoch 151/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0277 - accuracy: 0.9189\n",
      "Epoch 152/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0290 - accuracy: 0.9189\n",
      "Epoch 153/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 154/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0257 - accuracy: 0.9189\n",
      "Epoch 155/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0263 - accuracy: 0.9189\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0299 - accuracy: 0.9189\n",
      "Epoch 157/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 158/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0325 - accuracy: 0.9189\n",
      "Epoch 159/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0247 - accuracy: 0.9189\n",
      "Epoch 160/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0305 - accuracy: 0.9189\n",
      "Epoch 161/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0252 - accuracy: 0.9189\n",
      "Epoch 162/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0282 - accuracy: 0.9189\n",
      "Epoch 163/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0255 - accuracy: 0.9189\n",
      "Epoch 164/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 165/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0250 - accuracy: 0.9189\n",
      "Epoch 166/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0300 - accuracy: 0.9189\n",
      "Epoch 167/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 168/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0321 - accuracy: 0.9189\n",
      "Epoch 169/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0246 - accuracy: 0.9189\n",
      "Epoch 170/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0230 - accuracy: 0.9189\n",
      "Epoch 171/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 172/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0263 - accuracy: 0.9189\n",
      "Epoch 173/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0305 - accuracy: 0.9189\n",
      "Epoch 174/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0262 - accuracy: 0.9189\n",
      "Epoch 175/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0276 - accuracy: 0.9189\n",
      "Epoch 176/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0254 - accuracy: 0.9189\n",
      "Epoch 177/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0311 - accuracy: 0.9189\n",
      "Epoch 178/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0311 - accuracy: 0.9189\n",
      "Epoch 179/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0278 - accuracy: 0.9189\n",
      "Epoch 180/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0313 - accuracy: 0.9189\n",
      "Epoch 181/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0291 - accuracy: 0.9189\n",
      "Epoch 182/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0277 - accuracy: 0.9189\n",
      "Epoch 183/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0276 - accuracy: 0.9189\n",
      "Epoch 184/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0247 - accuracy: 0.9189\n",
      "Epoch 185/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0314 - accuracy: 0.9189\n",
      "Epoch 186/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0270 - accuracy: 0.9189\n",
      "Epoch 187/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0306 - accuracy: 0.9189\n",
      "Epoch 188/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0250 - accuracy: 0.9189\n",
      "Epoch 189/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 190/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0304 - accuracy: 0.9189\n",
      "Epoch 191/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0286 - accuracy: 0.9189\n",
      "Epoch 192/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0278 - accuracy: 0.9189\n",
      "Epoch 193/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0225 - accuracy: 0.9189\n",
      "Epoch 194/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0266 - accuracy: 0.9189\n",
      "Epoch 195/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0260 - accuracy: 0.9189\n",
      "Epoch 196/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0254 - accuracy: 0.9189\n",
      "Epoch 197/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 198/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0248 - accuracy: 0.9189\n",
      "Epoch 199/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0285 - accuracy: 0.9189\n",
      "Epoch 200/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0237 - accuracy: 0.9189\n",
      "Epoch 201/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0291 - accuracy: 0.9189\n",
      "Epoch 202/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0290 - accuracy: 0.9189\n",
      "Epoch 203/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0292 - accuracy: 0.9189\n",
      "Epoch 204/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0307 - accuracy: 0.9189\n",
      "Epoch 205/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0264 - accuracy: 0.9189\n",
      "Epoch 206/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0288 - accuracy: 0.9189\n",
      "Epoch 207/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0305 - accuracy: 0.9189\n",
      "Epoch 208/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0252 - accuracy: 0.9189\n",
      "Epoch 209/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0273 - accuracy: 0.9189\n",
      "Epoch 210/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 211/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 212/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0253 - accuracy: 0.9189\n",
      "Epoch 213/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0244 - accuracy: 0.9189\n",
      "Epoch 214/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0276 - accuracy: 0.9189\n",
      "Epoch 215/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 216/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0260 - accuracy: 0.9189\n",
      "Epoch 217/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 218/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0301 - accuracy: 0.9189\n",
      "Epoch 219/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 220/300\n",
      "74/74 [==============================] - 0s 144us/sample - loss: 0.0254 - accuracy: 0.9189\n",
      "Epoch 221/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 222/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0238 - accuracy: 0.9189\n",
      "Epoch 223/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0303 - accuracy: 0.9189\n",
      "Epoch 224/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0244 - accuracy: 0.9189\n",
      "Epoch 225/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0270 - accuracy: 0.9189\n",
      "Epoch 226/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0297 - accuracy: 0.9189\n",
      "Epoch 227/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0266 - accuracy: 0.9189\n",
      "Epoch 228/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 229/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0240 - accuracy: 0.9189\n",
      "Epoch 230/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 231/300\n",
      "74/74 [==============================] - 0s 141us/sample - loss: 0.0309 - accuracy: 0.9189\n",
      "Epoch 232/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0305 - accuracy: 0.9189\n",
      "Epoch 233/300\n",
      "74/74 [==============================] - 0s 143us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 234/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 149us/sample - loss: 0.0267 - accuracy: 0.9189\n",
      "Epoch 235/300\n",
      "74/74 [==============================] - 0s 140us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 236/300\n",
      "74/74 [==============================] - 0s 145us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 237/300\n",
      "74/74 [==============================] - 0s 160us/sample - loss: 0.0263 - accuracy: 0.9189\n",
      "Epoch 238/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0286 - accuracy: 0.9189\n",
      "Epoch 239/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 240/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0254 - accuracy: 0.9189\n",
      "Epoch 241/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0297 - accuracy: 0.9189\n",
      "Epoch 242/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0276 - accuracy: 0.9189\n",
      "Epoch 243/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 244/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0280 - accuracy: 0.9189\n",
      "Epoch 245/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0295 - accuracy: 0.9189\n",
      "Epoch 246/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0250 - accuracy: 0.9189\n",
      "Epoch 247/300\n",
      "74/74 [==============================] - 0s 133us/sample - loss: 0.0298 - accuracy: 0.9189\n",
      "Epoch 248/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0273 - accuracy: 0.9189\n",
      "Epoch 249/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 250/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0232 - accuracy: 0.9189\n",
      "Epoch 251/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0288 - accuracy: 0.9189\n",
      "Epoch 252/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0255 - accuracy: 0.9189\n",
      "Epoch 253/300\n",
      "74/74 [==============================] - 0s 146us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 254/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0249 - accuracy: 0.9189\n",
      "Epoch 255/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0269 - accuracy: 0.9189\n",
      "Epoch 256/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 257/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0259 - accuracy: 0.9189\n",
      "Epoch 258/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0247 - accuracy: 0.9189\n",
      "Epoch 259/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0288 - accuracy: 0.9189\n",
      "Epoch 260/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0272 - accuracy: 0.9189\n",
      "Epoch 261/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0262 - accuracy: 0.9189\n",
      "Epoch 262/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0285 - accuracy: 0.9189\n",
      "Epoch 263/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0269 - accuracy: 0.9189\n",
      "Epoch 264/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0289 - accuracy: 0.9189\n",
      "Epoch 265/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0248 - accuracy: 0.9189\n",
      "Epoch 266/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0243 - accuracy: 0.9189\n",
      "Epoch 267/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0282 - accuracy: 0.9189\n",
      "Epoch 268/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0269 - accuracy: 0.9189\n",
      "Epoch 269/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0268 - accuracy: 0.9189\n",
      "Epoch 270/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0272 - accuracy: 0.9189\n",
      "Epoch 271/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0258 - accuracy: 0.9189\n",
      "Epoch 272/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0255 - accuracy: 0.9189\n",
      "Epoch 273/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 274/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0262 - accuracy: 0.9189\n",
      "Epoch 275/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0281 - accuracy: 0.9189\n",
      "Epoch 276/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0247 - accuracy: 0.9189\n",
      "Epoch 277/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0245 - accuracy: 0.9189\n",
      "Epoch 278/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0279 - accuracy: 0.9189\n",
      "Epoch 279/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0309 - accuracy: 0.9189\n",
      "Epoch 280/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0240 - accuracy: 0.9189\n",
      "Epoch 281/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0265 - accuracy: 0.9189\n",
      "Epoch 282/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0267 - accuracy: 0.9189\n",
      "Epoch 283/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0292 - accuracy: 0.9189\n",
      "Epoch 284/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0270 - accuracy: 0.9189\n",
      "Epoch 285/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0267 - accuracy: 0.9189\n",
      "Epoch 286/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0299 - accuracy: 0.9189\n",
      "Epoch 287/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0283 - accuracy: 0.9189\n",
      "Epoch 288/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0248 - accuracy: 0.9189\n",
      "Epoch 289/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0257 - accuracy: 0.9189\n",
      "Epoch 290/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0257 - accuracy: 0.9189\n",
      "Epoch 291/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0272 - accuracy: 0.9189\n",
      "Epoch 292/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0301 - accuracy: 0.9189\n",
      "Epoch 293/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0321 - accuracy: 0.9189\n",
      "Epoch 294/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0241 - accuracy: 0.9189\n",
      "Epoch 295/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0277 - accuracy: 0.9189\n",
      "Epoch 296/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0254 - accuracy: 0.9189\n",
      "Epoch 297/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0263 - accuracy: 0.9189\n",
      "Epoch 298/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0276 - accuracy: 0.9189\n",
      "Epoch 299/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0225 - accuracy: 0.9189\n",
      "Epoch 300/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0309 - accuracy: 0.9189\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 128)               4608      \n",
      "_________________________________________________________________\n",
      "activation_34 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_35 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train on 74 samples\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 6ms/sample - loss: 0.1366 - accuracy: 0.6081\n",
      "Epoch 2/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.1310 - accuracy: 0.7027\n",
      "Epoch 3/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.1200 - accuracy: 0.7027\n",
      "Epoch 4/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.1165 - accuracy: 0.7568\n",
      "Epoch 5/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.1130 - accuracy: 0.7973\n",
      "Epoch 6/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.1052 - accuracy: 0.7973\n",
      "Epoch 7/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.1005 - accuracy: 0.8649\n",
      "Epoch 8/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0983 - accuracy: 0.7973\n",
      "Epoch 9/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0957 - accuracy: 0.7838\n",
      "Epoch 10/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0917 - accuracy: 0.8514\n",
      "Epoch 11/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0894 - accuracy: 0.8243\n",
      "Epoch 12/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0860 - accuracy: 0.8514\n",
      "Epoch 13/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0821 - accuracy: 0.8243\n",
      "Epoch 14/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0786 - accuracy: 0.8378\n",
      "Epoch 15/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0756 - accuracy: 0.8514\n",
      "Epoch 16/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0779 - accuracy: 0.8784\n",
      "Epoch 17/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0761 - accuracy: 0.8514\n",
      "Epoch 18/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0713 - accuracy: 0.8378\n",
      "Epoch 19/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0686 - accuracy: 0.8649\n",
      "Epoch 20/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0678 - accuracy: 0.8649\n",
      "Epoch 21/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0627 - accuracy: 0.8784\n",
      "Epoch 22/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0680 - accuracy: 0.8649\n",
      "Epoch 23/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0629 - accuracy: 0.8514\n",
      "Epoch 24/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0601 - accuracy: 0.8649\n",
      "Epoch 25/300\n",
      "74/74 [==============================] - 0s 132us/sample - loss: 0.0617 - accuracy: 0.8514\n",
      "Epoch 26/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0586 - accuracy: 0.8649\n",
      "Epoch 27/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0536 - accuracy: 0.8784\n",
      "Epoch 28/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0583 - accuracy: 0.8649\n",
      "Epoch 29/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0503 - accuracy: 0.8784\n",
      "Epoch 30/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0528 - accuracy: 0.8784\n",
      "Epoch 31/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0542 - accuracy: 0.8514\n",
      "Epoch 32/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0519 - accuracy: 0.8649\n",
      "Epoch 33/300\n",
      "74/74 [==============================] - 0s 105us/sample - loss: 0.0576 - accuracy: 0.8514\n",
      "Epoch 34/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0538 - accuracy: 0.8649\n",
      "Epoch 35/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0475 - accuracy: 0.8649\n",
      "Epoch 36/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0530 - accuracy: 0.8514\n",
      "Epoch 37/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0467 - accuracy: 0.8784\n",
      "Epoch 38/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0480 - accuracy: 0.8649\n",
      "Epoch 39/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0444 - accuracy: 0.8784\n",
      "Epoch 40/300\n",
      "74/74 [==============================] - 0s 107us/sample - loss: 0.0432 - accuracy: 0.8784\n",
      "Epoch 41/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0445 - accuracy: 0.8649\n",
      "Epoch 42/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0530 - accuracy: 0.8514\n",
      "Epoch 43/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0483 - accuracy: 0.8649\n",
      "Epoch 44/300\n",
      "74/74 [==============================] - 0s 151us/sample - loss: 0.0430 - accuracy: 0.8919\n",
      "Epoch 45/300\n",
      "74/74 [==============================] - 0s 159us/sample - loss: 0.0415 - accuracy: 0.8649\n",
      "Epoch 46/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0478 - accuracy: 0.8514\n",
      "Epoch 47/300\n",
      "74/74 [==============================] - 0s 138us/sample - loss: 0.0407 - accuracy: 0.8649\n",
      "Epoch 48/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0394 - accuracy: 0.8649\n",
      "Epoch 49/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0384 - accuracy: 0.8649\n",
      "Epoch 50/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0406 - accuracy: 0.8649\n",
      "Epoch 51/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0440 - accuracy: 0.8649\n",
      "Epoch 52/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0469 - accuracy: 0.8649\n",
      "Epoch 53/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0407 - accuracy: 0.8649\n",
      "Epoch 54/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0403 - accuracy: 0.8784\n",
      "Epoch 55/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0400 - accuracy: 0.8649\n",
      "Epoch 56/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0371 - accuracy: 0.8784\n",
      "Epoch 57/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0431 - accuracy: 0.8784\n",
      "Epoch 58/300\n",
      "74/74 [==============================] - 0s 107us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 59/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0371 - accuracy: 0.8649\n",
      "Epoch 60/300\n",
      "74/74 [==============================] - 0s 136us/sample - loss: 0.0380 - accuracy: 0.8784\n",
      "Epoch 61/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0354 - accuracy: 0.8919\n",
      "Epoch 62/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0371 - accuracy: 0.8649\n",
      "Epoch 63/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0360 - accuracy: 0.8784\n",
      "Epoch 64/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0401 - accuracy: 0.8649\n",
      "Epoch 65/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0412 - accuracy: 0.8649\n",
      "Epoch 66/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0402 - accuracy: 0.8649\n",
      "Epoch 67/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0416 - accuracy: 0.8649\n",
      "Epoch 68/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0368 - accuracy: 0.8649\n",
      "Epoch 69/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0395 - accuracy: 0.8649\n",
      "Epoch 70/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0357 - accuracy: 0.8649\n",
      "Epoch 71/300\n",
      "74/74 [==============================] - 0s 135us/sample - loss: 0.0365 - accuracy: 0.8649\n",
      "Epoch 72/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0360 - accuracy: 0.8784\n",
      "Epoch 73/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0360 - accuracy: 0.8649\n",
      "Epoch 74/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0324 - accuracy: 0.8649\n",
      "Epoch 75/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 76/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0390 - accuracy: 0.8784\n",
      "Epoch 77/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0297 - accuracy: 0.8919\n",
      "Epoch 78/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0415 - accuracy: 0.8649\n",
      "Epoch 79/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0354 - accuracy: 0.8784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0345 - accuracy: 0.8784\n",
      "Epoch 81/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0377 - accuracy: 0.8784\n",
      "Epoch 82/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0308 - accuracy: 0.8649\n",
      "Epoch 83/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0358 - accuracy: 0.8784\n",
      "Epoch 84/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 85/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0348 - accuracy: 0.8514\n",
      "Epoch 86/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0329 - accuracy: 0.8784\n",
      "Epoch 87/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0290 - accuracy: 0.8919\n",
      "Epoch 88/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 89/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0373 - accuracy: 0.8514\n",
      "Epoch 90/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0381 - accuracy: 0.8649\n",
      "Epoch 91/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0346 - accuracy: 0.8649\n",
      "Epoch 92/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0322 - accuracy: 0.8649\n",
      "Epoch 93/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0305 - accuracy: 0.8919\n",
      "Epoch 94/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0336 - accuracy: 0.8649\n",
      "Epoch 95/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0360 - accuracy: 0.8784\n",
      "Epoch 96/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0322 - accuracy: 0.8784\n",
      "Epoch 97/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0354 - accuracy: 0.8784\n",
      "Epoch 98/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0312 - accuracy: 0.8919\n",
      "Epoch 99/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0387 - accuracy: 0.8649\n",
      "Epoch 100/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0311 - accuracy: 0.8784\n",
      "Epoch 101/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0271 - accuracy: 0.8919\n",
      "Epoch 102/300\n",
      "74/74 [==============================] - 0s 130us/sample - loss: 0.0351 - accuracy: 0.8784\n",
      "Epoch 103/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0384 - accuracy: 0.8919\n",
      "Epoch 104/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 105/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0350 - accuracy: 0.8649\n",
      "Epoch 106/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0337 - accuracy: 0.9054\n",
      "Epoch 107/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0355 - accuracy: 0.8649\n",
      "Epoch 108/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0310 - accuracy: 0.8784\n",
      "Epoch 109/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0339 - accuracy: 0.8649\n",
      "Epoch 110/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0336 - accuracy: 0.8784\n",
      "Epoch 111/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0296 - accuracy: 0.8649\n",
      "Epoch 112/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0323 - accuracy: 0.8919\n",
      "Epoch 113/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0323 - accuracy: 0.8784\n",
      "Epoch 114/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0350 - accuracy: 0.8649\n",
      "Epoch 115/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 116/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 117/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0316 - accuracy: 0.8784\n",
      "Epoch 118/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0305 - accuracy: 0.9054\n",
      "Epoch 119/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0377 - accuracy: 0.8784\n",
      "Epoch 120/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0328 - accuracy: 0.8919\n",
      "Epoch 121/300\n",
      "74/74 [==============================] - 0s 134us/sample - loss: 0.0345 - accuracy: 0.8649\n",
      "Epoch 122/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0392 - accuracy: 0.8649\n",
      "Epoch 123/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0340 - accuracy: 0.8784\n",
      "Epoch 124/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0294 - accuracy: 0.8919\n",
      "Epoch 125/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 126/300\n",
      "74/74 [==============================] - 0s 128us/sample - loss: 0.0322 - accuracy: 0.8649\n",
      "Epoch 127/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0325 - accuracy: 0.8649\n",
      "Epoch 128/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0371 - accuracy: 0.8514\n",
      "Epoch 129/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0303 - accuracy: 0.8784\n",
      "Epoch 130/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0398 - accuracy: 0.8514\n",
      "Epoch 131/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0323 - accuracy: 0.8784\n",
      "Epoch 132/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0292 - accuracy: 0.8784\n",
      "Epoch 133/300\n",
      "74/74 [==============================] - 0s 123us/sample - loss: 0.0293 - accuracy: 0.9054\n",
      "Epoch 134/300\n",
      "74/74 [==============================] - 0s 106us/sample - loss: 0.0304 - accuracy: 0.8784\n",
      "Epoch 135/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0292 - accuracy: 0.8919\n",
      "Epoch 136/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0385 - accuracy: 0.8514\n",
      "Epoch 137/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0302 - accuracy: 0.8784\n",
      "Epoch 138/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0291 - accuracy: 0.8784\n",
      "Epoch 139/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0323 - accuracy: 0.8919\n",
      "Epoch 140/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0307 - accuracy: 0.8784\n",
      "Epoch 141/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0298 - accuracy: 0.8784\n",
      "Epoch 142/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0295 - accuracy: 0.8784\n",
      "Epoch 143/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0334 - accuracy: 0.8649\n",
      "Epoch 144/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 145/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0347 - accuracy: 0.8649\n",
      "Epoch 146/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0310 - accuracy: 0.8649\n",
      "Epoch 147/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0340 - accuracy: 0.8649\n",
      "Epoch 148/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0349 - accuracy: 0.8784\n",
      "Epoch 149/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0291 - accuracy: 0.8784\n",
      "Epoch 150/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0288 - accuracy: 0.8649\n",
      "Epoch 151/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0370 - accuracy: 0.8784\n",
      "Epoch 152/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0298 - accuracy: 0.8784\n",
      "Epoch 153/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0328 - accuracy: 0.8784\n",
      "Epoch 154/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0327 - accuracy: 0.8649\n",
      "Epoch 155/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0340 - accuracy: 0.8514\n",
      "Epoch 156/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0276 - accuracy: 0.8649\n",
      "Epoch 157/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0308 - accuracy: 0.8649\n",
      "Epoch 158/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0373 - accuracy: 0.8649\n",
      "Epoch 159/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0297 - accuracy: 0.8649\n",
      "Epoch 160/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0347 - accuracy: 0.8784\n",
      "Epoch 161/300\n",
      "74/74 [==============================] - 0s 142us/sample - loss: 0.0319 - accuracy: 0.8784\n",
      "Epoch 162/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0301 - accuracy: 0.8649\n",
      "Epoch 163/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0305 - accuracy: 0.8919\n",
      "Epoch 164/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 165/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0336 - accuracy: 0.8649\n",
      "Epoch 166/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 167/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0319 - accuracy: 0.8784\n",
      "Epoch 168/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0318 - accuracy: 0.8919\n",
      "Epoch 169/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0302 - accuracy: 0.8784\n",
      "Epoch 170/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0327 - accuracy: 0.8649\n",
      "Epoch 171/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0303 - accuracy: 0.8784\n",
      "Epoch 172/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0315 - accuracy: 0.8919\n",
      "Epoch 173/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 174/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0285 - accuracy: 0.8649\n",
      "Epoch 175/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0320 - accuracy: 0.8919\n",
      "Epoch 176/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 177/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0338 - accuracy: 0.8919\n",
      "Epoch 178/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0333 - accuracy: 0.8784\n",
      "Epoch 179/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0242 - accuracy: 0.9054\n",
      "Epoch 180/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0379 - accuracy: 0.8649\n",
      "Epoch 181/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0308 - accuracy: 0.8919\n",
      "Epoch 182/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0340 - accuracy: 0.8784\n",
      "Epoch 183/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0352 - accuracy: 0.8784\n",
      "Epoch 184/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0267 - accuracy: 0.9054\n",
      "Epoch 185/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0274 - accuracy: 0.8784\n",
      "Epoch 186/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0342 - accuracy: 0.8784\n",
      "Epoch 187/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 188/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0363 - accuracy: 0.8649\n",
      "Epoch 189/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0295 - accuracy: 0.8784\n",
      "Epoch 190/300\n",
      "74/74 [==============================] - 0s 107us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 191/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0307 - accuracy: 0.8784\n",
      "Epoch 192/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0287 - accuracy: 0.8784\n",
      "Epoch 193/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0304 - accuracy: 0.8919\n",
      "Epoch 194/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0301 - accuracy: 0.8649\n",
      "Epoch 195/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 196/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0355 - accuracy: 0.8514\n",
      "Epoch 197/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 198/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0389 - accuracy: 0.8514\n",
      "Epoch 199/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0331 - accuracy: 0.8649\n",
      "Epoch 200/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0315 - accuracy: 0.8919\n",
      "Epoch 201/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0311 - accuracy: 0.8784\n",
      "Epoch 202/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0384 - accuracy: 0.8649\n",
      "Epoch 203/300\n",
      "74/74 [==============================] - 0s 104us/sample - loss: 0.0288 - accuracy: 0.8649\n",
      "Epoch 204/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0279 - accuracy: 0.8919\n",
      "Epoch 205/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0381 - accuracy: 0.8514\n",
      "Epoch 206/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0280 - accuracy: 0.8784\n",
      "Epoch 207/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0319 - accuracy: 0.8514\n",
      "Epoch 208/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0324 - accuracy: 0.8919\n",
      "Epoch 209/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0294 - accuracy: 0.8784\n",
      "Epoch 210/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 211/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 212/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0298 - accuracy: 0.8784\n",
      "Epoch 213/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0333 - accuracy: 0.8514\n",
      "Epoch 214/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0302 - accuracy: 0.8649\n",
      "Epoch 215/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0306 - accuracy: 0.8919\n",
      "Epoch 216/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0307 - accuracy: 0.8649\n",
      "Epoch 217/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 218/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0337 - accuracy: 0.8649\n",
      "Epoch 219/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0311 - accuracy: 0.8649\n",
      "Epoch 220/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0345 - accuracy: 0.8649\n",
      "Epoch 221/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0351 - accuracy: 0.8649\n",
      "Epoch 222/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0289 - accuracy: 0.8784\n",
      "Epoch 223/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0295 - accuracy: 0.8649\n",
      "Epoch 224/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0302 - accuracy: 0.8649\n",
      "Epoch 225/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0366 - accuracy: 0.8514\n",
      "Epoch 226/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0282 - accuracy: 0.8649\n",
      "Epoch 227/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 228/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0319 - accuracy: 0.8784\n",
      "Epoch 229/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 230/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0320 - accuracy: 0.8649\n",
      "Epoch 231/300\n",
      "74/74 [==============================] - 0s 125us/sample - loss: 0.0369 - accuracy: 0.8649\n",
      "Epoch 232/300\n",
      "74/74 [==============================] - 0s 104us/sample - loss: 0.0306 - accuracy: 0.8649\n",
      "Epoch 233/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0324 - accuracy: 0.8784\n",
      "Epoch 234/300\n",
      "74/74 [==============================] - 0s 107us/sample - loss: 0.0245 - accuracy: 0.8919\n",
      "Epoch 235/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0406 - accuracy: 0.8514\n",
      "Epoch 236/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0310 - accuracy: 0.8649\n",
      "Epoch 237/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0309 - accuracy: 0.8649\n",
      "Epoch 238/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 239/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0293 - accuracy: 0.8649\n",
      "Epoch 240/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0354 - accuracy: 0.8649\n",
      "Epoch 241/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0353 - accuracy: 0.8784\n",
      "Epoch 242/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0329 - accuracy: 0.8649\n",
      "Epoch 243/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0321 - accuracy: 0.8649\n",
      "Epoch 244/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0279 - accuracy: 0.8649\n",
      "Epoch 245/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0312 - accuracy: 0.8649\n",
      "Epoch 246/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0355 - accuracy: 0.8649\n",
      "Epoch 247/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0292 - accuracy: 0.8649\n",
      "Epoch 248/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0353 - accuracy: 0.8649\n",
      "Epoch 249/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0272 - accuracy: 0.9054\n",
      "Epoch 250/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0301 - accuracy: 0.8649\n",
      "Epoch 251/300\n",
      "74/74 [==============================] - 0s 119us/sample - loss: 0.0294 - accuracy: 0.8784\n",
      "Epoch 252/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0270 - accuracy: 0.8919\n",
      "Epoch 253/300\n",
      "74/74 [==============================] - 0s 117us/sample - loss: 0.0326 - accuracy: 0.8649\n",
      "Epoch 254/300\n",
      "74/74 [==============================] - 0s 107us/sample - loss: 0.0338 - accuracy: 0.8649\n",
      "Epoch 255/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0295 - accuracy: 0.8784\n",
      "Epoch 256/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0312 - accuracy: 0.8784\n",
      "Epoch 257/300\n",
      "74/74 [==============================] - 0s 124us/sample - loss: 0.0346 - accuracy: 0.8514\n",
      "Epoch 258/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 259/300\n",
      "74/74 [==============================] - 0s 107us/sample - loss: 0.0308 - accuracy: 0.8649\n",
      "Epoch 260/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0315 - accuracy: 0.8649\n",
      "Epoch 261/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0296 - accuracy: 0.8649\n",
      "Epoch 262/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0304 - accuracy: 0.8784\n",
      "Epoch 263/300\n",
      "74/74 [==============================] - 0s 110us/sample - loss: 0.0290 - accuracy: 0.8649\n",
      "Epoch 264/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0315 - accuracy: 0.8784\n",
      "Epoch 265/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0350 - accuracy: 0.8649\n",
      "Epoch 266/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 267/300\n",
      "74/74 [==============================] - 0s 127us/sample - loss: 0.0289 - accuracy: 0.8784\n",
      "Epoch 268/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0344 - accuracy: 0.8784\n",
      "Epoch 269/300\n",
      "74/74 [==============================] - 0s 126us/sample - loss: 0.0318 - accuracy: 0.8649\n",
      "Epoch 270/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0305 - accuracy: 0.8649\n",
      "Epoch 271/300\n",
      "74/74 [==============================] - 0s 173us/sample - loss: 0.0291 - accuracy: 0.8649\n",
      "Epoch 272/300\n",
      "74/74 [==============================] - 0s 153us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 273/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 274/300\n",
      "74/74 [==============================] - 0s 122us/sample - loss: 0.0281 - accuracy: 0.8649\n",
      "Epoch 275/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0291 - accuracy: 0.8649\n",
      "Epoch 276/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0311 - accuracy: 0.8649\n",
      "Epoch 277/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0309 - accuracy: 0.8919\n",
      "Epoch 278/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0338 - accuracy: 0.8649\n",
      "Epoch 279/300\n",
      "74/74 [==============================] - 0s 129us/sample - loss: 0.0329 - accuracy: 0.8784\n",
      "Epoch 280/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0331 - accuracy: 0.8919\n",
      "Epoch 281/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 282/300\n",
      "74/74 [==============================] - 0s 112us/sample - loss: 0.0272 - accuracy: 0.8784\n",
      "Epoch 283/300\n",
      "74/74 [==============================] - 0s 113us/sample - loss: 0.0310 - accuracy: 0.8784\n",
      "Epoch 284/300\n",
      "74/74 [==============================] - 0s 116us/sample - loss: 0.0309 - accuracy: 0.8784\n",
      "Epoch 285/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0332 - accuracy: 0.8649\n",
      "Epoch 286/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0340 - accuracy: 0.8649\n",
      "Epoch 287/300\n",
      "74/74 [==============================] - 0s 108us/sample - loss: 0.0311 - accuracy: 0.8649\n",
      "Epoch 288/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0328 - accuracy: 0.8649\n",
      "Epoch 289/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 290/300\n",
      "74/74 [==============================] - 0s 118us/sample - loss: 0.0357 - accuracy: 0.8649\n",
      "Epoch 291/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0334 - accuracy: 0.8784\n",
      "Epoch 292/300\n",
      "74/74 [==============================] - 0s 121us/sample - loss: 0.0343 - accuracy: 0.8649\n",
      "Epoch 293/300\n",
      "74/74 [==============================] - 0s 131us/sample - loss: 0.0305 - accuracy: 0.8649\n",
      "Epoch 294/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0313 - accuracy: 0.8649\n",
      "Epoch 295/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0342 - accuracy: 0.8649\n",
      "Epoch 296/300\n",
      "74/74 [==============================] - 0s 109us/sample - loss: 0.0319 - accuracy: 0.8649\n",
      "Epoch 297/300\n",
      "74/74 [==============================] - 0s 120us/sample - loss: 0.0323 - accuracy: 0.8649\n",
      "Epoch 298/300\n",
      "74/74 [==============================] - 0s 115us/sample - loss: 0.0349 - accuracy: 0.8649\n",
      "Epoch 299/300\n",
      "74/74 [==============================] - 0s 114us/sample - loss: 0.0316 - accuracy: 0.8649\n",
      "Epoch 300/300\n",
      "74/74 [==============================] - 0s 111us/sample - loss: 0.0330 - accuracy: 0.8649\n"
     ]
    }
   ],
   "source": [
    "kfold = 3\n",
    "random_state = 11\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "time_k = np.zeros(kfold)\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "epochs = 300\n",
    "batch_size = 15\n",
    "\n",
    "# class_weight = {0 : 1., 1: 1.,}  # The weights can be changed and made inversely proportional to the class size to improve the accuracy.\n",
    "class_weight = {0 : 0.12, 1: 0.88,}\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(X_train.shape[1],))) \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weight)\n",
    "    end_time = time.time()\n",
    "    time_k[k] = end_time-start_time\n",
    "\n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred = model.predict_proba(X_train).round().astype(int)\n",
    "    test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 0.6\n",
      "Average Run time 3.3290751775105796\n"
     ]
    }
   ],
   "source": [
    "print ('Average f1 score', np.mean(test_F1))\n",
    "print ('Average Run time', np.mean(time_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building an LSTM Classifier on the sequences for comparison\n",
    "We built an LSTM Classifier on the sequences to compare the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = darpa_data['seq']\n",
    "encoded_X = np.ndarray(shape=(len(X),), dtype=list)\n",
    "for i in range(0,len(X)):\n",
    "    encoded_X[i]=X.iloc[i].split(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = np.max(darpa_data['seqlen'])\n",
    "encoded_X = tf.keras.preprocessing.sequence.pad_sequences(encoded_X, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_36 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74 samples\n",
      "Epoch 1/50\n",
      "74/74 [==============================] - 4s 60ms/sample - loss: 0.6934 - accuracy: 0.5135\n",
      "Epoch 2/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.6591 - accuracy: 0.8784\n",
      "Epoch 3/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.6201 - accuracy: 0.8784\n",
      "Epoch 4/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.5612 - accuracy: 0.8784\n",
      "Epoch 5/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.4500 - accuracy: 0.8784\n",
      "Epoch 6/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.3808 - accuracy: 0.8784\n",
      "Epoch 7/50\n",
      "74/74 [==============================] - 4s 49ms/sample - loss: 0.3807 - accuracy: 0.8784\n",
      "Epoch 8/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3795 - accuracy: 0.8784\n",
      "Epoch 9/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3718 - accuracy: 0.8784\n",
      "Epoch 10/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3713 - accuracy: 0.8784\n",
      "Epoch 11/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3697 - accuracy: 0.8784\n",
      "Epoch 12/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.3696 - accuracy: 0.8784\n",
      "Epoch 13/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3696 - accuracy: 0.8784\n",
      "Epoch 14/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3677 - accuracy: 0.8784\n",
      "Epoch 15/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3666 - accuracy: 0.8784\n",
      "Epoch 16/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3661 - accuracy: 0.8784\n",
      "Epoch 17/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3654 - accuracy: 0.8784\n",
      "Epoch 18/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3634 - accuracy: 0.8784\n",
      "Epoch 19/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3638 - accuracy: 0.8784\n",
      "Epoch 20/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3598 - accuracy: 0.8784\n",
      "Epoch 21/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3584 - accuracy: 0.8784\n",
      "Epoch 22/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3539 - accuracy: 0.8784\n",
      "Epoch 23/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3588 - accuracy: 0.8784\n",
      "Epoch 24/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3374 - accuracy: 0.8784\n",
      "Epoch 25/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3356 - accuracy: 0.8784\n",
      "Epoch 26/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3044 - accuracy: 0.8784\n",
      "Epoch 27/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.2896 - accuracy: 0.8784\n",
      "Epoch 28/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2864 - accuracy: 0.8784\n",
      "Epoch 29/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2430 - accuracy: 0.8784\n",
      "Epoch 30/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2675 - accuracy: 0.8784\n",
      "Epoch 31/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2764 - accuracy: 0.8784\n",
      "Epoch 32/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2404 - accuracy: 0.8784\n",
      "Epoch 33/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2131 - accuracy: 0.8784\n",
      "Epoch 34/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2109 - accuracy: 0.8784\n",
      "Epoch 35/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2060 - accuracy: 0.8919\n",
      "Epoch 36/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1925 - accuracy: 0.9054\n",
      "Epoch 37/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1913 - accuracy: 0.9189\n",
      "Epoch 38/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1947 - accuracy: 0.9324\n",
      "Epoch 39/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1762 - accuracy: 0.9324\n",
      "Epoch 40/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1856 - accuracy: 0.9459\n",
      "Epoch 41/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1689 - accuracy: 0.9324\n",
      "Epoch 42/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1762 - accuracy: 0.9324\n",
      "Epoch 43/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.1914 - accuracy: 0.9459\n",
      "Epoch 44/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.1867 - accuracy: 0.9595\n",
      "Epoch 45/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.1602 - accuracy: 0.9459\n",
      "Epoch 46/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1627 - accuracy: 0.9324\n",
      "Epoch 47/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1475 - accuracy: 0.9595\n",
      "Epoch 48/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1527 - accuracy: 0.9595\n",
      "Epoch 49/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1408 - accuracy: 0.9595\n",
      "Epoch 50/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1745 - accuracy: 0.9595\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74 samples\n",
      "Epoch 1/50\n",
      "74/74 [==============================] - 4s 59ms/sample - loss: 0.6898 - accuracy: 0.5676\n",
      "Epoch 2/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.6513 - accuracy: 0.8784\n",
      "Epoch 3/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.6120 - accuracy: 0.8784\n",
      "Epoch 4/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.5458 - accuracy: 0.8784\n",
      "Epoch 5/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.4240 - accuracy: 0.8784\n",
      "Epoch 6/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3963 - accuracy: 0.8784\n",
      "Epoch 7/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3924 - accuracy: 0.8784\n",
      "Epoch 8/50\n",
      "74/74 [==============================] - 3s 40ms/sample - loss: 0.3851 - accuracy: 0.8784\n",
      "Epoch 9/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3731 - accuracy: 0.8784\n",
      "Epoch 10/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3708 - accuracy: 0.8784\n",
      "Epoch 11/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.3737 - accuracy: 0.8784\n",
      "Epoch 12/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3716 - accuracy: 0.8784\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3706 - accuracy: 0.8784\n",
      "Epoch 14/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3697 - accuracy: 0.8784\n",
      "Epoch 15/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3698 - accuracy: 0.8784\n",
      "Epoch 16/50\n",
      "74/74 [==============================] - 3s 44ms/sample - loss: 0.3686 - accuracy: 0.8784\n",
      "Epoch 17/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3686 - accuracy: 0.8784\n",
      "Epoch 18/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3682 - accuracy: 0.8784\n",
      "Epoch 19/50\n",
      "74/74 [==============================] - 3s 44ms/sample - loss: 0.3667 - accuracy: 0.8784\n",
      "Epoch 20/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3678 - accuracy: 0.8784\n",
      "Epoch 21/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3640 - accuracy: 0.8784\n",
      "Epoch 22/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3621 - accuracy: 0.8784\n",
      "Epoch 23/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3601 - accuracy: 0.8784\n",
      "Epoch 24/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3574 - accuracy: 0.8784\n",
      "Epoch 25/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3514 - accuracy: 0.8784\n",
      "Epoch 26/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3552 - accuracy: 0.8784\n",
      "Epoch 27/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3381 - accuracy: 0.8784\n",
      "Epoch 28/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3274 - accuracy: 0.8784\n",
      "Epoch 29/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3118 - accuracy: 0.8784\n",
      "Epoch 30/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2943 - accuracy: 0.8784\n",
      "Epoch 31/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.2783 - accuracy: 0.8784\n",
      "Epoch 32/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.2459 - accuracy: 0.8784\n",
      "Epoch 33/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.2276 - accuracy: 0.8919\n",
      "Epoch 34/50\n",
      "74/74 [==============================] - 3s 46ms/sample - loss: 0.2345 - accuracy: 0.9189\n",
      "Epoch 35/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.1888 - accuracy: 0.9189\n",
      "Epoch 36/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.2413 - accuracy: 0.9189\n",
      "Epoch 37/50\n",
      "74/74 [==============================] - 3s 44ms/sample - loss: 0.2389 - accuracy: 0.8649\n",
      "Epoch 38/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.2136 - accuracy: 0.9054\n",
      "Epoch 39/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.1933 - accuracy: 0.9054\n",
      "Epoch 40/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.1882 - accuracy: 0.8919\n",
      "Epoch 41/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.1999 - accuracy: 0.9054\n",
      "Epoch 42/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1760 - accuracy: 0.8919\n",
      "Epoch 43/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.1990 - accuracy: 0.8243\n",
      "Epoch 44/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1632 - accuracy: 0.9189\n",
      "Epoch 45/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1626 - accuracy: 0.9189\n",
      "Epoch 46/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1700 - accuracy: 0.8784\n",
      "Epoch 47/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1529 - accuracy: 0.9189\n",
      "Epoch 48/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1641 - accuracy: 0.9189\n",
      "Epoch 49/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1482 - accuracy: 0.9189\n",
      "Epoch 50/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.1661 - accuracy: 0.8784\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1773, 32)          1600      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 9,953\n",
      "Trainable params: 9,953\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 74 samples\n",
      "Epoch 1/50\n",
      "74/74 [==============================] - 5s 63ms/sample - loss: 0.6756 - accuracy: 0.8919\n",
      "Epoch 2/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.6397 - accuracy: 0.8919\n",
      "Epoch 3/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.5892 - accuracy: 0.8919\n",
      "Epoch 4/50\n",
      "74/74 [==============================] - 3s 40ms/sample - loss: 0.5005 - accuracy: 0.8919\n",
      "Epoch 5/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3800 - accuracy: 0.8919\n",
      "Epoch 6/50\n",
      "74/74 [==============================] - 3s 40ms/sample - loss: 0.3459 - accuracy: 0.8919\n",
      "Epoch 7/50\n",
      "74/74 [==============================] - 3s 40ms/sample - loss: 0.3529 - accuracy: 0.8919\n",
      "Epoch 8/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3502 - accuracy: 0.8919\n",
      "Epoch 9/50\n",
      "74/74 [==============================] - 3s 40ms/sample - loss: 0.3455 - accuracy: 0.8919\n",
      "Epoch 10/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3438 - accuracy: 0.8919\n",
      "Epoch 11/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3434 - accuracy: 0.8919\n",
      "Epoch 12/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3431 - accuracy: 0.8919\n",
      "Epoch 13/50\n",
      "74/74 [==============================] - 3s 40ms/sample - loss: 0.3433 - accuracy: 0.8919\n",
      "Epoch 14/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3433 - accuracy: 0.8919\n",
      "Epoch 15/50\n",
      "74/74 [==============================] - 3s 44ms/sample - loss: 0.3432 - accuracy: 0.8919\n",
      "Epoch 16/50\n",
      "74/74 [==============================] - 3s 44ms/sample - loss: 0.3421 - accuracy: 0.8919\n",
      "Epoch 17/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3426 - accuracy: 0.8919\n",
      "Epoch 18/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3423 - accuracy: 0.8919\n",
      "Epoch 19/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3424 - accuracy: 0.8919\n",
      "Epoch 20/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.3420 - accuracy: 0.8919\n",
      "Epoch 21/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3429 - accuracy: 0.8919\n",
      "Epoch 22/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3412 - accuracy: 0.8919\n",
      "Epoch 23/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3402 - accuracy: 0.8919\n",
      "Epoch 24/50\n",
      "74/74 [==============================] - 3s 40ms/sample - loss: 0.3397 - accuracy: 0.8919\n",
      "Epoch 25/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3390 - accuracy: 0.8919\n",
      "Epoch 26/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3398 - accuracy: 0.8919\n",
      "Epoch 27/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3372 - accuracy: 0.8919\n",
      "Epoch 28/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3374 - accuracy: 0.8919\n",
      "Epoch 29/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3323 - accuracy: 0.8919\n",
      "Epoch 30/50\n",
      "74/74 [==============================] - 3s 44ms/sample - loss: 0.3323 - accuracy: 0.8919\n",
      "Epoch 31/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.3253 - accuracy: 0.8919\n",
      "Epoch 32/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.3228 - accuracy: 0.8919\n",
      "Epoch 33/50\n",
      "74/74 [==============================] - 3s 40ms/sample - loss: 0.3075 - accuracy: 0.8919\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2985 - accuracy: 0.8919\n",
      "Epoch 35/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.3000 - accuracy: 0.8919\n",
      "Epoch 36/50\n",
      "74/74 [==============================] - 3s 44ms/sample - loss: 0.2791 - accuracy: 0.8919\n",
      "Epoch 37/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.2580 - accuracy: 0.8919\n",
      "Epoch 38/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.2874 - accuracy: 0.8919\n",
      "Epoch 39/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.2712 - accuracy: 0.8919\n",
      "Epoch 40/50\n",
      "74/74 [==============================] - 3s 44ms/sample - loss: 0.2432 - accuracy: 0.8919\n",
      "Epoch 41/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.2231 - accuracy: 0.8919\n",
      "Epoch 42/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.2146 - accuracy: 0.8919\n",
      "Epoch 43/50\n",
      "74/74 [==============================] - 3s 45ms/sample - loss: 0.2026 - accuracy: 0.8919\n",
      "Epoch 44/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.2371 - accuracy: 0.9054\n",
      "Epoch 45/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.2293 - accuracy: 0.9189\n",
      "Epoch 46/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2524 - accuracy: 0.9324\n",
      "Epoch 47/50\n",
      "74/74 [==============================] - 3s 42ms/sample - loss: 0.2331 - accuracy: 0.9189\n",
      "Epoch 48/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.2046 - accuracy: 0.8919\n",
      "Epoch 49/50\n",
      "74/74 [==============================] - 3s 43ms/sample - loss: 0.2020 - accuracy: 0.8919\n",
      "Epoch 50/50\n",
      "74/74 [==============================] - 3s 41ms/sample - loss: 0.1992 - accuracy: 0.9054\n"
     ]
    }
   ],
   "source": [
    "kfold = 3\n",
    "random_state = 11\n",
    "\n",
    "test_F1 = np.zeros(kfold)\n",
    "time_k = np.zeros(kfold)\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 15\n",
    "skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)\n",
    "k = 0\n",
    "\n",
    "for train_index, test_index in skf.split(encoded_X, y):\n",
    "    X_train, X_test = encoded_X[train_index], encoded_X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    embedding_vecor_length = 32\n",
    "    top_words=50\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    end_time=time.time()\n",
    "    time_k[k]=end_time-start_time\n",
    "\n",
    "    y_pred = model.predict_proba(X_test).round().astype(int)\n",
    "    y_train_pred=model.predict_proba(X_train).round().astype(int)\n",
    "    test_F1[k]=sklearn.metrics.f1_score(y_test, y_pred)\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1 score 0.3313492063492064\n",
      "Average Run time 157.32080109914145\n"
     ]
    }
   ],
   "source": [
    "print ('Average f1 score', np.mean(test_F1))\n",
    "print ('Average Run time', np.mean(time_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the LSTM classifier gives a significantly lower F1 score. This may be improved by changing the model. However, we find that the SGT embedding could work with a small and unbalanced data without the need of a complicated classifier model.\n",
    "\n",
    "LSTM models typically require more data for training and also has significantly more computation time. The LSTM model above took 425.6 secs while the MLP model took just 9.1 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
